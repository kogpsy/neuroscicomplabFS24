[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurowissenschaft Computerlab",
    "section": "",
    "text": "Fr√ºhjahrssemester 2024"
  },
  {
    "objectID": "pages/admin/termine.html",
    "href": "pages/admin/termine.html",
    "title": "Termin√ºbersicht",
    "section": "",
    "text": "Termine der Leistungsnachweise\n\nGruppe Montag\n\n\n\nLeistungsnachweis\nForm\nBeginn\nAbgabetermin\nLink\nInhalt\n\n\n\n\nQ1: Vorkenntnisse\nQuiz auf Ilias\n19.02.2024\n28.02.2024 23:55\nQ1\nErheben der Vorkenntnisse\n\n\n√ú1: PsychoPy Experiment\n√úbung\n11.03.2024\n21.03.2024 23:55\n√ú1\nDatenerheben mit einem PsychoPy Experiment\n\n\n√ú2: Data wrangling\n√úbung\n25.03.2024\n17.04.2024 23:55\n√ú2\nDatens√§tze automatisiert einlesen und vorverarbeiten\n\n\n√ú3: Data visualization\n√úbung\n08.04.2024\n28.04.2024 23:55\n√ú3\nDaten visualisieren: Galerie\n\n\n√ú4: Data analysis I\n√úbung\n29.04.2024\n17.05.2024 23:55\n√ú4\n\n\n\n√ú5: Data analysis II\n√úbung\n13.05.2024\n31.05.2024 23:55\n\n\n\n\nQ2: Kursinhalte\nQuiz auf Ilias\n20.05.2024\n27.05.2024 23:55\n\n\n\n\n\n\n\nGruppe Mittwoch\n\n\n\nLeistungsnachweis\nForm\nBeginn\nAbgabetermin\nLink\nInhalt\n\n\n\n\nQ1: Vorkenntnisse\nQuiz auf Ilias\n21.02.2024\n28.02.2024 23:55\nQ1\nErheben der Vorkenntnisse\n\n\n√ú1: PsychoPy Experiment\n√úbung\n13.03.2024\n23.03.2024 23:55\n√ú1\nDatenerheben mit einem PsychoPy Experiment\n\n\n√ú2: Data wrangling\n√úbung\n27.03.2024\n17.04.2024 23:55\n√ú2\nDatens√§tze automatisiert einlesen und vorverarbeiten\n\n\n√ú3: Data visualization\n√úbung\n10.04.2024\n30.04.2024 23:55\n√ú3\nDaten visualisieren: Galerie\n\n\n√ú4: Data analysis I\n√úbung\n01.05.2024\n19.05.2024 23:55\n√ú4\n\n\n\n√ú5: Data analysis II\n√úbung\n15.05.2024\n02.06.2024 23:55\n\n\n\n\nQ2: Kursinhalte\nQuiz auf Ilias\n20.05.2024\n27.05.2024 23:55\n\n\n\n\n\n\n\n\nInhalte\nAn den 13 Sitzungen werden wir folgende Themen behandeln (kleine √Ñnderungen vorbehalten):\n\n1 Einf√ºhrung\nSitzung 1 (19./21. Februrar 2024)\n\nEinf√ºhrung in Programmiersprachen und -umgebungen im neurowissenschaftlichen Kontext (Python, R, Matlab, Julia, Bash) und ihre Programmierumgebungen (z.B. RStudio, PsychoPy, usw.)\n\n\n\n\nInstallieren der f√ºr den Kurs ben√∂tigten Software\nDatacamp-Kurse anschauen und belegen.\nQuiz 1 ausf√ºllen (siehe Ilias)\n\n\n\n2 Experimente programmieren\nSitzung 2 (26./28. Februrar 2024)\nSitzung 3 (4./6. M√§rz 2024)\n\n√úbersicht Leistungsnachweise,\nExperimente in den Neurowissenschaften: Elemente und Herausforderungen\nErstellen von zwei computerbasierten Experimenten mit Python und PsychoPy\n√úbung 1: Datenerhebung\n\n\n\n3 Data Wrangling & Daten visualisieren\nSitzung 4 (11./13. M√§rz 2024)\nSitzung 5 (18./20. M√§rz 2024)\nSitzung 6 (25./27. M√§rz 2024)\nFerien: 1./3. April: Vorlesungsfreie Zeit\nSitzung 7 (8./.10. April 2024)\nSitzung 8 (15./17. April 2024)\n\nArbeiten in R mit Projekten und Rmarkdown\nDaten mit R einlesen, bearbeiten und visualisieren\nDatenverarbeitung automatisieren\nOpen Science, Reproduzierbarkeit und Datenmanagement: wie neurowissenschaftliche Daten und Datenverarbeitungspipelines sinnvoll gestaltet und geteilt werden k√∂nnen\n√úbung 2 und 3: Datenverarbeiten und visualisieren\n\n\n\n4 Analysemethoden\nSitzung 8 (15./17. April 2024)\nSitzung 9 (22./24. April 2024)\nSitzung 10 (29. April und 1. Mai 2024)\n\nBesonderheiten neurowissenschaftlicher Daten und Analysen\nfrequentistische und bayesianische Analyseverfahren\n√úbung 4: Datensatz analysieren\n\n\n\n5 Modellieren von neurowissenschaftlichen Daten\nSitzung 11 (6../8. Mai 2024)\nSitzung 12 (13./15. Mai 2024)\nTermine von Mo 20. Mai 2024 und Mi 22. Mai 2024 fallen aus (Pfingstmontag) stattdessen wird Quiz 2 ausgef√ºllt.\nSitzung 13 (27./29. Mai 2014)\n\nSignalentdeckungstheorie (signal detection theory, SDT)\nDrift Diffusion Models (DDM)\nQuiz 2: Abschlussquiz\n√úbung 5: Datensatz analysieren\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "pages/admin/wichtiges.html",
    "href": "pages/admin/wichtiges.html",
    "title": "Wichtiges zum Kurs",
    "section": "",
    "text": "In diesem anwendungsorientierten Kurs erwerben Sie Wissen √ºber Methoden der Datenerhebung, -verarbeitung und -analyse im Feld der Neurowissenschaften. Wir behandeln im Rahmen der Veranstaltung folgende Inhalte:\n\nWichtige Programmiersprachen und - umgebungen in den Neurowissenschaften\nProgrammieren von computerbasierten Experimenten (mit PsychoPy)\nProgrammieren von Datenverarbeitungspipelines: Datenvorverarbeitung und Visualisierung (‚Äûgood practices‚Äù, Data Cleaning, Reproduzierbarkeit)\nAnalysemethoden und Modelle zugeschnitten auf unterschiedliche neurowissenschaftliche Datens√§tze und Fragestellungen mit frequentistischen wie auch bayesianischen Verfahren (u.a. Reaktionszeiten, Signal detection theory, Drift diffusion models)\nInterpretation und kritische Einordnung von Analyseergebnissen"
  },
  {
    "objectID": "pages/admin/wichtiges.html#ilias",
    "href": "pages/admin/wichtiges.html#ilias",
    "title": "Wichtiges zum Kurs",
    "section": "Ilias",
    "text": "Ilias\nUnter diesen Links finden Sie die Iliasgruppen:\nILIAS (Montag) üëâ 468703-FS2024-0\nILIAS (Mittwoch) üëâ 468703-FS2024-1"
  },
  {
    "objectID": "pages/admin/wichtiges.html#kursvoraussetzungen",
    "href": "pages/admin/wichtiges.html#kursvoraussetzungen",
    "title": "Wichtiges zum Kurs",
    "section": "Kursvoraussetzungen",
    "text": "Kursvoraussetzungen\nWir werden mit der Programmiersprache R und zu einem kleinen Teil mit Python arbeiten. Sie ben√∂tigen in der Veranstaltung deshalb einen eigenen Laptop (Tablets sind nicht geeignet!) mit ca. 20 GB freiem Speicherplatz und mit einer installierten (aktuellen) Version von R und RStudio (Link zum Download von R und RStudio).\nR Kenntnisse (gem√§ss Statistik I-IV in Psychologie) werden vorausgesetzt. Zur Auffrischung dient folgender Link (https://methodenlehre.github.io/einfuehrung-in-R/) oder f√ºr Fortgeschrittene die B√ºcher ‚ÄûAdvanced R‚Äù und ‚ÄûR for Data Scientists‚Äù von Hadley Wickham."
  },
  {
    "objectID": "pages/admin/wichtiges.html#anwesenheit",
    "href": "pages/admin/wichtiges.html#anwesenheit",
    "title": "Wichtiges zum Kurs",
    "section": "Anwesenheit",
    "text": "Anwesenheit\nDie Anwesenheit im Kurs wird vorausgesetzt (2 Abwesenheiten sind ok). Wir w√ºrden den Kurs gerne ohne Anwesenheitskontrollen durchf√ºhren - falls n√∂tig werden wir diese aber einf√ºhren.\nDas Online-Skript erlaubt das Nacharbeiten des wichtigsten Stoffes im Eigenstudium, wir k√∂nnen jedoch nicht f√ºr die Vollst√§ndigkeit garantieren. Hilfestellung beim Programmieren und Verstehen der Inhalte bieten wir w√§hrend der Kurszeiten an, aus zeitlichen Gr√ºnden k√∂nnen wir keine Beantwortung von Fragen zum Kursinhalt per E-Mail anbieten. Bitte stellen Sie Ihre Fragen in der Veranstaltung - auch Ihre Mitstudierenden werden davon profitieren, oft haben mehrere Personen dieselbe Frage."
  },
  {
    "objectID": "pages/admin/wichtiges.html#form-der-leistungsnachweise",
    "href": "pages/admin/wichtiges.html#form-der-leistungsnachweise",
    "title": "Wichtiges zum Kurs",
    "section": "Form der Leistungsnachweise",
    "text": "Form der Leistungsnachweise\nLeistungsnachweise werden in Form von 5 √úbungen und 2 Quizzes erbracht. Hiervon m√ºssen alle abgegeben und als bestanden bewertet werden. Bei Nicht-Bestehen erhalten Sie eine 2. Frist f√ºr die Abgabe oder einen Zusatzauftrag.\nAlle Leistungsnachweise werden in den Veranstaltungen angek√ºndigt. Die Termine f√ºr die Leistungsnachweise finden Sie hier.\n\n√úbungen\nDie √úbungen werden auf der Website aufgeschaltet. Die Ergebnisse der √úbungen m√ºssen in den entsprechenden Ordner auf ILIAS hochgeladen werden. Je nach Umfang der √úbung wird die Zeit bis zur Abgabe unterschiedlich ausfallen. Sie wird jedoch immer mindestens zwei Wochen betragen.\n\n\nQuizzes\nDas erste Quiz wird w√§hrend der ersten Veranstaltung durchgef√ºhrt und dient vor allem dazu zu erheben, welche Kenntnisse schon vorhanden sind. Das zweite Quiz wird gegen Ende des Semesters stattfinden und das Gelernte pr√ºfen."
  },
  {
    "objectID": "pages/chapters/Modeling_1.html",
    "href": "pages/chapters/Modeling_1.html",
    "title": "Einf√ºhrung",
    "section": "",
    "text": "Uns interessiert, wie Daten entstanden sind. Wir k√∂nnen Daten deskriptiv beschreiben. Aber auch da haben wir bereits ein Vorstellung vom Prozess, der die Daten generiert hat. Das Ziel ist es, von den Daten zu lernen, sie zu beschreiben, vorherzusagen und zu erkl√§ren.",
    "crumbs": [
      "Modellierung von Daten",
      "Einf√ºhrung"
    ]
  },
  {
    "objectID": "pages/chapters/Modeling_1.html#verbales-modell",
    "href": "pages/chapters/Modeling_1.html#verbales-modell",
    "title": "Einf√ºhrung",
    "section": "Verbales Modell",
    "text": "Verbales Modell\nBeginnen wir mit einem verbalen Modell von diesem Prozess. Die Endposition des Steins h√§ngt von der Kraft und der Richtung (Input) des Wurfs ab. Faktoren wie z.B. der Luftwiderstand oder die Gravitationskraft ber√ºcksichtigen wir vorerst nicht in unserem Modell, da sie konstant sind.\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  i((Input)):::A --&gt; p((Position)):::B\n  \n  classDef A fill:#ffffff, r:40px\n  classDef B fill:#e5e4e4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Simulation verbales DAG\n\n\n\nDen Prozess, der durch das verbale DAG beschrieben wird, wiederholen wir 10 Mal. Der Input bleibt konstant. Bei jeder Durchf√ºhrung notieren wir die Endposition des Steins.\n\n√úberlegen Sie sich, wie diese Daten aussehen.\nSimulieren (generieren) Sie diese Daten in R.\nMachen Sie eine sinnvolle Abbildung der simulierten Daten.",
    "crumbs": [
      "Modellierung von Daten",
      "Einf√ºhrung"
    ]
  },
  {
    "objectID": "pages/chapters/Modeling_1.html#statistisches-modell",
    "href": "pages/chapters/Modeling_1.html#statistisches-modell",
    "title": "Einf√ºhrung",
    "section": "Statistisches Modell",
    "text": "Statistisches Modell\nBer√ºcksichtigen wir nun, dass der Stein von einer Person geworfen wird. Die Person zielt immer auf die gleiche Stelle (Œº). Es ist aber unm√∂glich den Stein jedes Mal exakt gleich zu werfen. Es gibt also eine gewisse Variation der beobachteten Endpositionen (œÉ).\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  mu((Œº)):::a --&gt; i((Input)):::A\n  s((œÉ)):::a --&gt; i\n  i((\"Input \\n N(Œº,œÉ)\")):::A --&gt; p((Position)):::B\n\n  classDef a fill:#ffffff, r:20px\n  classDef A fill:#ffffff, r:40px\n  classDef B fill:#e5e4e4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Simulation\n\n\n\nDen Prozess, der durch das (statistische) DAG beschrieben wird, wiederholen wir 10 Mal. Der Input ist nun normal verteilt. Bei jeder Durchf√ºhrung notieren wir die Endposition des Steins.\n\n√úberlegen Sie sich, wie diese Daten aussehen.\nSimulieren (generieren) Sie diese Daten in R.\nMachen Sie eine sinnvolle Abbildung der simulierten Daten.\n\n\n\n\nSimulation vs.¬†Inferenz\n\n\n\n\n\nmu = 5\nsigma = 0.2\nsim_data = tibble(\n    pos = rnorm(n = 10, mu, sigma)\n    )\n\nhead(sim_data)\nhead(d)\nfit = lm(x ~ 1,\n         data = d)\n\ncoef(fit)[\"(Intercept)\"]\nsigma(fit)\n\n\n\n\n\nSimulation\nWir w√§hlen Werte f√ºr die Modell-Parameter:\nDamit k√∂nnen wir Daten simulieren:\n\n\n# A tibble: 6 √ó 1\n    pos\n  &lt;dbl&gt;\n1  5.00\n2  5.05\n3  4.81\n4  4.78\n5  4.89\n6  4.89\n\n\n\n\n\n\nInferenz (Parametersch√§tzung)\nWir beginnen mit Daten. Die Modell parameter sind uns nicht bekannt.\n\n\n# A tibble: 6 √ó 1\n      x\n  &lt;dbl&gt;\n1  4.25\n2  4.26\n3  4.29\n4  4.24\n5  4.37\n6  4.39\n\n\nBasierend auf dem DAG formulieren wir ein lineares Modell. In diesem Fall sch√§tzen wir nur den Intercept. Mit der Funktion lm() k√∂nnen wir die Parameter von diesem Modell sch√§tzen (frequentistisch).\n\n\n(Intercept) \n   4.285552 \n\n\n[1] 0.07948327\n\n\n\n\n\n\n\nKategorialer Pr√§diktor - Planet\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  g((Gravitation)):::A --&gt; p((Position)):::B\n  i((Input)):::A --&gt; p\n  \n  classDef A fill:#ffffff, r:40px\n  classDef B fill:#e5e4e4\n\n\n\n\n\n\n\n\nIn diesem Fall haben wir die Endposition auf der Erde und auf dem Mond gemessen.\n\n\n# A tibble: 6 √ó 2\n  planet   pos\n  &lt;chr&gt;  &lt;dbl&gt;\n1 erde    5.35\n2 erde    5.12\n3 erde    5.38\n4 mond    6.92\n5 mond    7.17\n6 mond    7.18\n\n\n\n\n\nHier f√ºgen wir dem linearen Modell einen kategorialen Pr√§diktor hinzu und sch√§tzen die Parameter mit der Funktion lm().\n\nlm(pos ~ 1 + planet,\n   data = d_cat)\n\n\nCall:\nlm(formula = pos ~ 1 + planet, data = d_cat)\n\nCoefficients:\n(Intercept)   planetmond  \n      5.001        1.986  \n\n\n\n\nKontinuierlicher Pr√§diktor - K√∂rpergr√∂sse\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  g((K√∂rpergr√∂sse)):::A --&gt; p((Position)):::B\n  i((Input)):::A --&gt; p\n  \n  classDef A fill:#ffffff, r:40px\n  classDef B fill:#e5e4e4\n\n\n\n\n\n\n\n\nHier interessiert uns der Einfluss der K√∂rpergr√∂sse auf die Endposition eines geworfenen Steins.\n\n\n# A tibble: 6 √ó 2\n  gr√∂sse   pos\n   &lt;dbl&gt; &lt;dbl&gt;\n1    165  5.44\n2    165  4.79\n3    165  4.97\n4    170  6.05\n5    170  6.23\n6    170  5.97\n\n\n\n\n\nHier f√ºgen wir dem linearen Modell einen kontinuierlichen Pr√§diktor hinzu und sch√§tzen die Parameter mit der Funktion lm().\n\nlm(pos ~ 1 + gr√∂sse,\n   data = d_cont)\n\n\nCall:\nlm(formula = pos ~ 1 + gr√∂sse, data = d_cont)\n\nCoefficients:\n(Intercept)       gr√∂sse  \n   -29.0323       0.2058  \n\n\n\n\n\n\n\n\nHands-on: Parameter Recovery\n\n\n\nMit parameter recovery kann √ºberpr√ºft werden, wie gut die Parameter des Modells gesch√§tzt werden k√∂nnen. Dazu werden zu erst Daten simuliert. In der anschliessenden Analyse der Daten sieht man wie nahe die gesch√§tzten Parameter den wahren (in der Simulation verwendeten) sind.\n\nEntscheiden Sie sich f√ºr ein Modell mit einem kategorialen oder einem kontinuierlichen Pr√§diktor.\nSimulieren (generieren) Sie die entsprechenden Daten in R.\nAnalysieren Sie die Simulierten Daten mit dem entsprechenden Modell.\n\nWie nahe ist die Sch√§tzung an den wahren Parametern?\nVon welchen Faktoren k√∂nnte das abh√§ngen?",
    "crumbs": [
      "Modellierung von Daten",
      "Einf√ºhrung"
    ]
  },
  {
    "objectID": "pages/chapters/computerlab.html",
    "href": "pages/chapters/computerlab.html",
    "title": "Computerlab",
    "section": "",
    "text": "In diesem anwendungsbasierten Kurs lernen Sie mit Daten zu arbeiten. Der Computer ist dabei ein absolut notwendiges Werkzeug. Unser Fokus liegt auf der Vermittlung von Anwendungskenntnisse. Die Termine setzen sich deshalb aus Inputs und Hands-on Sessions zusammen.\nAls Einstieg schauen wir uns an, wo der Computer in einem Forschungsprojekt gebraucht wird, wo er die Arbeit erleichtert."
  },
  {
    "objectID": "pages/chapters/computerlab.html#neurocomp-lab",
    "href": "pages/chapters/computerlab.html#neurocomp-lab",
    "title": "Computerlab",
    "section": "",
    "text": "In diesem anwendungsbasierten Kurs lernen Sie mit Daten zu arbeiten. Der Computer ist dabei ein absolut notwendiges Werkzeug. Unser Fokus liegt auf der Vermittlung von Anwendungskenntnisse. Die Termine setzen sich deshalb aus Inputs und Hands-on Sessions zusammen.\nAls Einstieg schauen wir uns an, wo der Computer in einem Forschungsprojekt gebraucht wird, wo er die Arbeit erleichtert."
  },
  {
    "objectID": "pages/chapters/data_analysis_hypothesistesting.html",
    "href": "pages/chapters/data_analysis_hypothesistesting.html",
    "title": "Hypothesentests",
    "section": "",
    "text": "In den Neurowissenschaften ist es wichtig zu wissen, welche experimentellen Manipulationen einen Effekt haben. Genauso wichtig ist es jedoch zu wissen, welche Manipulationen keinen Effekt haben. Diese Frage zu beantworten ist jedoch mit traditionellen statistischen Ans√§tzen schwierig. Nicht signifikante Ergebnisse sind schwer zu interpretieren: Unterst√ºtzen sie die Nullhypothese (evidence of absence) oder sind sie einfach nicht informativ (absence of evidence)?\nZwei Ans√§tze zur Beantwortung dieser Frage werden im Folgenden, anhand des Beispiels von \\(t\\)-Tests, vorgestellt:"
  },
  {
    "objectID": "pages/chapters/data_analysis_hypothesistesting.html#modellvergleich",
    "href": "pages/chapters/data_analysis_hypothesistesting.html#modellvergleich",
    "title": "Hypothesentests",
    "section": "Modellvergleich",
    "text": "Modellvergleich\nBeim Modellvergleich interessiert welches Modell die Daten besser erkl√§rt. Die Bayes‚Äôsche Regel kann verwendet werden, um die Wahrscheinlichkeit zweier Modelle \\(\\mathcal{M1}\\) und \\(\\mathcal{M2}\\) zu berechnen (gemittelt √ºber alle m√∂glichen Parameterwerte innerhalb des Modells):\n\\[\np(\\mathcal{M}_1 | y) = \\frac{P(y | \\mathcal{M}_1) p(\\mathcal{M}_1)}{p(y)}\n\\]\nund\n\\[\np(\\mathcal{M}_2 | y) = \\frac{P(y | \\mathcal{M}_2) p(\\mathcal{M}_2)}{p(y)}\n\\]\nHierf√ºr kann das Verh√§ltnis der beiden Wahrscheinlichkeiten (_Posterior Odds__) berechnet werden: \\(p(\\mathcal{M}_1 | y) / p(\\mathcal{M}_2 | y)\\), was gek√ºrzt folgende Formel ergibt:\n\\[\n\\underbrace{\\frac{p(\\mathcal{M}_1 | y)} {p(\\mathcal{M}_2 | y)}}_\\text{Posterior odds} = \\underbrace{\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}}_\\text{Ratio of marginal likelihoods} \\cdot \\underbrace{ \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)}}_\\text{Prior odds}\n\\]\nAuf der linken Seite steht das Verh√§ltnis der a-posteriori Wahrscheinlichkeiten der beiden Modelle, auf der rechten Seite das Verh√§ltnis der Marginal Likelihoods der beiden Modelle, multipliziert mit den a-priori Wahrscheinlichkeiten jedes Modells.\nDie Marginal Likelihoods (auch bekannt als Modell-Evidenz) zeigen, wie gut jedes Modell die Daten erkl√§rt. Diese geben dar√ºber Auskunft, wie wahrscheinlich die Daten sind, wenn wir alle m√∂glichen Parameterwerte ber√ºcksichtigen. Die Marginal Likelihoods sind also die Wahrscheinlichkeit der Daten, gemittelt √ºber alle m√∂glichen Parameterwerte."
  },
  {
    "objectID": "pages/chapters/data_analysis_hypothesistesting.html#bayes-factors",
    "href": "pages/chapters/data_analysis_hypothesistesting.html#bayes-factors",
    "title": "Hypothesentests",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nDie Posterior Odds sagen uns, welches Modell wir a-priori und a-posteriori f√ºr wahrscheinlicher halten. Da unsere a-priori √úberzeugungen aber subjektiv sein k√∂nnen, sind wir eigentlich nur an dem Verh√§ltnis der marginalen Likelihoods interessiert. Wir k√∂nnen annehmen, dass a-priori die beiden Modelle gleichwahrscheinlich sind; das heisst, wir setzen die Prior Odds auf 1 setzen. So erhalten wir den Bayes Factor:\n\\[\n\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}\n\\]\nWenn \\(P(y | \\mathcal{M}_1)\\) gr√∂sser ist als \\(P(y | \\mathcal{M}_2)\\), dann ist der Bayes Factor gr√∂sser als 1. Falls \\(P(y | \\mathcal{M}_1)\\) kleiner ist als \\(P(y | \\mathcal{M}_2)\\), dann ist der Bayes Factor kleiner als 1. Der Bayes Factor gibt also direkt an, welches Modell die Daten besser erkl√§rt.\nWenn wir zwei Modelle \\(\\mathcal{M}_1\\) und \\(\\mathcal{M}_2\\) vergleichen, wird der Bayes Factor oftmals so geschrieben:\n\\[ BF_{12} = \\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}\\]\n\\(BF_{12}\\) ist also der Bayes Factor f√ºr \\(\\mathcal{M}_1\\) und gibt an um wieviel \\(\\mathcal{M}_1\\) die Daten besser ‚Äúerkl√§rt‚Äù.\nAls Beispiel, wenn wir ein \\(BF_{12} = 5\\) erhalten, bedeutet dies, dass die Daten 5 Mal wahrscheinlicher unter Modell 1 als unter Modell 2 aufgetreten sind. Umgekehrt, wenn \\(BF_{12} = 0.2\\), dann sind die Daten 5 Mal wahrscheinlicher unter Modell 2 aufgetreten.\nWenn wir \\(BF_{12} = 0.2\\) erhalten, ist es einfacher, Z√§hler und Nenner zu vertauschen:\n\\[ BF_{21} = \\frac{P(y | \\mathcal{M}_2)}{P(y | \\mathcal{M}_1)}\\]\nDie folgenden Interpretationen von Bayes Factors werden manchmal verwendet, obwohl es nicht wirklich notwendig ist, diese zu klassifizieren. Bayes Factors sind ein kontinuierliches Mass f√ºr Evidenz.\nZusammenfassend kann gesagt werden:\n\nDer Bayes Factor ist ein Verh√§ltnis zweier konkurrierender statistischer Modelle, die durch ihre Evidenz dargestellt werden, und wird verwendet, um die Unterst√ºtzung f√ºr ein Modell gegen√ºber dem anderen zu quantifizieren. Die fraglichen Modelle k√∂nnen einen gemeinsamen Satz von Parametern haben, z. B. eine Nullhypothese und eine Alternative, dies ist jedoch nicht erforderlich. Zum Beispiel k√∂nnte es sich auch um ein nichtlineares Modell im Vergleich zu seiner linearen N√§herung handeln. Wikipedia\n\nBayes Factors sind eine alternative Methode, um Evidenz zu quantifizieren. Alternativ zu p-Werten bieten Bayes Factors Evidenz f√ºr oder gegen eine Hypothese.\n\n\\(p\\)-Werte sind schwierig zu erkl√§ren, auch f√ºr erfahrene Forschende, wie dieses Video zeigt."
  },
  {
    "objectID": "pages/chapters/data_analysis_hypothesistesting.html#bayesianischer-t-test",
    "href": "pages/chapters/data_analysis_hypothesistesting.html#bayesianischer-t-test",
    "title": "Hypothesentests",
    "section": "Bayesianischer \\(t\\)-Test",
    "text": "Bayesianischer \\(t\\)-Test\nWir f√ºhren oft Modellvergleiche zwischen einer Nullhypothese \\(\\mathcal{H}_0\\) und einer alternativen Hypothese \\(\\mathcal{H}_1\\) durch (Die Begriffe ‚ÄúModell‚Äù und ‚ÄúHypothese‚Äù werden synonym verwendet). Eine Nullhypothese bedeutet, dass wir den Wert des Parameters auf einen bestimmten Wert festlegen, z.B. \\(\\theta = 0.5\\). Die alternative Hypothese bedeutet, dass wir den Wert des Parameters nicht festlegen, sondern eine a-priori Verteilung annehmen. Im Gegensatz zu NHST muss die Alternativhypothese spezifiziert werden. Mit anderen Worten, die Parameter m√ºssen eine a-priori Verteilung erhalten.\nIn JASP werden Bayes Factors (BF) so berichtet:\n\\[ BF_{10} = \\frac{P(y | \\mathcal{H}_1)}{P(y | \\mathcal{H}_0)}\\]\nDies ist ein BF f√ºr eine ungerichtete Alternative \\(\\mathcal{H}_1\\) gegen die Nullhypothese \\(\\mathcal{H}_0\\). Wenn wir einen gerichteten Test durchf√ºhren, dann wird der BF entweder so (\\(&gt;0\\)):\n\\[ BF_{+0} = \\frac{P(y | \\mathcal{H}_+)}{P(y | \\mathcal{H}_0)}\\]\noder so (\\(&lt;0\\)) berichtet. \\[ BF_{-0} = \\frac{P(y | \\mathcal{H}_-)}{P(y | \\mathcal{H}_0)}\\]\nWenn wir nun einen BF f√ºr die Nullhypothese wollen, k√∂nnen wir einfach den Kehrwert von \\(BF_{10}\\) nehmen:\n\\[ BF_{01} = \\frac{1}{BF_{10}}\\]\n\n\n\n\n\n\nHands-on: Bayesiansicher \\(t\\)-Test in JASP\n\n\n\n\nLaden Sie hier den Stroop Datensatz herunter. Der Datensatz wurde f√ºr diese Zwecke ins wide-Format angepasst, das R-Skript daf√ºr k√∂nnen Sie sich hier ansehen.\nLaden Sie den Datensatz in JASP und schauen Sie ihn an.\nWie k√∂nnte die Forschungsfrage lauten? Was ist die Null- und Alternativhypothese?\nF√ºhren Sie einen `Bayesian Paired Sample \\(t\\)-Test aus.\nExplorieren Sie die Resultate und welche Optionen Sie f√ºr weitere Einstellungen haben.\nWelche Bayes Factors finden Sie f√ºr die Nullhypothese? Und f√ºr die Alternativhypothese?\n\n\n\n\n\n\n\n\n\nWeiterf√ºhrende Informationen zu Bayesianischem Hypothesentesten\n\n\n\n\nVertiefende Informationen, inkl. Herleitung, zu Bayesianischen Hypothesentest finden Sie hier.\nHier finden Sie eine interaktive Visualisierung.\nbrms ist ein R-Package, welches sich f√ºr Bayesianische Multilevel-Modelle eignet, da JASP relativ rasch an die Grenzen st√∂sst f√ºr komplexere Modelle."
  },
  {
    "objectID": "pages/chapters/data_analysis_intro.html",
    "href": "pages/chapters/data_analysis_intro.html",
    "title": "Einf√ºhrung",
    "section": "",
    "text": "In der Forschung und Diagnostik interessieren uns oft Eigenschaften eines Prozesses oder einer Person, welche wir nicht direkt messen k√∂nnen. Deshalb werden Testverfahren und Experimente angewendet um diese latenten Variablen messbar zu machen. Mit statistischen Verfahren wird dann versucht aus den gemessenen Daten Informationen √ºber die nicht direkt messbare Eigenschaft zu erhalten."
  },
  {
    "objectID": "pages/chapters/data_analysis_intro.html#vorbereitung",
    "href": "pages/chapters/data_analysis_intro.html#vorbereitung",
    "title": "Einf√ºhrung",
    "section": "Vorbereitung",
    "text": "Vorbereitung\n\n\n\n\n\n\nHands-on: Reaktivierung Statistikwissen\n\n\n\n\nBesprechen Sie in kleinen Gruppen folgende Fragen:\n\n\nWas ist eine Null-, was eine Alternativhypothese?\nWas bedeutet die Distanz zwischen den beiden Mittelwerten?\nWas ist statistische Power?\nWelche Rolle spielt die Stichprobengr√∂√üe\nWas ist ein p-Wert?\nWas sind Typ I und Typ II Fehler?\nWelche Fragen k√∂nnen Sie mit einem Nullhypothesen-Test beantworten?\n\n\nK√∂nnen Sie die Begrifflichkeiten in dieser Grafik einordnen?\n\n\n\n√úberlegen Sie sich, was Null- und Alternativhypothese in unseren Experimenten sein k√∂nnen.\n\n[10 Minuten]\n\n\n\nSie k√∂nnen zur Beantwortung dieser Fragen z.B. die Interaktive Visualisierung ‚ÄúUnderstanding Statistical Power and Significance Testing‚Äù nutzen.\n\n\n\n\n\n\nProjekt und Daten herunterladen\n\n\n\nHier finden Sie die Daten zum herunterladen.\nLesen Sie anschliessend die Daten ein:\n\n## Daten einlesen\nlibrary(tidyverse)\nd_stroop &lt;- read_csv(\"data/dataset_stroop_clean.csv\") |&gt;\n    mutate(across(where(is.character), as.factor)) |&gt;\n    mutate(congruent = as.factor(congruent)) |&gt;\n    filter(rt &lt; 4 & rt &gt; 0.1) |&gt;\n    filter(corr == 1) |&gt;\n    na.omit()"
  },
  {
    "objectID": "pages/chapters/data_analysis_intro.html#directed-acyclic-graphs-dags",
    "href": "pages/chapters/data_analysis_intro.html#directed-acyclic-graphs-dags",
    "title": "Einf√ºhrung",
    "section": "Directed Acyclic Graphs (DAGs)",
    "text": "Directed Acyclic Graphs (DAGs)\nEin DAG (directed acyclic graph) eignet sich f√ºr die Darstellung komplexer Zusammenh√§nge in Daten und Prozessen. Mit einem DAG kann veranschaulicht werden, welche Variablen einander beeinflussen. Die Kreise (nodes) werden f√ºr einzelne Elemente verwendet und die Pfeile (arrows oder edges) beschreiben die Beziehung zwischen den Elementen. Die Darstellung beschreibt einen Prozess also mit gerichteten (directed) und nicht zyklischen (acyclic) Beziehungen.\nWir k√∂nnen beispielsweise annehmen, dass die Farbe-Wort-Kongruenz im Stroop Task beeinflusst, wie schnell die Aufgabe gel√∂st werden kann.\nErstellen eines DAGs\n1. Beobachtete Variable bestimmen\nDie beobachtete Variable nennen wir hier \\(y\\). Der Kreis ist grau eingef√§rbt, weil die Werte in dieser Variable gemessen wurde, also bekannt ist.\nIn unserem Beispiel haben wir die Reaktionszeit gemessen. Im Datensatz enth√§lt die Variable rt die Information, wie schnell eine Person in jedem Trial geantwortethat.\n2. Verteilung bestimmen\nEs muss festgelegt werden, welche Verteilung die Daten \\(y\\) am besten beschreibt. Eine Verteilung ist immer nur eine Ann√§herung, die gemessenen Daten entsprechen dieser Annahme eigentlich nie perfekt. Es geht darum eine Verteilung zu finden die gut genug zu den Daten passt. Jede Verteilung hat Parameter, die gesch√§tzt werden k√∂nnen. Es gibt Verteilungen, welche durch einen Parameter definiert werden, andere brauchen mehrere Parameter.\nEine sehr h√§ufig verwendete Verteilung in statistischen Analysen ist die Normalverteilung. Die Annahme einer Normalverteilung erm√∂glicht es uns, mit nur 2 Parametern die Daten in der Variable zu beschreiben (nat√ºrlich ist das nur eine Ann√§herung aber meistens eine gen√ºgend gute): Dem Mittelwert (\\(\\mu\\)) und der Standardabweichung (\\(\\sigma\\)).\n\nHier im Distribution Zoo werden Verteilungen, zugrundeliegende Daten sowie Code und Formeln zusammengefasst.\n\nUm die Verteilung unserer Datenpunkte zu bestimmen bzw. zu √ºberpr√ºfen k√∂nnen die Daten in R geplottet werden, z.B. mit geom_histogram. Das Argument binwidth bestimmt, wie breit ein Balken wird (hier 50 ms).\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_histogram(colour=\"black\", fill = \"white\", \n                   binwidth = 0.05, \n                   alpha = 0.5) +\n    theme_minimal()\n\n\n\n\nDiese Verteilung k√∂nnte beispielsweise mit einer Normalverteilung beschrieben werden. Der Mittelwert und die Standardabweichung k√∂nnen wir uns ausrechnen:\n\n# clean dataset first\nmu = mean(d_stroop$rt)\nmu\n\n[1] 0.7585427\n\nsigma = sd(d_stroop$rt)\nsigma\n\n[1] 0.3668467\n\n\nUm zu schauen, wie gut diese Normalverteilung mit den Parametern \\(\\mu\\) = 0.759 und \\(\\sigma\\) = 0.367 unsere Daten beschreibt k√∂nnen wir die Daten und simulierte Daten mit der angenommenenen Verteilung √ºbereinander plotten:\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_histogram(colour=\"black\", fill = \"white\", \n                   binwidth = 0.05, \n                   alpha = 0.5) +\n    geom_histogram(aes(x = rnorm(1:length(rt), mu, sigma)),\n                   binwidth = 0.05,\n                   alpha = 0.2) +\n    theme_minimal()\n\n\n\n\nWir k√∂nnen auch density-Plots daf√ºr nutzen:\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_density(colour=\"black\") +\n    geom_density(aes(x = rnorm(1:length(rt), mu, sigma)),\n                 colour=\"darkgrey\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHands-on: Verteilungen\n\n\n\n\nWelche Daten stammen aus unseren Daten, welche entsprechen der Normalverteilung \\(N(0.759, 0.367)\\) ?\nWie gut passt die Annahme der Normalverteilung f√ºr unsere Reaktionszeitdaten?\nFinden Sie auf Distribution Zoo eine passendere Verteilung?\n\nPr√ºfen Sie Ihre Verteilung indem Sie unten an den obigen Plot diese Verteilung mit gew√§hlten Parametern folgenden Code einf√ºgen.\n\nW√§hlen Sie dazu eine Verteilung und passende Parameter auf Distribution Zoo aus.\nSchauen Sie unter dem Reiter Code mit welcher Funktion die Daten in R generiert werden k√∂nnen. W√§hlen Sie Language: R und Property: random sample of size n aus.\nKopieren Sie die Funktion und ersetzen Sie rnorm(1:length(rt), mu, sigma) in unserem R-Code f√ºr das Histogram oder den Density-Plot mit Ihrer neuen Funktion. Das n m√ºssen Sie wieder 1:length(rt) nennen.\n\n\n\n\n\nBei Reaktionszeiten ist die Verteilung gar nicht so einfach anzupassen: Hier finden Sie ‚Äúbesser‚Äù geeignete Verteilungen sowie die M√∂glichkeit f√ºr einen vorgegebenen Datensatz oder Ihre eigenen Daten Parameterwerte anzupassen.\n3. Weitere Einflussfaktoren\nIn einem DAG k√∂nnen auch weitere Informationen, zum Beispiel Bedingungen sowie Messwiederholungen, hinzugef√ºgt werden.\n\\(\\mu\\) kann sich zum Beispiel in Abh√§ngigkeit der Bedingung (condition) ver√§ndern, also je nachdem ob die angezeigte Farbe kongruent war oder nicht.\nWenn wir nun den Einfluss der Bedingung untersuchen m√∂chten, k√∂nnten wir uns fragen, wie stark diese eine Ver√§nderung im Wert \\(\\mu\\) bewirkt. Genau dies tun wir z.B. bei Mittelwertsvergleichen wie z.B. bei t-Tests.\n\n\n\n\n\n\nHands-on: DAG zeichnen\n\n\n\nWie w√ºrde ein DAG f√ºr die Accuracy (Korrektheit) der Stroop-Daten aussehen?"
  },
  {
    "objectID": "pages/chapters/data_analysis_intro.html#daten-simulieren",
    "href": "pages/chapters/data_analysis_intro.html#daten-simulieren",
    "title": "Einf√ºhrung",
    "section": "Daten simulieren",
    "text": "Daten simulieren\nSich Gedanken zum datengenerierenden Prozess zu machen (wie beispielsweise mit einem aufgezeichneten Modell) hilft nicht nur in der Planung der Datenanalyse, sondern erm√∂glicht u.a. auch das Simulieren von Daten.\nM√∂gliche Schritte in der Datensimulation \nShiny-App f√ºr Datensimulation\nDatensimulation ist n√ºtzlich f√ºr:\n\nDie Vorbereitung von Pr√§registrationen und Registered Reports\nTesten/Debugging von Analysekripten (weil die ground truth bekannt ist)\nPower f√ºr komplexe Modelle sch√§tzen\nErstellen von reproduzierbaren Beispielsdatens√§tzen (f√ºr Demos, Lehre, oder wenn echte Datens√§tze nicht ver√∂ffentlicht werden k√∂nnen)\nPrior distribution checks in der Bayesianischen Statistik\nVerstehen von Modellen und Statistik\n\nWeitere Infos zu Datensimulation"
  },
  {
    "objectID": "pages/chapters/data_analysis_parameterestimates.html",
    "href": "pages/chapters/data_analysis_parameterestimates.html",
    "title": "Parametersch√§tzung",
    "section": "",
    "text": "Nach dem Data Cleaning und Preprocessing geht es darum, welche Informationen die Daten √ºber den zu untersuchenden Prozess beinhalten. Anhand der Daten sollen also R√ºckschl√ºsse auf den Prozess der zu diesen Daten gef√ºhrt hat geschlossen werden. Dies wird mit folgenden Schritten gemacht"
  },
  {
    "objectID": "pages/chapters/data_analysis_parameterestimates.html#maximum-likelihood-sch√§tzung",
    "href": "pages/chapters/data_analysis_parameterestimates.html#maximum-likelihood-sch√§tzung",
    "title": "Parametersch√§tzung",
    "section": "Maximum-Likelihood Sch√§tzung",
    "text": "Maximum-Likelihood Sch√§tzung\n\\(\\theta\\) ist der Parameterwert unter dem die beobachteten Daten am wahrscheinlichsten entstanden sind. Die beste Punktsch√§tzung des Parameters \\(\\theta\\), die wir machen k√∂nnen, wenn wir nur die Daten betrachten, und kein weiteres Vorwissen ber√ºcksichtigen, ist die Maximum-Likelihood Sch√§tzung.\nM√∂chten wir also z.B. sch√§tzen mit welcher Wahrscheinlichkeit die Person beim n√§chsten Trial eine richtige Antwort gibt, k√∂nnen wir dies aus den bisherigen Trials berechnen:\n\\[\\theta = correct / all \\]\nWenn die Person also 15 Mal richtig geantwortet hat in insgesamt 20 Trials, w√§re die Sch√§tzung also\n\\(\\theta = 15 / 20 = 0.75\\)\n\ntheta &lt;- correct / trials\ntheta\n\n[1] 0.75\n\n\nWir erhalten eine Punktsch√§tzung (einen Wert), die uns angibt mit welcher Wahrscheinlichkeit die Person beim n√§chsten Trial richtig antworten wird, n√§mlich 0.75, in 3/4 der F√§lle.\nWenn man ganz viele Male diese Spiele wiederholen w√ºrde, dann w√ºrde man diese Messung am wahrscheinlichsten reproduzieren k√∂nnen, wenn man f√ºr \\(\\theta\\) den Wert 0.75 einsetzt.\nDer grosse Nachteil einer Punktsch√§tzung ist es, dass wir keine Wahrscheinlichkeitsverteilung erhalten. Es g√§be auch noch viele andere Parameterwerte, die dieses Ergebnis von 15 korrekten Antworten in 20 Trials hervorbringen k√∂nnten, diese werden bei der Punktsch√§tzung nicht beachtet.\nUm das zu veranschaulichen plotten wir die Wahrscheinlichkeit von 15 korrekten Antworten in 20 Trials f√ºr alle Werte welche \\(\\theta\\) annehmen k√∂nnte. Diese Werte liegen zwischen 0 und 1, da wir von einer Wahrscheinlichkeit sprechen.\n\nlibrary(tidyverse)\n\n\ntibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% \n  mutate(density = dbinom(15, 20, x)) %&gt;% \n  \n  ggplot(aes(x = x, ymin = 0, ymax = density)) +\n  geom_ribbon(size = 0, alpha = 1/4, fill = \"steelblue\") +\n  geom_vline(xintercept = theta, linetype = 2, linewidth = 1.2) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 1)) +\n  xlab(\"Wahrscheinlichkeit\") +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nDie Punktsch√§tzung von \\(\\theta\\) wird mit der schwarzen gestrichelten Linie dargestellt. Die hellblaue Fl√§che zeigt, wie wahrscheinlich die einzelnen Werte jeweils sind (hier abgebildet sehen Sie relative Wahrscheinlichkeiten.\n\n\n\n\n\n\nHands-on: Punktsch√§tzung\n\n\n\nDiskutieren Sie in kleinen Gruppen, wie sinnvoll es ist sich hier auf einen Wert festzulegen:\n\nWie genau denken Sie bildet die Punktsch√§tzung die Realit√§t ab?\nWie viel wahrscheinlicher ist das berechnete \\(\\theta\\) von 0.75 im Vergleich zu einem \\(\\theta\\) von 0.70?\nWas kann das Sch√§tzen der Wahrscheinlichkeit f√ºr alle Parameterwerte f√ºr einen Mehrwert bringen?"
  },
  {
    "objectID": "pages/chapters/data_analysis_parameterestimates.html#posterior-sch√§tzung-in-der-bayesianischen-statistik",
    "href": "pages/chapters/data_analysis_parameterestimates.html#posterior-sch√§tzung-in-der-bayesianischen-statistik",
    "title": "Parametersch√§tzung",
    "section": "Posterior-Sch√§tzung in der Bayesianischen Statistik",
    "text": "Posterior-Sch√§tzung in der Bayesianischen Statistik\nIn der Bayesianischen Statistik wird die Wahrscheinkeitslehre angewandt, um die Wahrscheinlichkeit von Parameterwerten zu berechnen. Im Gegensatz zu der Frequentistischen Statistik wird nicht nur ein ‚Äúwahrer Wert‚Äù gesch√§tzt, sondern eine Verteilung. Es wird also f√ºr jeden m√∂glichen Parameterwert eine Wahrscheinlichkeit gesch√§tzt.\nDer Posterior wird also √ºber alle m√∂glichen Parameterwerte integriert, was ein wesentlicher Vorteil der Bayesian Statistik ist. So wird nicht nur der wahrscheinlichste Parameterwert ber√ºcksichtigt (Punktsch√§tzung), sondern durch das Einbeziehen der ganzen Parameterverteilung k√∂nnen auch Nebenoptima und ‚Äúfast‚Äù genauso wahrscheinliche Werte einbezogen werden.\nUm die Wahrscheinlichkeit von Parametern zu berechnen wird in der Bayesianischen Statistik das Bayes Theorem verwendet.\n\n\n\n\n\n\nBayes Theorem\n\n\n\nDas Bayes Theorem gibt die Formel f√ºr eine bedingte Wahrscheinlichkeit \\(P(A|B)\\) an.\n\\[ P(A|B) = \\frac{P(B|A)‚ãÖP(A)}{P(B)} \\]\nDas kann gelesen werden als:\n‚ÄúDie Wahrscheinlichkeit eines Ereignisses A unter der Bedingung, dass ein Ereignis B wahr ist, ist gleich der a priori Wahrscheinlichkeit, dass A wahr ist, multipliziert mit der Wahrscheinlichkeit, dass B eintritt, wenn A wahr ist. Dividiert wird das Ganze durch die Wahrscheinlichkeit, dass B eintritt, egal ob A wahr oder falsch ist.‚Äù\n\n\nDas bedeutet, um eine Bayesianische Parametersch√§tzung zu machen, m√ºssen wir Vorwissen integrieren. Dies tun wir in Form einer Prior-Verteilung. Ein sehr simple Variante ist, den Prior ist so zu w√§hlen, dass er allen m√∂glichen Werten dieselbe Wahrscheinlichkeit zuschreibt (wie in der Grafik unten). Dies ist aber selten empfehlenswert, darauf wird sp√§ter noch eingegangen.\nParametersch√§tzung\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Bayesianische Parametersch√§tzung in JASP\n\n\n\nAktivieren Sie in JASP das Modul ‚ÄúLearn Bayes‚Äù. W√§hlen Sie unter ‚ÄúLearn Bayes‚Äù: ‚ÄúBinomial Estimation‚Äù. W√§hlen Sie ‚ÄúEnter Sequence‚Äù.\nStellen Sie sich vor, sie untersuchen eine Person, welche behauptet, extrasensorische F√§higkeiten zu besitzen. Diese Person behauptet, dass vorhersagen kann, auf welcher Seite eine M√ºnze landet, bevor sie geworfen wurde. Sie werfen die M√ºnze 10 mal und die Person macht 7 korrekte Vorhersagen.\n\nWelche Fragen k√∂nnten von Interesse sein?\nWie w√ºrden Sie die Behauptung der Person √ºberpr√ºfen?\nGlauben Sie, dass die Person √ºber extra-sensorische F√§higkeiten verf√ºgt? Sind Sie skeptisch?\nUnter den Dropdown Menus Model, Prior and Posterior Distributions und Plots gibt es verschiedene Checkboxes. Versuchen Sie herauszufinden, was diese bewirken.\nWie k√∂nnen Sie Ihr Vorwissen in die Analyse einbeziehen? Wie verbinden Sie Ihr Vorwissen mit den beobachteten Daten? Passen Sie Ihren Prior f√ºr \\(\\theta\\) an:\n\nBeta Verteilungen"
  },
  {
    "objectID": "pages/chapters/data_analysis_parameterestimates.html#wrap-up",
    "href": "pages/chapters/data_analysis_parameterestimates.html#wrap-up",
    "title": "Parametersch√§tzung",
    "section": "Wrap-up",
    "text": "Wrap-up\nZusammenfassend kann gesagt werden:\n\nIn der frequentistischen Statistik wird angenommen, dass der Parameter einen wahren Wert hat, den wir aber nicht kennen. Wir erhalten eine Punktsch√§tzung f√ºr den Parameter und k√∂nnen keine Aussage √ºber die Wahrscheinlichkeit eines Parameterwerts machen. Der 95%-CI (confidence interval) sagt aus, dass bei Wiederholung des Experiments der ‚Äúwahre‚Äù Parameterwert in 95% der Konfidenzintervalle enthalten sein wird.\nIn der bayesianischen Statistik nehmen wir an, dass der Parameter eine Wahrscheinlichkeitsverteilung hat, die wir sch√§tzen k√∂nnen. Es muss zus√§tzlich eine Priorverteilung festgelegt werden. Wir erhalten eine Posterior Verteilung f√ºr die Parameterwerte und k√∂nnen eine Aussage √ºber Wahrscheinlichkeit eines Parameterwerts oder eines Modelles machen. Der 95%-CI (credible interval) enth√§lt zu 95% den ‚Äúwahren‚Äù Parameterwert.\n\n\n‚ÄúWahr‚Äù bedeutet hier, den Parameterwert der (wenn wir ein passendes Modell verwendet haben) zu diesen Daten gef√ºhrt hat."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html",
    "href": "pages/chapters/data_visualization_1.html",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "",
    "text": "Hands-on: Vorbereitung\n\n\n\n\nLaden Sie hier den Beispielsdatensatz herunter, speichern Sie diesen in einem data-Folder in einem R-Project.\n√ñffnen Sie ein neues RScript (.R) oder RMarkdown-File (.Rmd). In einem RMarkdown-File k√∂nnen Code und Text verbunden werden und die die Outputs des Codes (z.B. Grafiken) werden anzeigt."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#daten",
    "href": "pages/chapters/data_visualization_1.html#daten",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Daten",
    "text": "Daten\nDie wichtigste Komponente einer Grafik sind die Daten. Bevor eine Grafik erstellt wird, m√ºssen die Eigenschaften des Datensatzes bekannt sein.\n\nDer verwendete Datensatz stammt von @matejka_same_2017.\n\n# Einlesen des √úbungsdatensatzes\nd &lt;- read.csv(\"data/DatasaurusDozen.csv\") %&gt;%\n    mutate(condition = as.factor(condition)) # Variable condition soll ein Faktor sein\n\n# Datensatz anschauen\nglimpse(d)\n\nRows: 1,846\nColumns: 3\n$ condition &lt;fct&gt; away, away, away, away, away, away, away, away, away, away, ‚Ä¶\n$ value1    &lt;dbl&gt; 32.33111, 53.42146, 63.92020, 70.28951, 34.11883, 67.67072, ‚Ä¶\n$ value2    &lt;dbl&gt; 61.411101, 26.186880, 30.832194, 82.533649, 45.734551, 37.11‚Ä¶\n\n\nDatenformat\nAm einfachsten ist das plotten mit ggplot(), wenn die Daten im long-Format vorliegen. Das bedeutet:\n\nJede Variable die gemessen/erhoben wird hat eine Spalte (z.B. Versuchspersonennummer, Reaktionszeit, Taste).\nJede Messung hat eine Zeile. In unserem PsychoPy-Experiment entspricht dies einer Zeile pro Trial.\n\nDie hier eingelesenen Daten sind schon im long-Format.\n\nFalls die Daten im wide-Format abgespeichert sind, lohnt es sich diese umzuformatieren z.B. mit pivot_longer().\nVariablen\nF√ºr die Grafik ist es relevant, welches Skalenniveau die zu visualisierenden Variablen haben. Je nach Anzahl Variablen und den entsprechenden Skalenniveaus eignen sich andere Grafik-Formen.\n\n\nCC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=724035\n\n\n\n\n\n\n\nHands-on: Datensatz anschauen\n\n\n\nSchauen Sie sich den Datensatz an.\n\nWie viele unterschiedliche Variablen gibt es?\nWie heissen die Variablen?\nWelches Skalenniveau haben sie?\n\n\n\nSubsetting\nWenn nur ein gewisser Teil der Daten visualisiert werden soll, muss der Datensatz gefiltert werden. Der aktuelle Datensatz enth√§lt beispielsweise verschiedene Bedingungen, jeweils mit Werten f√ºr Variable value1 und value2. Folgende 13 Bedingungen sind enthalten:\n\nunique(d$condition)\n\n [1] away       bullseye   circle     dino       dots       h_lines   \n [7] high_lines slant_down slant_up   star       v_lines    wide_lines\n[13] x_shape   \n13 Levels: away bullseye circle dino dots h_lines high_lines ... x_shape\n\n\nF√ºrs erste entscheiden wir uns f√ºr die Bedingung away.\n\nd_away &lt;- d %&gt;%\n    filter(condition == \"away\")\n\nWir k√∂nnen f√ºr diese Bedingung zus√§tzlich summary statistics berechnen, hier Mittelwert und Standardabweichung.\n\nd_away_summary &lt;- d_away %&gt;%\n    summarise(mean_v1 = mean(value1),\n              sd_v1 = sd(value1),\n              mean_v2 = mean(value2),\n              sd_v2 = sd(value2))\n\nglimpse(d_away_summary)\n\nRows: 1\nColumns: 4\n$ mean_v1 &lt;dbl&gt; 54.2661\n$ sd_v1   &lt;dbl&gt; 16.76982\n$ mean_v2 &lt;dbl&gt; 47.83472\n$ sd_v2   &lt;dbl&gt; 26.93974\n\n\nDiese Werte geben einen Anhaltspunkt, in welchem Bereich sich die Werte bewegen werden.\nPlot\nIn den folgenden Beispielen werden die Daten der Bedingung away verwendet. Als erstes Argument wird der Funktion ggplot() der Datensatz √ºbergeben (data = data_away).\n\nggplot(data = d_away)"
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#mapping",
    "href": "pages/chapters/data_visualization_1.html#mapping",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Mapping",
    "text": "Mapping\nDas mapping beschreibt, welche Variable auf der x- und y-Achse abgetragen werden sollen. Es wird also definiert, wie die Variablen auf die Formen (aesthetics) gemappt werden sollen. Am einfachsten wir dies zu Beginn in festgelegt (das mapping kann aber auch in der Funktion geom_ selbst definiert werden). Weitere Variablen k√∂nnten als group = ... oder color = ... eingef√ºgt werden.\n\nggplot(data = d_away,\n       mapping = aes(x = value1,\n                     y = value2)) \n\n\n\n\nDie Grafik verf√ºgt nun √ºber Achsen, diese werden automatisch mit den Variablennamen beschriftet. Da noch keine Formen (geoms) hinzugef√ºgt wurde ist die Grafik aber leer."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#geom-formen",
    "href": "pages/chapters/data_visualization_1.html#geom-formen",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Geom / Formen",
    "text": "Geom / Formen\nAls dritte Komponente werden in ggplot() wird die Form mit geom_ hinzugef√ºgt. Jede Form, die eingef√ºgt wird ben√∂tigt Angaben zum mapping, falls kein mapping angegeben wird, wird dieses aus der ggplot()-Funktion in der ersten Zeile √ºbernommen.\nEs gibt viele verschiedene Formen zur Auswahl. Beispielsweise werden mit geom_point() Punkte erstellt, mit geom_line() Linien, mit geom_boxplot Boxplots, usw. Bei der Wahl der passenden Form kommt es einerseits auf die Daten an. Sind die Daten z.B. Faktoren oder numerische Werte (siehe auch Skalenniveau oben)? Wie viele Variablen werden gleichzeitig in die Grafik eingebunden? Andererseits ist es wichtig, was mit der Grafik gezeigt werden soll: Unterschiede? Gemeinsamkeiten? Ver√§nderungen √ºber Zeit?\nGeome zur Visualisierung von Datenpunkten und Verl√§ufen:\n\nPunkte / Scatterplots - geom_point()\n\nLinien - geom_line()\n\n\nGeome zur Visualisierung von zusammenfassenden Werten:\n\nHistogramme - geom_histogram()\n\nMittelwerte und Standardabweichungen - geom_pointrange()\n\nDichteplots - geom_density()\n\nBoxplots - geom_boxplot()\n\nViolinplots - geom_violin()\n\n\n\nEs gibt auch weitere, sehr informative Arten der Visualisierung, wie heat maps oder shift functions, auf die wir in dieser Veranstaltung nicht eingehen.\n\n\n\n\n\n\nHands-on: Geoms\n\n\n\nWelche geoms eignen sich f√ºr welches Skalenniveau und welche Variablenanzahl?\nüëâ Hier finden Sie das ggplot-Cheatsheet.\nSchauen Sie sich die verschiedenen Formen von Plots hier an."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#kombinieren-von-mehreren-geoms-in-einer-grafik",
    "href": "pages/chapters/data_visualization_1.html#kombinieren-von-mehreren-geoms-in-einer-grafik",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Kombinieren von mehreren geoms in einer Grafik",
    "text": "Kombinieren von mehreren geoms in einer Grafik\nTeilweise werden in Visualisierungen mehrere geoms kombiniert. In vielen F√§llen macht es beispielsweise Sinn, nicht nur die tats√§chlichen Werte, sondern in derselben Grafik auch zusammenfassende Masse zu visualisieren.\n\nWeiterf√ºhrende Info zum Kombinieren von Plots finden Sie hier.\nVerwenden verschiedener geoms in einem Plot:\n\nggplot(data = d_away, \n       mapping = aes(x = condition,\n                     y = value2)) +\n    geom_boxplot(width = 0.3) +\n    geom_jitter(width = 0.1) \n\n\n\n\nKombiniert werden k√∂nnen aber nicht nur verschiedene Formen, sondern auch mehrere Datens√§tze. Dies kann in ggplot() einfach umgesetzt werden indem mehrere Geoms √ºbereinandergelegt werden und nicht das mapping aus der ggplot()-Funktion genutzt wird, sondern indem f√ºr jedes geom ein separater Datensatz und ein separates mapping spezifiziert werden.\n\nggplot(data = d_away, \n       mapping = aes(x = condition,\n                     y = value2)) +\n    geom_jitter(width = 0.1) + # verwendet Datensatz \"d_away\"\n    geom_point(data = d_away_summary, # verwendet Datensatz \"d_away_summary\"\n               aes(x = \"away\", y = mean_v1), # condition ist nicht im Datensatz enthalten, deshalb hier hardcoded\n               color = \"red\", # Punkt rot einf√§rben\n               size = 3) # Punkt vergr√∂ssern"
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#beschriftungen-und-themes",
    "href": "pages/chapters/data_visualization_1.html#beschriftungen-und-themes",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Beschriftungen und Themes",
    "text": "Beschriftungen und Themes\nSch√∂nere und informativere Plots lassen sich gestalten, wenn wir einen Titel hinzuf√ºgen, die Achsenbeschriftung anpassen und das theme ver√§ndern:\n\nggplot(data = d_away,\n       mapping = aes(x = value1,\n                     y = value2)) +\n    geom_point() +\n    labs(title = \"Ein etwas sch√∂nerer Plot\", \n         subtitle = \"Verteilung der Rohwerte\",\n        x = \"Wert 1  [a.u.]\",\n        y = \"Wert 2 [a.u.]\") +\n    theme_minimal()\n\n\n\n\n\nAuch theme_classic() oder theme_bw() ergeben schlichte aber sch√∂ne Plots.\n\n\n\n\n\n\nHands-on\n\n\n\nErstellen Sie eine Grafik. F√ºgen Sie mit labs() passende Beschriftungen hinzu. Gibt es noch weitere, oben nicht verwendete Optionen?"
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#mehrere-plots-in-einer-grafik-darstellen",
    "href": "pages/chapters/data_visualization_1.html#mehrere-plots-in-einer-grafik-darstellen",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Mehrere Plots in einer Grafik darstellen",
    "text": "Mehrere Plots in einer Grafik darstellen\nDies k√∂nnen Sie mit dem Package patchwork sehr einfach machen. Sie finden oben oder hier ein Beispiel."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#grafik-abspeichern",
    "href": "pages/chapters/data_visualization_1.html#grafik-abspeichern",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Grafik abspeichern",
    "text": "Grafik abspeichern\nEine Grafik l√§sst sich abspeichern unter dem Reiter Plots &gt; Export oder mit der Funktion ggsave()."
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#inspiration",
    "href": "pages/chapters/data_visualization_1.html#inspiration",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Inspiration",
    "text": "Inspiration\n\nGrafiken f√ºr verschiedene Datenarten: From Data to Viz\nSimple bis crazy Chartideen: R Charts: Ggplot\nFarben f√ºr Grafiken: R Charts: Colors, noch mehr Farben"
  },
  {
    "objectID": "pages/chapters/data_visualization_1.html#weiterf√ºhrende-ressourcen-zur-datenvisualisierung-mit-ggplot",
    "href": "pages/chapters/data_visualization_1.html#weiterf√ºhrende-ressourcen-zur-datenvisualisierung-mit-ggplot",
    "title": "Grafiken erstellen mit ggplot\n",
    "section": "Weiterf√ºhrende Ressourcen zur Datenvisualisierung mit ggplot()\n",
    "text": "Weiterf√ºhrende Ressourcen zur Datenvisualisierung mit ggplot()\n\n\nDokumentation von ggplot2\nKurzweilige, kompakte und sehr informative Informationen und Videos √ºber das Erstellen von Grafiken in ggplot finden Sie hier: Website PsyTeachR: Data Skills for reproducible research\nHier ist der Start der PsyTeachR Videoliste von Lisa deBruine, dort finden sich auch hilfreiche Kurzvideos zu Themen von Daten einlesen bis zu statistischen Analysen. Beispielsweise zu Basic Plots, Common Plots und Plot Themes and Customization\nEinf√ºhrung in R von Andrew Ellis und Boris Mayer"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html",
    "href": "pages/chapters/data_visualization_2.html",
    "title": "Daten Visualisierung",
    "section": "",
    "text": "Datenvisualisierung ist ein wichtiger Schritt in der Analyse neurowissenschaftlicher Daten. Das grafische Darstellen von Informationen dient dazu die Datenkomplexit√§t zu reduzieren und wichtige Eigenschaften herauszuheben und zusammenzufassen.\nDabei geht es nicht nur darum Ergebnisse zu kommunizieren, sondern auch dazu Einsichten √ºber die Daten zu gewinnen: Auch wenn in den meisten wissenschaftlichen Artikeln nur wenige Grafiken gezeigt werden, wurden die Daten oft w√§hrend der Analyse zahlreiche Male visualisiert.\nWir schauen uns drei Kernaufgaben der Datenvisualisierung an:\nJe nachdem welchem Zweck eine Grafik dienen soll, m√ºssen andere Grafikeigenschaften ber√ºcksichtigt werden. Diagnostische Grafiken m√ºssen beispielsweise nicht in erster Linie √§sthetisch ansprechend sein, sondern aufzeigen, wo Probleme im Datensatz vorliegen k√∂nnten. Eine ‚Äúgute‚Äù Grafik komprimiert die Information in den Daten so, dass Erkenntnisse gewonnen werden k√∂nnen.\n# Package laden\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Daten einlesen und Textvariablen zu Faktoren umwandeln\nd &lt;- read_csv(\"data/dataset_rdk_clean.csv\") |&gt;\n    mutate(across(where(is.character), as.factor))\n\n# Datensatz anschauen\nd |&gt;\n    slice_head(n = 10)\n\n# A tibble: 10 √ó 8\n   id           trial direction condition corrAns resp   corr    rt\n   &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 sub-10209782     1 left      speed     left    right     0 5.88 \n 2 sub-10209782     2 right     speed     right   right     1 0.822\n 3 sub-10209782     3 left      speed     left    left      1 0.935\n 4 sub-10209782     4 right     speed     right   right     1 0.745\n 5 sub-10209782     5 right     speed     right   right     1 1.51 \n 6 sub-10209782     6 left      speed     left    left      1 0.940\n 7 sub-10209782     7 right     speed     right   right     1 1.64 \n 8 sub-10209782     8 left      speed     left    left      1 2.17 \n 9 sub-10209782     9 right     speed     right   right     1 1.38 \n10 sub-10209782    10 left      speed     left    left      1 1.55"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#fehlende-werte",
    "href": "pages/chapters/data_visualization_2.html#fehlende-werte",
    "title": "Daten Visualisierung",
    "section": "Fehlende Werte",
    "text": "Fehlende Werte\nHierbei ist es wichtig, vor allem systematisch fehlende Datenpunkte zu entdecken: Fehlt bei einer Person die H√§lfte der Antworten? M√∂chten wir diese ausschliessen?\nDiese k√∂nnen mit dem Package naniar relativ schnell sichtbar gemacht werden.\n\nnaniar::vis_miss(d)\n\n\n\n\n\nBevor Sie das Package verwenden k√∂nnen, m√ºssen Sie dies erst herunterladen. Sie k√∂nnen dies unter dem Reiter Tools &gt; Install Packages ... tun oder in der Konsole mit install.packages(\"naniar\").\n\n\n\n\n\n\nHands-on: Missings\n\n\n\n\nWas sehen Sie in der Grafik? (Wie viele Datenpunkte fehlen? In welchen Variablen?)\n\nUm mehr √ºber die fehlenden Werte zu erfahren, k√∂nnen wir uns die betroffenen Zeilen anschauen. Das kann direkt im Datensatz betrachtet werden oder indem eine zus√§tzliche Variable mit naniar erstellt wird, die angibt ob in dieser Zeile missings vorliegen:\n\nd_missings &lt;- d |&gt; naniar::add_label_missings() |&gt;\n    filter(any_missing == \"Missing\")\n\nhead(d_missings)\n\n# A tibble: 6 √ó 9\n  id           trial direction condition corrAns resp   corr    rt any_missing\n  &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 sub-16853779     1 right     accuracy  right   right     1    NA Missing    \n2 sub-16853779     2 left      accuracy  left    left      1    NA Missing    \n3 sub-16853779     3 right     accuracy  right   left      0    NA Missing    \n4 sub-16853779     4 left      accuracy  left    right     0    NA Missing    \n5 sub-16853779     5 right     accuracy  right   right     1    NA Missing    \n6 sub-16853779     6 left      accuracy  left    left      1    NA Missing    \n\n\n\nWas k√∂nnte die Ursache f√ºr die Missings sein?\nWas ist zu tun?\n\n\n\n\n\n\n\n\n\nFehlende Werte\n\n\n\n\n\nJe nachdem wie die fehlenden Werte zustande gekommen sind, gehen wir anders vor. Ein Ansatz k√∂nnte sein, dass wir die Trials, die keine Reaktionszeiten enthalten rausl√∂schen:\n\nd &lt;- d |&gt;\n    filter(rt != \"NA\")\n\nnaniar::vis_miss(d)\n\n\n\n\nDrei wichtige Punkte:\n\nWir l√∂schen die Datenpunkte nie aus den Rohdaten, sondern nur aus dem aktuell geladenen Datensatz, den wir f√ºr die Analysen verwenden. So k√∂nnen wir uns immer noch umentscheiden und verlieren nicht die Information, welche Daten gefehlt haben.\nDadurch, dass wir die Datenverarbeitung in reproduzierbarem Code geschrieben haben, konnten wir √ºberpr√ºfen, ob ein Fehler in unserer Datenverarbeitung zu den missings gef√ºhrt hat und diesen evtl. korrigieren.\nEs macht nicht immer Sinn die Trials mit missing data zu l√∂schen! Dies muss von Fall zu Fall entschieden werden. Wenn Versuchspersonen zum Beispiel teilweise zu lange brauchten um eine Aufgabe zu l√∂sen, dann ist das eine wichtige Information, wird diese rausgel√∂scht, wird die Leistung der Versuchsperson systematisch √ºbersch√§tzt.\n\n\n\n\nWir berechnen nur f√ºr die kommenden Grafiken die Anzahl Trials pro Person, die accuracy, sowie die mittlere Reaktionszeit (wie im Kapitel Aggregierte Statistiken beschrieben).\nWir schliessen vorher alle Reaktionszeiten unter 100ms und √ºber 10 Sekunden aus.\n\n# zu schnelle und zu langsame Antworten ausschliessen\nd &lt;- d |&gt;\n    filter(rt &gt; 0.09 & rt &lt; 15)\n\n# Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\nNachdem wir Trials ohne Antwort ausgeschlossen haben, interessiert es uns, wie viele Trials jede Versuchsperson gel√∂st hat:\n\n# Plot: Anzahl Trials pro Bedingung f√ºr jede Versuchsperson \nacc_rt_individual |&gt; \n    ggplot(aes(x = id, y = N)) +\n    geom_point() +\n    facet_wrap(~ condition) +\n    geom_hline(yintercept = 40) + # Horizontale Linie einf√ºgen\n    theme_minimal()\n\n\n\n\nWir schliessen alle Personen aus, die weniger als 40 g√ºltige Trials hatten\n\n# Datensatz mit allen Ids, welche zuwenig Trials hatten\nn_exclusions &lt;- acc_rt_individual |&gt;\n    filter(N &lt; 40) \n\n# Aus dem Hauptdatensatz diese Ids ausschliessen\nd &lt;- d |&gt;\n    filter(!id %in% n_exclusions$id) \n\n# Check\nd_acc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nd_acc_rt_individual |&gt; \n    ggplot(aes(x = id, y = N)) +\n    geom_point() +\n    facet_wrap(~ condition) +\n    geom_hline(yintercept = 40) + # Horizontale Linie einf√ºgen\n    theme_minimal()"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#extreme-datenpunkte-ausreisser",
    "href": "pages/chapters/data_visualization_2.html#extreme-datenpunkte-ausreisser",
    "title": "Daten Visualisierung",
    "section": "Extreme Datenpunkte (Ausreisser)",
    "text": "Extreme Datenpunkte (Ausreisser)\nWir k√∂nnen Visualisierungen auch verwenden, um extreme Datenpunkte zu identifizieren. Daf√ºr teilen wir hier die Accuracywerte in 3 Gruppen ein und plotten diese:\n\n# Trials nach accuracy einteilen\nd_acc_rt_individual_grouped &lt;- d_acc_rt_individual %&gt;% \n  mutate(\n    performance = case_when(\n      accuracy &gt; 0.75 ~ \"good\",\n      accuracy &lt; 0.4 ~ \"bad\",\n      TRUE ~ \"chance level\") %&gt;% \n      factor(levels = c(\"good\", \"chance level\", \"bad\")))\n\n# Outlier visualisieren\nd_acc_rt_individual_grouped %&gt;% \n    ggplot(aes(x = id, y = accuracy, color = performance, shape = performance)) +\n    geom_point(size = 2, alpha = 0.6) + \n    geom_point(data = filter(d_acc_rt_individual_grouped, performance != \"OK\"), \n               alpha = 0.9) + \n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray40\", \"steelblue\", \"red\")) +\n    geom_hline(yintercept = 0.5, linetype='dotted', col = 'black')+\n    annotate(\"text\", x = \"sub-36817827\", y = 0.5, label = \"chance level\", vjust = -1, size = 3) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\nDasselbe k√∂nnte man f√ºr die Reaktionszeiten machen. Informationen dazu, wie Ausreisser in Reaktionszeiten gefunden und visualisiert werden k√∂nnen, finden Sie hier."
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#verlaufseffekte-erm√ºdung-und-lernen",
    "href": "pages/chapters/data_visualization_2.html#verlaufseffekte-erm√ºdung-und-lernen",
    "title": "Daten Visualisierung",
    "section": "Verlaufseffekte: Erm√ºdung und Lernen",
    "text": "Verlaufseffekte: Erm√ºdung und Lernen\nVerlaufseffekte k√∂nnen uns interessieren, weil wir starke Erm√ºdungs- oder Lerneffekte ausschliessen m√∂chten. Sie k√∂nnten aber auch inhaltlich interessant sein.\nIn unserem Experiment m√∂chten wir sicher sein, dass die Performanz sich √ºber die Zeit hinweg nicht zu stark ver√§ndert. Hierzu k√∂nnen wir beispielsweise die accuracy in den beiden Bl√∂cken plotten:\n\nd_acc_rt_trial &lt;- d |&gt;\n    group_by(condition, trial) |&gt;\n    summarise(\n        accuracy = mean(corr),\n        median_rt = median(rt)\n        )\n\nd_acc_rt_trial |&gt;\n    ggplot(aes(x = trial, y = accuracy, color = condition)) +\n    geom_point(size = 2, alpha = 0.8) +\n    geom_line() +\n    scale_color_manual(values = c(accuracy = \"tomato3\",\n                                  speed = \"skyblue3\")) +\n    facet_wrap(~ condition) +\n    theme_minimal(base_size = 12)\n\n\n\n\nOder wir k√∂nnen die Reaktionszeiten √ºber die Zeit hinweg anschauen.\n\nd_acc_rt_trial |&gt;\n    ggplot(aes(x = trial, y = median_rt, color = condition)) +\n    geom_point(size = 2, alpha = 0.8) +\n    geom_line() +\n    scale_color_manual(values = c(accuracy = \"tomato3\",\n                                  speed = \"skyblue3\")) +\n    facet_wrap(~ condition) +\n    theme_minimal(base_size = 12)\n\n\n\n\nDas tun wir hier f√ºr 3 Versuchspersonen.\n\n# Plot: Reaktionszeit √ºber die Trials hinweg (f√ºr 3 Versuchspersonen)\nd |&gt;\n    filter(id %in% c(\"sub-10209782\", \"sub-36817827\", \"sub-48177911\")) |&gt;\n    ggplot(aes(x = trial, y = rt, color = condition)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_point(alpha = 0.5) +\n    scale_color_manual(values = c(accuracy = \"tomato3\",\n                                 speed = \"skyblue3\")) +\n    facet_wrap(~ id) +\n    theme_minimal()"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#aufgabenschwierigkeit-und-performanz-der-versuchspersonen",
    "href": "pages/chapters/data_visualization_2.html#aufgabenschwierigkeit-und-performanz-der-versuchspersonen",
    "title": "Daten Visualisierung",
    "section": "Aufgabenschwierigkeit und Performanz der Versuchspersonen",
    "text": "Aufgabenschwierigkeit und Performanz der Versuchspersonen\nBevor wir die Daten analysieren, m√∂chten wir wissen, ob die Personen die Aufgabe einigermassen gut l√∂sen konnten (und wollten). In unserem Experiment erwarten wir eine Genauigkeit (accuracy) √ºber dem Rateniveau von 50%. Wir plotten hierf√ºr die accuracy f√ºr jede Person und Bedingung.\n\n# Plot accuracy per person and condition\np1 &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(size = 3, alpha = 0.4, \n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(accuracy = \"tomato2\",\n                                 speed = \"skyblue3\")) +\n    labs(x = \"Instruction\",\n         y = \"Proportion correct\",\n         title = \"Accuracy per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np2 &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = median_rt, color = condition)) +\n    geom_jitter(size = 3, alpha = 0.4, \n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(accuracy = \"tomato2\",\n                                 speed = \"skyblue3\")) +\n    labs(x = \"Instruction\",\n         y = \"Median Response Time [s]\",\n         title = \"Median Response Time per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\nUnd wir interessieren uns, wie sich die accuracy zwischen den Bedingungen unterscheidet. Das zeigt uns, ob die Instruktion eine Wirkung hatte. Daf√ºr f√ºgen wir Linien ein, die die accuracy- Werte pro Versuchsperson verbindet:\n\np3 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = condition, y = accuracy, color = condition, group = id)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8, \n                width = 0, height = 0) +\n    scale_color_manual(values = c(accuracy = \"tomato2\",\n                                 speed = \"skyblue3\")) +\n    labs(x = \"Instruction\",\n         y = \"Proportion correct\",\n         title = \"Accuracy per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np4 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = condition, y = median_rt, color = condition, group = id)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8, \n                width = 0, height = 0) +\n    scale_color_manual(values = c(accuracy = \"tomato2\",\n                                 speed = \"skyblue3\")) +\n    labs(x = \"Instruction\",\n         y = \"Median Response Time [s]\",\n         title = \"Median Response Time per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np3 + p4\n\n\n\n\n\n\n\n\n\n\nHands-on: Datenqualit√§t\n\n\n\nBesprechen Sie miteinander, was Sie nun √ºber unsere Daten wissen.\n\nHaben die Versuchspersonen die Aufgaben l√∂sen k√∂nnen?\nWar die Aufgabe zu einfach, zu schwierig?\nDenken Sie, die Personen waren motiviert?\nWelche Datens√§tze / Trials m√∂chten wir ausschliessen? (Dies m√ºsste eigentlich vor dem Anschauen der Daten entschieden werden, um zu verhindern, dass man Datenpunkte ausschliesst, welche die Hypothese nicht best√§tigen.)\nWie gut eignen sich die Daten, um die Forschungsfrage zu beantworten?\nWas k√∂nnte bei einem n√§chsten Experiment besser gemacht werden?"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#verteilung-der-rohdaten",
    "href": "pages/chapters/data_visualization_2.html#verteilung-der-rohdaten",
    "title": "Daten Visualisierung",
    "section": "Verteilung der Rohdaten",
    "text": "Verteilung der Rohdaten\nDaten von neurowissenschaftlichen Studien k√∂nnen wichtige Informationen enthalten, die ohne Grafiken √ºbersehen werden k√∂nnen (Rousselet, Pernet, and Wilcox (2017)). Das Visualisieren kann Muster zum Vorschein bringen, die durch statistische Auswertungen nicht sichtbar sind. Die Wichtigkeit von Datenvisualisierung f√ºr das Entdecken von Mustern in den Daten zeigte Francis Anscombe 1973 mit dem Anscombe‚Äôs Quartet. Dies diente als Inspiration f√ºr das Erstellen des ‚Äúk√ºnstlichen‚Äù Datensatzes DatasaurusDozen, welchen wir in der letzten Veranstaltung visualisiert haben. Verschiedene Rohwerte, k√∂nnen dieselben Mittelwerte, Standardabweichungen und Korrelationen ergeben. Nur wenn man die Rohwerte plottet erkennt man, wie unterschiedlich die Datenpunkte verteilt sind.\nDies wird ersichtlich, wenn wir die Mittelwerte und Standardabweichungen f√ºr jede Gruppe berechnen und plotten:\n\n# load DatasaurusDozen dataset\ndino_data &lt;- read.csv(\"data/DatasaurusDozen.csv\") %&gt;%\n    mutate(condition = as.factor(condition))\n\n# Plot mean and standard deviation for value 1 per condition \ndino_data |&gt;   \n    group_by(condition) |&gt;\n    summarise(mean_value1 = mean(value1),\n              sd_value1 = sd(value1)) |&gt;\n    ggplot(mapping = aes(x = mean_value1,\n                     y = condition)) +\n    geom_point() +\n    geom_errorbar(aes(xmin = mean_value1 - sd_value1, \n                      xmax = mean_value1 + sd_value1), \n                  width = 0.2) +\n    theme_minimal()\n\n\n\n\nUnd dann die Rohwerte visualisieren:\n\n# Plot raw values\ndino_data |&gt; \n    ggplot(aes(x = value1, y = value2)) +\n    geom_point(size = 1) +\n    facet_wrap(~condition) +\n    theme_minimal()\n\n\n\n\nHier sehen Sie das Ganze animiert:\n\n\nDatensatz und Visualisierung von Matejka and Fitzmaurice (2017)"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#zentrale-tendenz-und-verteilungsmasse",
    "href": "pages/chapters/data_visualization_2.html#zentrale-tendenz-und-verteilungsmasse",
    "title": "Daten Visualisierung",
    "section": "Zentrale Tendenz und Verteilungsmasse",
    "text": "Zentrale Tendenz und Verteilungsmasse\nMasse der zentralen Tendenz sind beispielsweise der Mittelwert, der Median und Modus. Wenn wir uns daf√ºr interessieren, wie sich die accuracy in Bezug auf alle Teilnehmenden verh√§lt, schauen wir uns die zentrale Tendenz √ºber alle Personen hinweg an. Es sollte nie nur die zentrale Tendenz, sondern immer auch ein passendes Verteilungsmass berichtet werden.\nWie oben schon gezeigt k√∂nnen wir dies z.B. mit Boxplots umsetzen Diese zeigen uns den Median und die Quartile sowie Ausreisser an. Eine andere M√∂glichkeit Verteilungen anzuzeigen sind die Violinplots. Hier wurden mit geom_jitter() auch die Mittelwerte der einzelnen Personen im Plot eingef√ºgt.\n\n# Boxplot\np_boxplot &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(alpha = 0.25, width = 0.2) +\n    geom_boxplot(alpha = 0, width = 0.2, color = \"black\") +\n    scale_fill_manual(values = c(accuracy = \"tomato3\",\n                                     speed = \"skyblue3\")) +\n    labs(title = \"Boxplot\",\n         x = \"Instruction\",\n         y = \"Proportion correct\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n# Violin Plot\np_violin &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(alpha = 0.5, width = 0.2) +\n    geom_violin(alpha = 0, width = 0.2, color = \"black\") +\n    scale_fill_manual(values = c(accuracy = \"tomato3\",\n                                 speed = \"skyblue3\")) +\n    labs(title = \"Violin Plot\",\n         x = \"Instruction\",\n         y = \"Proportion correct\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np_boxplot + p_violin\n\n\n\n\nDas Package ggridges bietet die M√∂glichkeit die Verteilungen zu plotten. Mehr Informationen hierzu finden Sie hier in der Dokumentation.\n\nlibrary(ggridges)\np5 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = accuracy, y = condition, fill = condition)) + geom_density_ridges2(scale = 0.5, alpha = 0.5) +\n    scale_fill_manual(values = c(accuracy = \"tomato3\",\n                                 speed = \"skyblue3\")) +\n    labs(title = \"Accuracy\",\n         x = \"Proportion corect\",\n         y = \"Instruction\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np6 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = median_rt, y = condition, fill = condition)) + geom_density_ridges2(scale = 1, alpha = 0.5) +\n    scale_fill_manual(values = c(accuracy = \"tomato3\",\n                                 speed = \"skyblue3\")) +\n    labs(title = \"Median Response Time\",\n         x = \"Median Response Time [s]\",\n         y = \"Instruction\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np5 + p6"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#aggregierte-statistiken",
    "href": "pages/chapters/data_visualization_2.html#aggregierte-statistiken",
    "title": "Daten Visualisierung",
    "section": "Aggregierte Statistiken",
    "text": "Aggregierte Statistiken\nWenn wir Mittelwerte und Standardfehler angeben m√∂chten k√∂nnen wir dies wie folgt tun. Wichtig ist hier, dass wir within-subject Standardfehler berechnen. Genaueres zu den Unterschieden zwischen within- und between-subject Standardfehlern finden Sie hier. Wir verwenden das Package Rmisc, da dieses jedoch wegen der Namensgebung oft Konflikte ausl√∂st, laden wir Rmisc nicht mit der library()-Funktion, sondern stellen es einfach vor die ben√∂tigte\n\nd_acc_within &lt;- d |&gt;\n    Rmisc::summarySEwithin(measurevar = \"corr\",\n                               withinvars = \"condition\",\n                               idvar = \"id\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\np7 &lt;- d_acc_within |&gt;\n    ggplot(aes(x = condition, y = corr, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = corr-se, ymax = corr+se)) +\n    geom_point(size = 3) +\n    labs(title = \"Accuracy\",\n         x = \"Instruction\",\n         y = \"Accuracy\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n\nd_rt_within &lt;- d |&gt;\n    Rmisc::summarySEwithin(measurevar = \"rt\",\n                               withinvars = \"condition\",\n                               idvar = \"id\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\np8 &lt;- d_rt_within |&gt;\n    ggplot(aes(x = condition, y = rt, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = rt-se, ymax = rt+se)) +\n    geom_point(size = 3) +\n    labs(title = \"Median Response Time\",\n         x = \"Instruction\",\n         y = \"Median Response Time [s]\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np7 + p8\n\n\n\n\n\nHier finden Sie weitere Code-Beispiele f√ºr das Plotten von Verteilungsmassen.\nHier finden Sie Informationen, wie Reaktionszeiten zusammengefasst und visualisiert werden k√∂nnten."
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#visualisieren-von-modellsch√§tzungen",
    "href": "pages/chapters/data_visualization_2.html#visualisieren-von-modellsch√§tzungen",
    "title": "Daten Visualisierung",
    "section": "Visualisieren von Modellsch√§tzungen",
    "text": "Visualisieren von Modellsch√§tzungen\nWenn f√ºr die statistische Analyse ein Modell gesch√§tzt wurde, kann auch dies visualisiert werden. Auf diese Form der Visualisierung wird hier aber nicht eingegangen. Wir lernen dies im Rahmen der noch kommenden Versanstaltungen kennen.\n\nsjPlot: Package zum Plotten von Fixed Effects\nsee: Package zum Visualisieren von Statistischen Modellen"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#beschriftungen",
    "href": "pages/chapters/data_visualization_2.html#beschriftungen",
    "title": "Daten Visualisierung",
    "section": "Beschriftungen",
    "text": "Beschriftungen\nDie genaue Beschriftung und deren Lesbarkeit ist f√ºr diese Form von Grafiken zentral. Achten Sie sich auf Folgendes:\n\nDie Achsenbeschriftungen enthalten die verwendete Variable in Klartext (nicht den R Variablennamen) und wenn zutreffend auch die Masseinheit (z.B. Response Time [ms]). Beschriftungen k√∂nnen Sie einf√ºgen mit labs().\n\n\np_boxplot +\nlabs(title = \"Der Titel der Grafik\", \n     subtitle = \"Der Subtitel der Grafik\",\n     x = \"hier kommt Label x [Masseinheit]\", \n     y = \"hier kommt Label y [Masseinheit]\",\n     caption = \" Hier kommt eine Caption\")\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\nFarben / Formen usw. werden in einer Legende den Gruppen zugeordnet (Ausnahme: wenn Daten von einzelnen Personen geplottet werden, wird die Versuchspersonennummer nicht aufgef√ºhrt).\nMasse der zentralen Tendenz und Varianzmasse werden beschrieben (z.B. Standardfehler oder Standardabweichung?)"
  },
  {
    "objectID": "pages/chapters/data_visualization_2.html#merkmale-einer-guten-grafik",
    "href": "pages/chapters/data_visualization_2.html#merkmale-einer-guten-grafik",
    "title": "Daten Visualisierung",
    "section": "5 Merkmale einer guten Grafik",
    "text": "5 Merkmale einer guten Grafik\nEs gibt unz√§hlige Optionen die eigenen Daten zu visualisieren. Folgende Prinzipien helfen beim Erstellen einer informativen Grafik, die zur Kommunikation der Ergebnisse dient.\n\nDie Punkte 3-5 wurden aus dem Buch ‚ÄúThe Visual Display of Quantitative Information‚Äù von Edward Tufte, 1986 entnommen: Link.\n1. Eine Frage beantworten\nJede Grafik sollte mindestens eine teilweise aber auch mehrere Fragen beantworten.\nüëâ Welche Frage m√∂chte ich beantworten? Welche Form der Visualisierung beantwortet diese Frage am besten?\nHierbei kann es hilfreich sein den ‚ÄúArbeitstitel‚Äù der Grafik als Frage zu formulieren.\n2. Zielgruppe ber√ºcksichtigen\nBeim Erstellen der Grafik sollte beachtet werden, an wen sich die Grafik richtet. F√ºr eine Pr√§sentation m√ºssen die Achsenbeschriftungen vergr√∂ssert und die Grafik simpel gehalten werden. In einem wissenschaftlichen Artikel kann die Grafik komplexer gestaltet werden, da die Lesenden sich mehr Zeit zum Anschauen nehmen k√∂nnen. Zudem sollten hier die Vorgaben des Journals ber√ºcksichtigt werden. Auch wichtig ist das Verwenden von ‚Äúfarbenblind-freundlichen‚Äù Palletten, rot und gr√ºn ist z.B. eine schlechte Wahl.\nüëâ F√ºr welchen Zweck / f√ºr wen erstelle ich die Grafik? Wie ist das Vorwissen des Zielpublikums?\n\nF√ºr einen Fachartikel lohnt es sich, zu Beginn die Vorgaben der Fachzeitschrift zu ber√ºcksichtigen.\n3. Die Daten zeigen\nDas t√∂nt simpel, wird aber oft nicht ber√ºcksichtigt. Bei einer Grafik geht es in erster Linie um die Daten. Es sollte die simpelste Form gew√§hlt werden, welche die Informationen vermittelt. Oft braucht es keine ausgefallenen Grafikideen oder neuartigen Formate. Hierbei ist es wichtig, die Art der Daten zu ber√ºcksichtigen: Wie viele Variablen sind es? Sind diese kontinuierlich (z.B. Reaktionszeiten) oder diskret (z.B. Experimentalbedingungen)? Wie viele Dimensionen haben meine Daten? Mit zwei Achsen lassen sich zwei Dimensionen darstellen, zus√§tzlich k√∂nnen mit Farben und Formen noch weitere Dimensionen abgebildet werden (z.B. Millisekunden, Bedingung 1 und Bedingung 2). Es k√∂nnen Rohwerte geplottet werden oder summary statistics (z.B. Mittelwerte, Standardabweichungen)\nüëâ Welche Art Grafik eignet sich f√ºr meine Frage und meine Daten? Schauen Sie z.B. hier nach oder nutzen Sie das esquisse-Package.\n\nBeispiele f√ºr verschiedenen Plots in R sind z.B. histogram, boxplot, violin plot, scatter plot / correlogram, jitter plot, raincloud plot, percentiles / shift functions, area chart, heat map.\n4. Optimieren des data-ink ratios\nDas Daten-Tinte-Verh√§ltnis sollte so optimal wie m√∂glich sein. Das bedeutet, das idealerweise jeder Strich, jeder Punkt, jedes Textfeld Information beinhaltet. Alles was keine Information transportiert oder nur wiederholt kann weggelassen werden.\nüëâ Was kann ich weglassen?\n\nIn R kann zum Schluss des Plots + theme_minimal() hinzugef√ºgt werden, dies entfernt u.a. den grauen Hintergrund. Das Grau des Hintergrunds ist Farbe (ink), welche keine Information transportiert, das Weglassen l√§sst die Grafik ruhiger wirken.\n5. Feedback einholen und revidieren\nDas Erstellen einer guten Grafik ist iterativ, das heisst, sie wird immer wieder √ºberarbeitet, bis sie die Information m√∂glichst einfach, genau aber klar kommuniziert. Hierbei ist Feedback oft unerl√§sslich.\nüëâ Was denken andere √ºber Ihre Grafik?"
  },
  {
    "objectID": "pages/chapters/datacamp.html",
    "href": "pages/chapters/datacamp.html",
    "title": "DataCamp",
    "section": "",
    "text": "DataCamp ist eine Online-Lernplattform, welche sich auf Data Science und Datenanalyse konzentriert. Es bietet interaktive Kurse, Tutorials und Projekte in verschiedenen Programmiersprachen wie Python, R und SQL auf unterschiedlichen Niveaus an; sowohl f√ºr Anf√§nger als auch f√ºr Fortgeschrittene gibt es ein breites Angebot.\nIm Rahmen dieser Lehrveranstaltung k√∂nnen alle Teilnehmenden sich unter folgendem Link mit ihrer Uni Bern E-Mail Adresse (*students.unibe.ch) registrieren:\nüëâüèº Einladungslink DataCamp Registration\nWir werden jeweils die empfohlenen Datacamp Kurse verlinken. Sie haben mit dem Link jedoch Zugriff auf alle Kurse bis Ende FS24.\nüëâüèº Zur Auffrischung von R-Kenntnissen eignet sich dieser Kurs: Introduction to R\nüëâüèº Als Einf√ºhrung in Python eignet sich folgender Kurs: Introduction to Python"
  },
  {
    "objectID": "pages/chapters/datacamp.html#datacamp",
    "href": "pages/chapters/datacamp.html#datacamp",
    "title": "DataCamp",
    "section": "",
    "text": "DataCamp ist eine Online-Lernplattform, welche sich auf Data Science und Datenanalyse konzentriert. Es bietet interaktive Kurse, Tutorials und Projekte in verschiedenen Programmiersprachen wie Python, R und SQL auf unterschiedlichen Niveaus an; sowohl f√ºr Anf√§nger als auch f√ºr Fortgeschrittene gibt es ein breites Angebot.\nIm Rahmen dieser Lehrveranstaltung k√∂nnen alle Teilnehmenden sich unter folgendem Link mit ihrer Uni Bern E-Mail Adresse (*students.unibe.ch) registrieren:\nüëâüèº Einladungslink DataCamp Registration\nWir werden jeweils die empfohlenen Datacamp Kurse verlinken. Sie haben mit dem Link jedoch Zugriff auf alle Kurse bis Ende FS24.\nüëâüèº Zur Auffrischung von R-Kenntnissen eignet sich dieser Kurs: Introduction to R\nüëâüèº Als Einf√ºhrung in Python eignet sich folgender Kurs: Introduction to Python"
  },
  {
    "objectID": "pages/chapters/datawrangling.html",
    "href": "pages/chapters/datawrangling.html",
    "title": "Daten importieren und vorverarbeiten",
    "section": "",
    "text": "In R gibt es sehr viele hilfreiche Funktionen und Packages, die f√ºr die Vorverarbeitung und Analyse neurowissenschaftlicher Verhaltensdaten (oder extrahierten Daten aus bildgebenden Verfahren) verwendet werden k√∂nnen. Beim Arbeiten in R empfiehlt es sich durchgehend in einem R-Markdown Dokument (.Rmd) innerhalb eines RStudio-Projects zu arbeiten, da so ein reproduzierbarer Workflow erstellt werden kann. Ein weiterer Vorteil ist zudem, dass Rohdaten nicht ver√§ndert werden m√ºssen.\nIllustration by Allison Horst"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#datenformate",
    "href": "pages/chapters/datawrangling.html#datenformate",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Datenformate",
    "text": "Datenformate\nBevor mit einem Datensatz gearbeitet wird, empfiehlt es sich den Datensatz anzuschauen und Folgendes zu identifizieren:\n\nIn welchem Dateiformat ist der Datensatz gespeichert? (z.B. in .csv, .xlsx oder anderen?)\nIn welchem Datenformat ist der Datensatz geordnet? (long oder wide oder mixed?)\nGibt es ein data dictionary mit Erkl√§rungen zu den Variablen?"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#set-up",
    "href": "pages/chapters/datawrangling.html#set-up",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Set up",
    "text": "Set up\n\n\n\n\n\n\nHands-on: Vorbereitung\n\n\n\n\n√ñffnen Sie RStudio.\n\nErstellen Sie ein neues RStudio-Project\n\nKlicken Sie daf√ºr auf File &gt; New Project\nBenennen Sie das Project complab_datawrangling_stroop und speichern Sie es an einem sinnvollen Ort auf Ihrem Computer. W√†hlen Sie, dass ein neuer Ordner erstellt werden soll.\n\n\nErstellen Sie in diesem Projekt-Ordner einen Ordner namens data.\nKopieren Sie in den data-Ordner Ihre erhobenen Daten des Stroop Experiments. Falls Sie noch keine Daten erhoben haben, dann laden Sie hier Beispiels-Datens√§tze herunter. Sie m√ºssen den Ordner entzippen und den Stroop-Datensatz in den data-Ordner Ihres Projekts kopieren.\nErstellen Sie ein neues .Rmd-File (File &gt; New File &gt; R Markdown) und speichern Sie dieses unter data-import im Projekt-Ordner.\n\n\n\n\n\n\n\n\n\nTipp: Namensgebung f√ºr Files und Variablen\n\n\n\nWenn Sie Filenamen ausw√§hlen, achten Sie darauf dass diese machine-readable sind:\n\nkeine L√ºcken (verwenden Sie stattdessen den camelCase, den snake_case oder -)\nkeine √§, √∂, √º oder andere Sonderzeichen verwenden"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#packages-installieren-und-laden",
    "href": "pages/chapters/datawrangling.html#packages-installieren-und-laden",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Packages installieren und laden",
    "text": "Packages installieren und laden\nF√ºr das Bearbeiten der Daten verwenden eignen sich Funktionen aus dem Package tidyverse, eine Sammlung von verschiedenen, f√ºr data science sehr geeigneten R Packages. Funktionen aus dem tidyverse erm√∂glichen und vereinfachen viele Schritte der Datenverarbeitung. Im Folgenden werden die wichtigsten und h√§ufigst verwendeten Funktionen beschrieben. Das tidyverse k√∂nnen Sie direkt in R herunterladen:\n\nMehr Informationen zum tidyversefinden Sie hier.\n\n# Download und Installieren des Packages (nur einmal ausf√ºhren)\ninstall.packages(\"tidyverse\")\n\nEin Package muss nur einmal heruntergeladen und installiert werden, danach ist es lokal auf dem Computer gespeichert. Aber: Jedes Mal wenn R ge√∂ffnet wird, m√ºssen Packages wieder neu geladen werden.\n\n# Package laden (bei jedem √ñffnen von R zu Beginn des Skripts ausf√ºhren)\nlibrary(\"tidyverse\") \n\nSobald ein Package installiert ist, k√∂nnen die Funktionen auch verwendet werden ohne, dass das ganze Package mit library() geladen wird, indem die Funktion mit dem Package-Namen zusammen aufgerufen wird: packagename::packagefunction(). Dies macht Sinn, wenn verschiedene Packages dieselben Namen f√ºr verschiedene Funktionen nutzen und es so zu Konflikten kommt oder wenn nur eine Funktion aus einem Package verwendet werden soll und alle anderen sowieso nicht gebraucht werden."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#daten-importieren-in-r-read.csv",
    "href": "pages/chapters/datawrangling.html#daten-importieren-in-r-read.csv",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Daten importieren in R: read.csv()\n",
    "text": "Daten importieren in R: read.csv()\n\nEinen Datensatz in .csv-Format kann mit der Funktion read.csv() importiert werden. Teilweise muss innerhalb der Klammer zus√§tzlich der Separator mit sep = \",\" angegeben werden, also mit welchem Zeichen die Spalten getrennt sind. Normalerweise ist dies , in .csv (comma separated values), es kann aber auch ;, . oder eine L√ºcke  sein.\n\n# Daten laden und anschauen\nd_stroop &lt;- read.csv(\"data/stroop_example.csv\", sep = \",\")\nglimpse(d_stroop)\n\n\n\n\n\n\n\nHands-on: Daten einlesen\n\n\n\nLesen Sie den Stroop Datensatz in Ihrem data-Ordner ein und schauen Sie ihn dann mit den Funktionen glimpse() und head() an.\n\nWelche Variablen sind wichtig f√ºr die weitere Auswertung?\nWelche braucht es wahrscheinlich nicht mehr?\nFinden Sie Versuchspersonenidentifikation? / Reaktionszeit? / Antwort der Versuchsperson?\n\n\n\n\n\n\n\n\n\nTipp: Daten anderer Formate einlesen\n\n\n\nFalls Sie eine Excel-Datei einlesen m√∂chten, k√∂nnen Sie dies mit der read_excel()-Funktion aus dem Package readxl() tun: readxl::read_excel().\nFalls Sie nicht wissen, mit welcher Funktion Sie Ihre Daten einlesen k√∂nnen, k√∂nnen Sie dies in RStudio ausprobieren indem Sie beim Reiter Environment auf Import Dataset klicken und dort Ihren Datensatz ausw√§hlen oder √ºber File &gt; Import Dataset. Sie k√∂nnen dort diverse Einstellungen t√§tigen. In der R Console k√∂nnen Sie dann den Code sehen, der zum Einlesen verwendet wurde und die dortige Funktion in Ihren Code kopieren."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#verwenden-der-pipe-oder",
    "href": "pages/chapters/datawrangling.html#verwenden-der-pipe-oder",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Verwenden der Pipe: |> oder %>%\n",
    "text": "Verwenden der Pipe: |&gt; oder %&gt;%\n\nIn R k√∂nnen Sie die Pipe verwenden um mehrere Datenverarbeitungsschritte aneinander zu h√§ngen. Damit sparen Sie sich aufw√§ndige Zwischenschritte und vermeiden das Erstellen von immer neuen Datens√§tzen. Statt zwei einzelne Datenverarbeitungsschritte zu machen wie oben, k√∂nnen mehrere Schritte (hier Daten einlesen und anzeigen) zusammengefasst werden, in dem nach Zeilenende eine Pipe eingef√ºgt wird:\n\nWann Pipes ungeeignet sind wird hier beschrieben.\n\nd_stroop &lt;- read.csv(\"data/stroop_example.csv\", sep = \",\") |&gt;\n    glimpse()\n\nDie Base R Pipe |&gt; und die magritter Pipe %&gt;%_ unterscheiden sich in Details, in unseren weiteren Schritten spielt es jedoch keine Rolle, welche Pipe Sie verwenden.\n\n\n\n\n\n\nTipp\n\n\n\nAchtung: Wenn wir zu Beginn ein &lt;- oder = verwenden, wird alles was nach der Pipe kommt wird ebenfalls im Datensatz ver√§ndert. Wird z.B. der Code ‚Ä¶\n\nd_stroop &lt;- read.csv(\"data/stroop_example1.csv\", sep = \",\") |&gt;\n    head()\n\n‚Ä¶eingegeben, besteht der Datensatz d_stroop dann nur noch aus 6 Zeilen.\nWird die Pipe ohne &lt;- oder = verwendet, bleibt der Datensatz unver√§ndert:\n\nd_stroop |&gt;\n    head()"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#daten-filtern-filter",
    "href": "pages/chapters/datawrangling.html#daten-filtern-filter",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Daten filtern: filter()\n",
    "text": "Daten filtern: filter()\n\nMit filter() k√∂nnen bestimmte Beobachtungen oder Untergruppen ausgew√§hlt werden. Hierf√ºr muss in der Funktion filter(.data, filter, ...) der Datensatz, die betreffende Variable, sowie eine Bedingung eingegeben werden. Es wird die ganze Zeile im Datensatz behalten in der die Variable der Bedingung entspricht.\nBeispiele:\n\n# nur die Trials mit den rot angezeigten W√∂rtern behalten\nd_stroop_filtered &lt;- filter(d_stroop, color == \"red\")\n\n# dasselbe mit der Pipe\nd_filtered &lt;- d_stroop |&gt; filter(color == \"red\")\n\n# nur Trials die ohne blau angezeigten W√∂rter behalten\nd_filtered &lt;- d_stroop |&gt; filter(color != \"blue\")\n\n# nur √úbungstrials mit einer Antwortszeit unter oder gleich gross wie 1 Sekunde behalten\nd_filtered &lt;- d_stroop |&gt; filter(respPractice.rt &lt;= 1)\n\n# nur √úbungstrials mit Antwortzeiten zwischen 1 und 2 Sekunden behalten\nd_filtered &lt;- d_stroop |&gt; filter(respPractice.rt &gt; 1 & respPractice.rt &lt; 2)\n\n# mehrere Filter verwenden\nd_filtered &lt;- d_stroop |&gt; \n    filter(color == \"red\") |&gt;\n    filter(respPractice.rt &lt;= 1)\n\nIn unserem Datensatz m√∂chten wir nur die g√ºltigen Experimentdaten behalten, die Color-To-Key (ctk) Bedingung sowie die Practice Trials m√∂chten wir ausschliessen.\nDie Variable trials_test.thisN enth√§lt die Trialnummer, sie enth√§lt nur Eintr√§ge, w√§hrend der g√ºltigen Trials. Wir k√∂nnen dies nutzen und alle Zeilen behalten in welchen die Zelle der Variable trials_test.thisN nicht leer ist:\n\nd_stroop &lt;- d_stroop |&gt; \n    filter(!is.na(trials_test.thisN)) \n\n\n\n\n\n\n\nHands-on: Daten filtern\n\n\n\nErstellen Sie einen neuen Datensatz namens d_stroop_correct und filtern Sie diesen so dass er nur Trials mit richtigen Antworten enth√§lt. Schauen Sie in der Variable keyResp_test_run.corr, ob tats√§chlich nur noch richtige Antworten √ºbrig geblieben sind.\nAchtung: Arbeiten Sie in den weiteren Schritten nicht mit diesem Datensatz weiter, da wir die falschen Antworten in einem n√§chsten Schritt noch im Datensatz brauchen."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#variablen-ausw√§hlen-select",
    "href": "pages/chapters/datawrangling.html#variablen-ausw√§hlen-select",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Variablen ausw√§hlen: select()\n",
    "text": "Variablen ausw√§hlen: select()\n\nEin komplexer Datensatz mit sehr vielen Variablen wird oft f√ºr die Analyse aus Gr√ºnden der Einfachheit oder Anonymisierung reduziert. Das bedeutet, dass man sich die n√∂tigen Variablen herausliest, und nur mit diesem reduzierten Datensatz weiterarbeitet. Hierzu eignet sich die Funktion select() sehr gut: Mit select(.data, variablenname, ...) k√∂nnen die zu behaltenden Variablen ausgew√§hlt werden. Wird ein ! vor einen Variablennamen gesetzt, wird die Variable nicht behalten.\nMit select() k√∂nnen wir auch die Variablen sortieren und umbenennen, damit unser Datensatz so strukturiert ist, wie wir ihn gebrauchen k√∂nnen.\nBeispiele:\n\n# Variable word und color behalten ohne Pipe\nd_simpler &lt;- select(d_stroop, word, color)\n\n# Variable word und color behalten mit Pipe\nd_simpler &lt;- d_stroop |&gt; select(word, color)\n\n# alle Variablen ausser word behalten\nd_simpler &lt;- d_stroop |&gt; select(!word)\n\n# Variablennamen ver√§ndern\nd_simpler &lt;- d_stroop |&gt; select(newvariablename = word)\n\nSollen mehrere Variablen am St√ºck ausgew√§hlt werden, kann man die erste Variable in der Reihe (z.B. word) und die letzte in der Reihe (z.B. congruent) als word:congruent eingeben, dann werden auch alle dazwischen liegenden Variablen ausgew√§hlt.\n\n\n\n\n\n\nHands-on: Variablen ausw√§hlen\n\n\n\nSchauen Sie sich Ihren Datensatz an, welche Variablen ben√∂tigen Sie f√ºr die weitere Analysen?\nErstellen Sie einen neuen Datensatz d_stroop_clean in welchem Sie die entsprechenden Variablen ausw√§hlen und umbennen, wenn Sie Ihnen zu lange/kompliziert erscheinen.\nUntenstehend finden Sie ein Beispiel, wie der Datensatz danach aussehen k√∂nnte.\n\n\n\n\nRows: 120\nColumns: 10\n$ id         &lt;chr&gt; \"sub-154989\", \"sub-154989\", \"sub-154989\", \"sub-154989\", \"su‚Ä¶\n$ experiment &lt;chr&gt; \"stroop_test\", \"stroop_test\", \"stroop_test\", \"stroop_test\",‚Ä¶\n$ trial      &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ word       &lt;chr&gt; \"rot\", \"rot\", \"blau\", \"gelb\", \"rot\", \"blau\", \"blau\", \"gelb\"‚Ä¶\n$ color      &lt;chr&gt; \"red\", \"red\", \"blue\", \"yellow\", \"yellow\", \"yellow\", \"red\", ‚Ä¶\n$ corrAns    &lt;chr&gt; \"r\", \"r\", \"b\", \"g\", \"g\", \"g\", \"r\", \"r\", \"b\", \"g\", \"b\", \"b\",‚Ä¶\n$ congruent  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,‚Ä¶\n$ response   &lt;chr&gt; \"r\", \"r\", \"b\", \"g\", \"g\", \"g\", \"r\", \"r\", \"b\", \"g\", \"b\", \"b\",‚Ä¶\n$ rt         &lt;dbl&gt; 1.0639791, 0.7370255, 1.1883303, 1.2007897, 1.6688681, 1.58‚Ä¶\n$ accuracy   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#neue-variablen-generieren-und-ver√§ndern-mutate-und-case_when",
    "href": "pages/chapters/datawrangling.html#neue-variablen-generieren-und-ver√§ndern-mutate-und-case_when",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Neue Variablen generieren und ver√§ndern: mutate() und case_when()\n",
    "text": "Neue Variablen generieren und ver√§ndern: mutate() und case_when()\n\nMit der mutate(.data, ‚Ä¶) Funktion k√∂nnen im Datensatz neue Variablen generiert oder bestehende ver√§ndert werden.\nBeispiel:\n\n# Neue Variablen erstellen\nd_new &lt;- d_stroop_clean |&gt;\n    mutate(num_variable = 1.434,\n           chr_variable = \"1.434\",\n           sumofxy_variable = rt + 1,\n           copy_variable = word)\n\n# Bestehende Variablen ver√§ndern\nd_new &lt;- d_new |&gt;\n    mutate(num_variable = num_variable * 1000) # z.B. um Sekunden zu Millisekunden zu machen\n\nMit case_when() kann eine neue Variable erstellt werden in Abh√§ngigkeit von Werten anderer Variablen. Damit kann z.B. eine Variable accuracy erstellt werden, die den Wert correct hat, wenn die Aufgabe richtig gel√∂st wurde (z.B. Bedingung rot und Tastendruck r) und sonst den Wert error hat.\nBeispiel:\n\nd_condvariable &lt;- d_stroop_clean |&gt;\n    mutate(cond_variable = case_when(rt &gt; 0.8 ~ \"higher\",\n                                     rt &lt;= 0.8 ~ \"lower\",\n                                     .default = NA))\n\n\n\n\n\n\n\nHands-on: Variablen generieren und ver√§ndern\n\n\n\n\nErstellen Sie im Datensatz d_stroop_clean eine neue Variable mit dem Namen researcher, den Ihren Namen enth√§lt.\nErstellen Sie zudem eine Variable accuracy_check, mit correct f√ºr korrekte und error f√ºr inkorrekte Trials. Kontrollieren Sie mit der Variable keyResp_test_run.corr (oder Ihrem neuen Variablennamen, wenn Sie diese umbenannt haben) im Datensatz, ob Sie die Aufgabe richtig gel√∂st haben.\n√Ñndern Sie die Trialnummer, so dass sie nicht mehr mit 0 beginnt, sondern mit 1."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#variablenklasse-ver√§ndern-as.factor-as.numeric",
    "href": "pages/chapters/datawrangling.html#variablenklasse-ver√§ndern-as.factor-as.numeric",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Variablenklasse ver√§ndern: as.factor(), as.numeric(), ‚Ä¶",
    "text": "Variablenklasse ver√§ndern: as.factor(), as.numeric(), ‚Ä¶\nVariablen k√∂nnen verschiedene Klassen haben, sie k√∂nnen z.B. kategoriale (factor, character) oder numerische (integer, numeric, double) Informationen enthalten. Beim Einlesen ‚Äúr√§t‚Äù R, welche Klasse eine Variable hat. Teilweise m√∂chten wir dies √§ndern. Wenn wir eine Variable zu einem Faktor machen m√∂chten, verwenden wir as.factor(). Dies macht z.B. Sinn, wenn die Versuchspersonennummer als Zahl eingelesen wurde. Um von einem Faktor zu einer numerischen Variable zu kommen, verwenden wir as.numeric().\n\n# Die Variable \"congruent\" zu einem Faktor machen\nd_stroop_clean |&gt; \n    mutate(congruent = as.factor(congruent))\n\n\n\n\n\n\n\nHands-on: Variablenklassen\n\n\n\nSchauen Sie sich den Datensatz mit glimpse() oder mit View() an. Welche Klassen enth√§lt Ihr Datensatz und was bedeuten Sie?"
  },
  {
    "objectID": "pages/chapters/datawrangling.html#daten-gruppieren-und-zusammenfassen-group_by-und-summarise",
    "href": "pages/chapters/datawrangling.html#daten-gruppieren-und-zusammenfassen-group_by-und-summarise",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Daten gruppieren und zusammenfassen: group_by() und summarise()\n",
    "text": "Daten gruppieren und zusammenfassen: group_by() und summarise()\n\nMit diesen beiden Funktionen k√∂nnen wir mit wenig Code den Datensatz gruppieren und zusammenfassen.\n\n# Nach W√∂rter gruppieren\nd_stroop_clean |&gt; group_by(word) |&gt;\n    summarise(mean_rt = mean(rt),\n              sd_rt = sd(rt))\n\n# A tibble: 3 √ó 3\n  word  mean_rt sd_rt\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 blau     1.23 0.490\n2 gelb     1.26 0.447\n3 rot      1.16 0.552\n\n\n\n\n\n\n\n\nHands-on: Daten zusammenfassen\n\n\n\nErstellen Sie einen neuen Datensatz d_stroop_summary\n\nGruppieren Sie den Datensatz f√ºr Wortfarbe und Kongruenz.\nFassen Sie f√ºr diese Gruppen die durchschnittliche Antwortzeit und Accuracy sowie die Standardabweichungen zusammen."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#datens√§tze-speichern-write.csv",
    "href": "pages/chapters/datawrangling.html#datens√§tze-speichern-write.csv",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Datens√§tze speichern: write.csv()\n",
    "text": "Datens√§tze speichern: write.csv()\n\n\nwrite.csv(d_stroop_clean, file = \"data/dataset_stroop_clean.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nHands-on: Datens√§tze speichern\n\n\n\nSpeichern Sie einen neuen Datensatz mit den vorverarbeiteten Daten."
  },
  {
    "objectID": "pages/chapters/datawrangling.html#data-wrangling-workflow-implementieren",
    "href": "pages/chapters/datawrangling.html#data-wrangling-workflow-implementieren",
    "title": "Daten importieren und vorverarbeiten",
    "section": "Data wrangling workflow implementieren",
    "text": "Data wrangling workflow implementieren\n\n\n\n\n\n\nHands-on: Data wrangling workflow\n\n\n\nErstellen Sie nun ein Projekt f√ºr das Random-Dot Experiment und f√ºhren Sie die gelernten data wrangling Schritte selbstst√§ndig durch.\n\n\n\nZu den gelernten Funktionen finden Sie hier Grafiken die evtl. helfen, sich die Funktions-Namen zu merken."
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html",
    "href": "pages/chapters/datawrangling_automatisiert.html",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "",
    "text": "F√ºr diesen Teil ben√∂tigen Sie das R-Project complab_datawrangling_stroop mit den 3 von Ihnen erhobenen Datens√§tzen des Stroop Experiments."
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html#setup",
    "href": "pages/chapters/datawrangling_automatisiert.html#setup",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "Setup",
    "text": "Setup\n\n# Package laden (bei jedem √ñffnen von R zu Beginn des Skripts ausf√ºhren)\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html#stroop-experiment-data-wrangling",
    "href": "pages/chapters/datawrangling_automatisiert.html#stroop-experiment-data-wrangling",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "\nStroop-Experiment data wrangling",
    "text": "Stroop-Experiment data wrangling\n\n# Daten vorverarbeiten\nd_stroop = read_csv(\"data/stroop_example_1.csv\") |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color, \n           congruent, \n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)"
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html#daten-mit-eigener-funktion-einlesen",
    "href": "pages/chapters/datawrangling_automatisiert.html#daten-mit-eigener-funktion-einlesen",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "Daten mit eigener Funktion einlesen",
    "text": "Daten mit eigener Funktion einlesen\n\nread_stroop = function(path){\n    # Code kopiert von oben\n    d_stroop = read_csv(path) |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color, \n           congruent, \n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)\n    # ---------------------\n    d_stroop\n}\n\nread_stroop(path = \"data/stroop_example_1.csv\")"
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html#einlesen-automatisieren",
    "href": "pages/chapters/datawrangling_automatisiert.html#einlesen-automatisieren",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "Einlesen Automatisieren",
    "text": "Einlesen Automatisieren\nWir ben√∂tigen eine Liste aller Daten files. Die Funktion list.files() gibt einer liste aller Dokumente in einem Ordner zur√ºck.\n\nlist.files(path = 'data/')\n\nlist.files(path = 'data/', pattern = 'stroop')\n\nUm die Files einzulesen, reichen nur die Namen der Dateien nicht aus. Dazu ben√∂tigen wir die kompletten Pfade.\n\nfiles = list.files(path = 'data/', pattern = 'stroop') %&gt;% \n    paste('data/', ., sep = '')\n\n\nHier wird die Pipe des magritter-Packages verwendet (%&gt;%) statt die Base-R Pipe (|&gt;). Mit %&gt;% haben wir die M√∂glichkeit mit dem . zu bestimmen wo die weitergeleiteten Inhalte der Pipe eingef√ºgt werden (nach data/). Informationen zu den Unterschieden der Pipes finden Sie hier.\nAlle Files von Hand einlesen\nJedes Daten File wird einzeln eingelesen. Anschliessend m√ºssen alle Files zusammengef√ºgt werden. Diese L√∂sung ist einfach zu verstehen, ist bei vielen Dokumenten aber zu aufw√§ndig.\n\nfile1 = files[1]\nfile2 = files[2]\nfile3 = files[3]\n\nd1 = read_stroop(file1)\nd2 = read_stroop(file2)\nd3 = read_stroop(file3)\n\nd_hand = bind_rows(d1, d2, d3)\n\nAlle Files mit for-Loop einlesen\nDas Einlesen kann mit einem for-Loop automatisiert werden. Der Loop iteriert √ºber alle Daten Files. Als erstes muss ein leerer Data Frame d_loop erstellt werden. Bei jeder Iteration des Loops wird ein Daten File eingelesen und dem erstellten Data Frame d_loop angeh√§ngt.\n\nd_loop = tibble()\n\nfor (file in files){\n    d_tmp = read_stroop(file)\n    d_loop = bind_rows(d_loop, d_tmp)\n}\n\nAlle Files mit der Funktion map() einlesen\nmap() wendet eine Funktion auf alle Elemente eines Vektors an. Der Vektor files enth√§lt die Pfade zu den Daten Files. Mit map() k√∂nnen wir also unsere selbst erstellte Funktion read_stroop() auf jeden Pfad anwenden. Im Anschluss m√ºssen die Dataframes noch verbunden werden.\n\nd_map1 = files |&gt;\n    map(read_stroop) %&gt;%\n    bind_rows()\n\nDie Funktion map_dfr() macht das gleiche wie map() f√ºgt aber zus√§tzlich die einzelnen Data Frames automatisch zusammen.\n\nd_map2 = files |&gt;\n    map_dfr(read_stroop)"
  },
  {
    "objectID": "pages/chapters/datawrangling_automatisiert.html#kompletter-stroop-code-an-einem-ort",
    "href": "pages/chapters/datawrangling_automatisiert.html#kompletter-stroop-code-an-einem-ort",
    "title": "Automatisiertes importieren und vorverarbeiten",
    "section": "Kompletter Stroop Code an einem Ort",
    "text": "Kompletter Stroop Code an einem Ort\n\nread_stroop = function(path){\n    d_stroop = read_csv(path) |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color, \n           congruent, \n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)\n    d_stroop\n}\n\nd = list.files(path = 'data/', pattern = 'stroop') %&gt;% \n    paste('data/', ., sep = '') |&gt;\n    map_dfr(read_stroop)\n\nd |&gt; write.csv(file = \"data/clean/dataset_stroop_clean.csv\", row.names = FALSE) # neuer Datensatz in anderen Ordner speichern um Verdoppelung zu vermeiden"
  },
  {
    "objectID": "pages/chapters/gallery_mi.html",
    "href": "pages/chapters/gallery_mi.html",
    "title": "Plot Gallery- Mi",
    "section": "",
    "text": "ReuseCC BY-SA 4.0CitationBibTeX citation:@online{fitze,\n  author = {Fitze, Daniel and Wyssen, Gerda},\n  title = {Plot {Gallery-} {Mi}},\n  url = {https://kogpsy.github.io/neuroscicomplabFS24//pages/chapters/gallery_mi.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFitze, Daniel, and Gerda Wyssen. n.d. ‚ÄúPlot Gallery- Mi.‚Äù\nhttps://kogpsy.github.io/neuroscicomplabFS24//pages/chapters/gallery_mi.html."
  },
  {
    "objectID": "pages/chapters/gallery_mo.html",
    "href": "pages/chapters/gallery_mo.html",
    "title": "Plot Gallery - Mo",
    "section": "",
    "text": "PlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#d &lt;- as.data.frame()\n\n# Plot erstellen\n# Rohdaten plotten\nplot_rohdaten &lt;- ggplot(d, aes(x = trial, y = rt, color = congruent)) +\n  geom_point() +\n  labs(title = \"Stroop Aufgabe\",\n       subtitle = \"Vergleich der Reaktionszeiten nach Bedingung\",\n       x = \"Versuch\",\n       y = \"Reaktionszeit (s)\",\n       color = \"Kongruenz\",\n       caption = \"dataset_stroop_clean\") +\n  theme_minimal()\n\n# Zusammenfassendes Ma√ü (Boxplot) erstellen\nplot_zusammenfassung &lt;- ggplot(d, aes(x = as.factor(congruent), y = rt, fill = as.factor(congruent))) +\n  geom_boxplot() +\n  labs(title = \"Stroop Aufgabe\",\n       subtitle = \"Verteilung der Reaktionszeiten nach Kongruenz\",\n       x = \"Kongruenz\",\n       y = \"Reaktionszeit (s)\",\n       fill = \"Kongruenz\",\n       caption = \"dataset_stroop_clean\") +\n  theme_minimal()\n\n# Optional: Facets verwenden, um nach weiteren Variablen zu unterscheiden\n# Hier m√ºsstest du die entsprechenden Facets anpassen und deine Daten entsprechend strukturieren\nplot_facets &lt;- ggplot(d, aes(x = trial, y = rt, color = congruent)) +\n  geom_point() +\n  facet_wrap(~word) +\n  labs(title = \"Stroop Aufgabe\",\n       subtitle = \"Vergleich der Reaktionszeiten nach Bedingung und Worttyp\",\n       x = \"Versuch\",\n       y = \"Reaktionszeit (s)\",\n       color = \"Kongruenz\",\n       caption = \"dataset_stroop_clean\") +\n  theme_minimal()\n# print (d)\n# Optional: Speichern des Plots als Bild\n#ggsave(\"stroop_plot.png\", plot_rohdaten, width = 8, height = 6, units = \"in\")\n\n# Plots anzeigen\n#plot_rohdaten\n#plot_zusammenfassung\n#plot_facets\n\n\n\n# Plot erstellen √ºbereinander\nplot_all &lt;- ggplot(d, aes(x = trial, y = rt)) +\n  geom_point(aes(color = factor(congruent)), position = position_jitter(width = 0.2), alpha = 0.5) +  # Rohdaten anzeigen\n  geom_boxplot(aes(fill = factor(congruent)), alpha = 0.8, outlier.shape = NA) +  # Zusammenfassende Boxplots\n  labs(title = \"Stroop Task: Reaktionszeiten\",\n       subtitle = \"Vergleich der Reaktionszeiten nach Bedingung\",\n       x = \"Versuch\",\n       y = \"Reaktionszeit (in Sekunden)\",\n       color = \"Kongruenz\",\n       fill = \"Kongruenz\",\n       caption = \"dataset_stroop_clean\") +\n  theme_minimal()\n\n# Plot anzeigen\nprint(plot_all)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- d |&gt;\n    filter(rt &gt; 0.1 & rt &lt; 1.75) |&gt;\n    select(id, trial, congruent, rt)|&gt;\n    mutate (congruent = as.factor(congruent)) |&gt;\n    mutate(congruent = case_match(congruent,\n                      \"0\" ~ \"inkongruent\",\n                      \"1\" ~ \"kongruent\"))\n\np_boxplot &lt;- d |&gt;\n    filter(id %in% c(\"sub-10318869\", \"sub-1106725\", \"sub-113945\", \"sub-11959984\",\n                     \"sub-12224605\", \"sub-12242654\", \"sub-13366559\", \"sub-13662910\",\n                     \"sub-13937586\", \"sub-13771505\", \"sub-73916681\", \"sub-74487595\",\n                     \"sub-75143607\", \"sub-89031395\", \"sub-90289188\")) |&gt;\n    ggplot(aes(x = congruent, y = rt, color = congruent)) +\n    geom_jitter(alpha = 0.25, width = 0.2) +\n    geom_boxplot(alpha = 0, width = 0.2, color = \"black\") +\n    scale_color_manual(values = c(kongruent  = \"green2\",\n                                  inkongruent = \"pink\")) +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Reaktionszeit bei In- & Kongruenz\",\n         subtitle = \"Ist die Reaktionszeit geringer bei Kongurenz?\",\n         x = \"Kongruenz\",\n         y = \"Reaktionszeit\",\n         caption = \"Daten von 15 Vpn\" )\n\np_boxplot\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Fragestellung: Zeigt sich ein gr√∂sserer Lerneffekt (der sich in einer schnelleren Reaktionszeit wiederspiegelt) bei inkongruenten im Verlgeich zu kongruenten Items √ºber die Trials hinweg?\n\n#Erstellen eines Datensatzes mit den f√ºr die Fragestellung relevanten Variablen ausgew√§hlt werden und Gruppieren der Trials in 6 Trialgruppen\nd_grouped &lt;- d %&gt;%\n    na.omit() %&gt;%\n    group_by(trial) %&gt;%\n    reframe(id,\n            rt,\n            congruent = congruent == 1,\n            trial_group = case_when(\n                trial &gt;= 1 & trial &lt;= 20 ~ \"Trials 1-20\",\n                trial &gt;= 21 & trial &lt;= 40 ~ \"Trials 21-40\",\n                trial &gt;= 41 & trial &lt;= 60 ~ \"Trials 41-60\",\n                trial &gt;= 61 & trial &lt;= 80 ~ \"Trials 61-80\",\n                trial &gt;= 81 & trial &lt;= 100 ~ \"Trials 81-100\",\n                trial &gt;= 101 & trial &lt;= 120 ~ \"Trials 101-120\"\n            )) %&gt;%\n    mutate(trial_group = factor(trial_group, levels = c(\"Trials 1-20\", \"Trials 21-40\", \"Trials 41-60\", \"Trials 61-80\", \"Trials 81-100\", \"Trials 101-120\")))\n\n#Erstellen einer Variable die den Gesammittelwert der Variable Reaktionszeit beinhaltet\noverall_mean &lt;- mean(d$rt, na.rm = TRUE)\n\n#Erstellen eines Datensatzes in dem zuf√§llig Trials einzelner Teilnehmer:innen ausgew√§hlt und gespeichert werden\nrandom_participants &lt;- d_grouped %&gt;%\n    filter(rt &lt;= 5.0) %&gt;%\n    group_by(trial_group) %&gt;%\n    sample_n(300, replace = FALSE)\n\n#Erstellen des Plots der die genannte Fragestellung beantwortet\np = d_grouped %&gt;%\n    filter(rt &lt;= 5.0) %&gt;%\n    ggplot(mapping = aes(x = trial_group,\n                         y = rt,\n                         fill = congruent)) +\n    geom_boxplot(color = \"black\") +\n    geom_hline(yintercept = overall_mean, linetype = \"dashed\", color = \"blue\", size = 0.8) +\n    geom_jitter(data = random_participants, aes(x = trial_group, y = rt, color = congruent, alpha = 0.3)) +\n    labs(title = \"Reaktionszeiten von kongruenten und inkongruenten Items\",\n         subtitle = \"Zeigt sich ein gr√∂sserer Lerneffekt bei inkongruenten im Vergleich zu kongruenten Items √ºber die Trials hinweg?\",\n         x = \"Trialgruppen\",\n         y = \"Reaktionszeit in s\",\n         fill = \"kongruent\",\n         color = \"kongruent\") +\n    theme_light()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# daten vorbereiten\nd_clean &lt;- d %&gt;%\n    drop_na() %&gt;%\n    mutate(across(where(is.character),\n           as.factor)) %&gt;%  # aller text zu factors\n    mutate(corr_fct = as.factor(corr),\n           congr_fct = as.factor(congruent))  # neue vars mit corr und congr als fct\n# plot\nd_clean %&gt;%\n    filter(corr_fct == 1 & rt &lt; 1.5) %&gt;%\n    ggplot(aes(\n    x = color,\n    y = rt,\n    color = color,\n    fill = color)) +\n    geom_jitter(alpha = 0.3, width = 0.2, size = 0.5) +\n    geom_violin(alpha = 0.3, color = \"white\") +\n    geom_boxplot(alpha = 0.1, width = 0.2, color = \"white\") +\n    theme_minimal() +\n    labs(title = \"BLAU MACHT SCHNELL\",\n         subtitle = \"Stoop-Test: Unterscheiden sich die Reaktionszeiten je nach Farbe des Stimulusmaterials?\n(Verwendete Daten: Nur kongruente Stimuluspaare und korrekte Antworten)\",\n         x = \"Farbe Stimulusmaterial\",\n         y = \"Reaktionszeit (s)\",\n         caption = \"Konklusion: Ich streiche mein Studierzimmer blau\") +\n    theme(plot.title = element_text(face = \"bold\")) +\n    scale_color_manual(values = c(\"blue\" = \"royalblue3\",\n                                  \"red\"=\"red3\",\n                                  \"yellow\"=\"gold3\")) +\n    scale_fill_manual(values = c(\"blue\" = \"royalblue3\",\n                                  \"red\"=\"red3\",\n                                  \"yellow\"=\"gold3\"))\n\n\n### links:\n# https://www.statology.org/color-by-factor-ggplot2/\n# https://sape.inf.usi.ch/quick-reference/ggplot2/colour\n# https://kogpsy.github.io/neuroscicomplabFS24/pages/chapters/data_visualization_1.html\n# https://kogpsy.github.io/neuroscicomplabFS24/pages/chapters/data_visualization_2.html\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- d |&gt;\n  filter(rt &gt; 0.09 & rt &lt; 15)\n\n\nd_color_rt &lt;- d |&gt;\n  group_by(id, color) |&gt;\n  summarise(\n    mean_rt = mean(rt)\n  )\n\n\np = d_color_rt |&gt;\n  ggplot(aes(x = color, y = mean_rt, color = color)) +\n  geom_jitter(size = 1.5, alpha = 0.4,\n              width = 0.2, height = 0) +\n  geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n  scale_color_manual(values = c(blue = \"blue2\",\n                                red = \"red1\",\n                                yellow = \"yellow1\")) +\n  labs(title = \"Uebung 3\",\n       subtitle = \"Unterscheidet sich die Reaktionszeit der Probanden, je nach Farbe die pr√§sentiert wird?\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n# library(ggplot2)\n\n# Datensatz anschauen\n# glimpse(d)\n\n# unique(d$color)\n\nd_congruent &lt;- d %&gt;%\n    mutate(congruent = as.factor(congruent))%&gt;%\n    filter(rt &lt; 4 & rt &gt; 0.1)\n\n\nd_rt_summary &lt;- d_congruent %&gt;%\nsummarise(mean_rt = mean(rt),\n              sd_rt = sd(rt))\n\n\n# glimpse(d_rt_summary)\n\n#plot\np = d_congruent |&gt;\n    ggplot(mapping = aes(x =congruent ,\n                         y = rt, color= congruent)) +\n    geom_jitter(width = 0.4, alpha= 0.09) +\n    scale_y_continuous(breaks = seq(0, 4, by = 0.5))+\n    geom_hline(yintercept = d_rt_summary$mean_rt, linetype = \"dashed\", color = \"red\", linewidth= 1)+\n    scale_color_manual(values= c(\"darkviolet\", \"darkblue\"))+\n    labs(title = \"Geringere Reaktionszeit in der kongruenten Bedingungen\",\n         subtitle = \"Unterscheidet sich die Reaktionszeit zwischen der kongruenten und inkongruenten Bedinung?\",\n         x = \"inkongruent vs. kongruent\",\n         y = \"Reaktionszeit in Sekunden\") +\n    theme_classic() +\n    guides(color = FALSE)\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#Libraries laden\nlibrary(patchwork)\nlibrary(naniar)\nlibrary(psych)\nlibrary(ggpubr)\n\n#Daten vorverarbeiten\n\nd_factor &lt;- d %&gt;%\n    mutate(across(where(is.character), as.factor))\n\nd_filtered &lt;- d_factor %&gt;%\n    filter(rt &gt; 0.09 & rt &lt; 15)\n\n\nd_acc_rt_trial &lt;- d_filtered %&gt;%\n    group_by(congruent, trial) %&gt;%\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)) %&gt;%\n    mutate(median_rt = median_rt*1000) %&gt;%\n    filter(accuracy &gt; 0.5)\n\n#Bedingungen in Faktor umwandeln und Levels umbenennen\nd_acc_rt_trial$congruent_string &lt;- factor(d_acc_rt_trial$congruent, levels = c(\"0\", \"1\"))\nlevels(d_acc_rt_trial$congruent_string) &lt;- list(inkongruent = \"0\", kongruent = \"1\")\n\n\n#Plots erstellen\nmy_comparisonsp1 &lt;- list(c(\"kongruent\", \"inkongruent\"))\n\np1 &lt;- d_acc_rt_trial %&gt;%\n    ggplot(aes(x = congruent_string, y= accuracy, color = congruent_string)) +\n    geom_jitter(size = 2.5, alpha = 0.4,\n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    labs(x = \"Bedingung\",\n         y = \"Genauigkeit\",\n         title = \"Mittlere Genauigkeit pro Bedingung\",\n         subtitle = \"Unterscheidet sich die Genauigkeit\\nzwischen den Bedingungen?\") +\n    theme_classic(base_size = 12) +\n    theme(legend.position = \"none\") +\n    stat_compare_means(comparisons = my_comparisonsp1, label = \"p.signif\", method = \"wilcox.test\", paired = TRUE, ref.group = \"inkongruent\")\n\n\nmy_comparisonsp2 &lt;- list(c(\"kongruent\", \"inkongruent\"))\n\np2 &lt;- d_acc_rt_trial %&gt;%\n    ggplot(aes(x = congruent_string, y= median_rt, color = congruent_string)) +\n    geom_jitter(size = 2.5, alpha = 0.4,\n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    labs(x = \"Bedingung\",\n         y = \"Mittlere Reaktionszeit [ms]\",\n         title = \"Mittlere Reaktionszeit pro Bedingung\",\n         subtitle = \"Unterscheidet sich die mittlere Reaktionszeit\\nzwischen den Bedingungen?\") +\n    theme_classic(base_size = 12) +\n    theme(legend.position = \"none\") +\n    stat_compare_means(comparisons = my_comparisonsp1, label = \"p.signif\", method = \"wilcox.test\", paired = TRUE, ref.group = \"inkongruent\")\n\np1 + p2\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(aes(x = color, y = rt, color = color)) +\n    geom_jitter(width = 0.2, size = 0.5, alpha = 0.25) +\n    geom_boxplot(width = 0.4, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(blue = \"blue\",\n                                 red = \"red\",\n                                 yellow = \"yellow\")) +\n    theme_minimal() +\n    labs(x = \"color of presented word\",\n         y = \"reaction time [s]\",\n         title = \"Reaction times per color\",\n         subtitle = \"Were the reaction times different for the 3 colors presented?\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n# Anmerkung: Habe noch versucht, Signifikanzwerte etc. anzeigen zu lassen (einfach aus Neugier) - vielleicht seht ihr das besser, ob es funktioniert hat.\n\nlibrary(ggsignif)\n\n# Variable \"congruent\" in einen Faktor mit Labels \"Inkongruent\" bei 0, \"Kongruent\" bei 1\nd$congruent &lt;- factor(d$congruent, levels = c(0,1), labels = c(\"Inkongruent\", \"Kongruent\"))\n\n# Durchf√ºhrung eines t-Tests\nt_test_result &lt;- t.test(rt ~ congruent, data = d, var.equal = FALSE)\n\n# Extrahieren des p-Werts aus dem Testergebnis\np_value &lt;- t_test_result$p.value\n\n# Plotting mit ggplot\np &lt;- ggplot(data = d,\n            mapping = aes(x = congruent, y = rt, color = congruent)) +\n  geom_jitter(height = 0.1, width = 0.1) +\n  geom_boxplot(width = 0.1, color = \"blue\", fill = \"white\", alpha = 0.5) +  # Boxplot √ºber den Dichteplot legen\n  labs(title = \"Reaktionszeiten f√ºr Kongruente und nicht-kongruente Aufgaben im Stroop-Paradigma\",\n       subtitle = \"Wieviel Zeit ben√∂tigt eine Person, um bei nicht-kongruenten Farb-Wort-Aufgaben vs. kongruenten Farb-Wort-Aufgaben zu inhibieren?\",\n       x = \"Congruent\",\n       y = \"RT\",\n       color = \"Congruent\") +\n  theme_minimal()\n\n# Hinzuf√ºgen der Signifikanzanzeige\np &lt;- p + geom_signif(comparisons = list(c(\"Incongruent\", \"Congruent\")),\n                     map_signif_level = TRUE,\n                     textsize = 3,\n                     vjust = 0.5,\n                     manual = F) +\n  annotate(\"text\", x = 1.5, y = max(d$rt), label = ifelse(p_value &lt; 0.05, \"*\", \"\"), size = 6)\n\n# Plot anschauen\np\n\n\n# Urspr√ºngliche Aufgabenstellung: Beides, Rohdaten UND mind. 1 zusammenfassendes Mass(z.B. Mittelwert mit Standardabweichungen, Box-/Violinplot, etc.). TIPP: Mehrere Geoms k√∂nnen √ºbereinander gelegt werden.\n# Mind. 2 unterschiedliche Farben.\n# Beschriftungen: Titel, Subtitel, Achsenbeschriftungen, (optional: Captions)\n# Der Subtitel beinhaltet die Frage, welche der Plot beantwortet.\n# Ein Theme verwenden.\n# Optional: Facets verwenden.\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nt_test_resultate &lt;- t.test(rt ~ congruent, data = d)\n\np_wert &lt;- t_test_resultate$p.value\n\nsignifikanz_label &lt;- ifelse(p_wert &lt; 0.05, \"Es besteht eine Signifikante Interaktion (p &lt; 0.05) zwischen den Kongruenbedingugen.\", \"Es besteht keine signifikante Interaktion (p &gt;= 0.05) zwischen den Kongruenzbedingungen.\")\n\np = d %&gt;%\n    ggplot(aes(x = as.factor(congruent), y = rt, fill = congruent)) +\n    geom_violin(position = 'dodge', alpha = 0.5, color = \"red\", fill = \"orange\") +\n    geom_point(size = 0.5, alpha = 0.5, color = \"red\") +\n    stat_summary(fun = mean, geom = \"point\", size = 3, color = \"black\") +\n    stat_summary(fun = mean, geom = \"line\", aes(group = 1), linetype = \"dashed\", color = \"black\") +\n    stat_summary(fun = mean, geom = \"text\", aes(label = paste(round(..y.., digits = 2))), vjust = -0.5, color = \"black\") +\n    labs(\n        title = \"Stroop Task: Reaktionszeit nach Kongruenz\",\n        subtitle = \"Gibt es Unterschiede in den Reaktionszeiten zwischen den Kongruenzbedingungen?\",\n        x = \"Kongruenz\",\n        y = \"Reaktionszeit (ms)\",\n        caption = signifikanz_label) +\n    theme_grey() +\n    theme(legend.position = \"none\") +\n    scale_x_discrete(\n        breaks = c(0, 1),\n        labels = c(\"Nicht Kongruent\", \"Kongruent\")\n    )\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Zu langsame und zu schnelle herausfiltern\nd &lt;- d |&gt;\n  filter(rt &gt; 0.09 & rt &lt; 15)\n\n# Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nacc_rt_individual &lt;- d |&gt;\n  group_by(id, congruent) |&gt;\n  summarise(\n    N = n(),\n    ncorrect = sum(corr),\n    accuracy = mean(corr),\n    median_rt = median(rt)\n  )\n\n# Datensatz mit allen Ids, welche zuwenig Trials hatten\nn_exclusions &lt;- acc_rt_individual |&gt;\n  filter(N &lt; 40)\n\n# Aus dem Hauptdatensatz diese Ids ausschliessen\nd &lt;- d |&gt;\n  filter(!id %in% n_exclusions$id)\n\n# Check\nd_acc_rt_individual &lt;- d |&gt;\n  group_by(id, congruent) |&gt;\n  summarise(\n    N = n(),\n    ncorrect = sum(corr),\n    accuracy = mean(corr),\n    median_rt = median(rt)\n  )\n\n# Gesamtmittelwert und Mittelwerte Reaktionszeit berechnen\noverall_mean_rt &lt;- mean(d$rt)\n\navg_rt_congruent &lt;- aggregate(rt ~ trial, data = d[d$congruent == 1, ], FUN = mean)\navg_rt_incongruent &lt;- aggregate(rt ~ trial, data = d[d$congruent == 0, ], FUN = mean)\n\n# Plot erstellen\np &lt;- d |&gt;\n  ggplot(aes(x = trial, y = rt, color = factor(congruent))) +\n  geom_line() +\n  geom_line(data = avg_rt_congruent, aes(y = rt, color = \"Kongruent\")) +\n  geom_line(data = avg_rt_incongruent, aes(y = rt, color = \"Inkongruent\")) +\n  geom_hline(yintercept = overall_mean_rt, linetype = \"solid\", color = \"black\") +\n  labs(title = \"Stroop-Experiment\",\n       subtitle = \"Zeigen die Reaktionszeiten f√ºr kongruente und inkongruente Aufgaben eine √§hnliche Tendenz im Verlauf der Trials verglichen zur mittleren Reaktionszeit?\",\n       x = \"Trial\",\n       y = \"Reaktionszeit (ms)\") +\n  scale_color_manual(values = c(\"violet\", \"orange\", \"blue\", \"red\"),\n                     labels = c(\"Inkongruent\", \"Kongruent\", \"Mittlere Inongruenz\", \"Mittlere Kongruenz\"),\n                     name = \"Kongruenz\") +\n  facet_wrap(~ factor(congruent, levels = c(0, 1), labels = c(\"Inkongruent\", \"Kongruent\")), scales = \"free\", ncol = 1) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd$congruent &lt;- factor(d$congruent, levels = c(0, 1), labels = c(\"Inkongruent\", \"Kongruent\"))\n\np &lt;-  ggplot(d, aes(x = congruent, y = rt, color = congruent)) +\n    geom_jitter(size = 3, alpha = 0.4, width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(\"Inkongruent\" = \"tomato2\", \"Kongruent\" = \"skyblue3\")) +\n    labs(title = \"Reaktionszeiten beim Stroop-test\",\n         subtitle = \"Reaktionszeiten bei unterschiedlicher Farb/Schrift-Kongruenz\",\n         x = \"Kongruenz von Farbe und Schrift\",\n         y = \"Reaktionszeit in Sekunden\") +\n    theme_minimal() +\n    guides(color = FALSE)\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n#Frage: Unterscheidet sich die Geschwindigkeit je nach Farbe?\n\n# Daten zusammenfassen und fehlende Werte entfernen\nd_summary &lt;- d %&gt;%\n    filter(!is.na(rt)) %&gt;%\n    group_by(color) %&gt;%\n    summarise(mean_rt = mean(rt),\n              sd_rt = sd(rt)) # Standardabweichung berechnen\n# glimpse(d_summary)\n\n#Rohdaten verarbeiten damit nur relevanter Bereich gezeigt wird\nd = d %&gt;% filter(rt&lt;1.5)\n\n# Erstellen einer Zuordnung von Farben zu Farben\ncolor_mapping &lt;- c(\"blue\" = \"dodgerblue4\", \"red\" = \"firebrick\", \"yellow\" = \"gold\")\n\n# ggplot mit individuellen Farben f√ºr jeden Balken, Standardabweichung und Rohdaten\n\np &lt;- ggplot(d_summary, aes(y = color, x = mean_rt, fill = color)) +\n    geom_bar(stat = 'identity') +\n    geom_point(data = d, aes(x = rt, group = color), position = position_jitter(width = 0.2), color = \"lightgrey\", size = 2.5, alpha = 0.05)+\n    geom_errorbar(aes(xmin = mean_rt - sd_rt, xmax = mean_rt + sd_rt), width = 0.2)+ # Fehlerbalken f√ºr Standardabweichung\n    geom_text(aes(label = paste(\"mw =\", round(mean_rt, 3)), x = 0.15), vjust = 0, color = \"black\", size = 3) + # Mittelwert als Zahl in die Balken schreiben\n    geom_text(aes(label = paste(\"sd\")), vjust = -0.5, color = \"black\", size = 3)+ # sd beschriften\n    scale_fill_manual(values = color_mapping) +\n    labs(title = \"Stroop-Test\",\n         subtitle = \"Unterscheidet sich die Reaktionsgeschwindigkeit je nach Farbe?\",\n         y = \"Farbe\",\n         x = \"Durchschnittliche Reaktionszeit in s\")+\n    theme_light() +\n    theme(legend.position = \"none\")  # Farbskala entfernen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- d %&gt;% mutate(congruent = as.factor(congruent))\n\npalette &lt;- c (\"#F56598\", \"#74E8CE\")\n\n#Fragestellung: Ist die Reaktionszeit schneller, wenn die Farbe des Worts mit dem Text √ºbereinstimmt?\n# glimpse(d)\nminigrafik = d |&gt;\n    ggplot(mapping =\n           aes(x = congruent,\n               y = rt,\n               color = congruent,\n               fill = congruent)) +\n    geom_jitter(size= 1,\n                alpha = 0.5,) +\n    geom_boxplot(width = 0.3, fill = \"white\", color =\"black\", alpha = 0.5, outlier.colour = \"black\", outlier.shape = NA) +\n    scale_color_manual(values = palette, name = \"Kongruenz\", labels = c(\"Inkongruent = 0\", \"Kongruent = 1\")) +\n    scale_fill_manual(values = palette) +\n\n    # Mittelwert\n    stat_summary(fun=mean,\n                 geom=\"point\",\n                 shape=19,\n                 size=1,\n                 color=\"black\",\n                 fill=\"black\",\n                 position=position_dodge(width=0.5)) +\n\n    # Linie um Mittelwert-Differenz darzustellen\n    stat_summary(fun=mean,\n                geom=\"line\",\n                aes(group=1),\n                linetype=\"solid\",\n                color=\"black\",\n                position=position_dodge(width=0.5)) +\n\n    # Mittelwert beschriften\n    geom_text(aes(label = paste(\"MW:\", round(..y.., 2))),\n              stat = \"summary\",\n              vjust = -0.5,\n              color = \"black\",\n              fill = \"black\",\n              position=position_dodge(width=0.5))  +\n\n    # Achsenbeschriftung\n    labs(title = \"Stroop-Experiment\",\n         subtitle = \"Ist die Reaktionszeit schneller, wenn die Farbe des Worts mit dem Text kongruent ist?\",\n         x = \"Kongruenz von Wort und Farbe\",\n         y = \"Reaktionszeit\",\n         caption = \"Datenquelle: Computerlab Neurowissenschaften\") +\n\n    # Definiert die Achsenskalierung f√ºr die y-Achse\n    scale_y_continuous(limits = c(0, 3), breaks = seq(0, 3, by = 0.5)) +\n\n    guides(fill = FALSE) +\n    theme_classic()\n\nminigrafik\n\n#ggsave(filename = \"grafik_yael_hess.png\",\n#       plot = minigrafik)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nd_grouped_and_summarized &lt;- d %&gt;%\n    group_by(id, congruent) %&gt;%\n    summarise(\n        sum_corr = sum(corr),\n        mean_corr = mean(sum_corr)\n    )\n\np = ggplot(data = d_grouped_and_summarized,\n       aes(x = factor(congruent),\n           y = mean_corr,\n           fill = factor(congruent))) +\n    geom_boxplot() +\n    geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n    labs(x = \"Kongruenz\", y = \"Anzahl der richtigen Antworten\", fill = \"Kongruenz\") +\n    scale_fill_manual(values = c(\"1\" = \"lightblue\", \"0\" = \"lightgreen\"),\n                      labels = c(\"kongruent\", \"inkongruent\")) +\n    scale_x_discrete(labels = c(\"1\" = \"kongruent\", \"0\" = \"inkongruent\")) +  # √Ñndern der Labels auf der x-Achse\n    coord_cartesian(ylim = c(40, 65)) + # limitieren der Spannweite der y-Achse zur besseren √úbersicht\n    theme_minimal() +\n    theme(legend.position = \"right\") +\n    ggtitle(\"Einfluss der Kongruenz auf die Korrektheit der Antwort\") +  # Titel hinzuf√ºgen\n    labs(subtitle = \"Wie unterscheiden sich kongruente Trials von inkongruenten in Bezug auf die H√§ufigkeit korrekter Antworten?\")  # Untertitel hinzuf√ºgen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_point &lt;- d %&gt;%\n    group_by(word, color) %&gt;%\n    na.omit() %&gt;%\n    summarize(mean_rt = mean(rt),\n              accuracy = mean(corr)) %&gt;%\n    mutate(interaction_word_color = factor(interaction(word, color), levels = c(\"blau.blue\", \"rot.red\", \"gelb.yellow\", \"gelb.blue\", \"blau.yellow\", \"rot.blue\", \"blau.red\", \"gelb.red\", \"rot.yellow\")))\n\nd_jitter &lt;- d %&gt;%\n    group_by(word, color) %&gt;%\n    na.omit() %&gt;%\n    summarize(rt = rt,\n              accuracy = mean(corr)) %&gt;%\n    mutate(interaction_word_color = factor(interaction(word, color), levels = c(\"blau.blue\", \"rot.red\", \"gelb.yellow\", \"gelb.blue\", \"blau.yellow\", \"rot.blue\", \"blau.red\", \"gelb.red\", \"rot.yellow\")))\n\nd_boxplot &lt;- d %&gt;%\n    group_by(word, color) %&gt;%\n    na.omit() %&gt;%\n    summarize(rt = rt,\n              accuracy = mean(corr)) %&gt;%\n    mutate(interaction_word_color = factor(interaction(word, color), levels = c(\"blau.blue\", \"rot.red\", \"gelb.yellow\", \"gelb.blue\", \"blau.yellow\", \"rot.blue\", \"blau.red\", \"gelb.red\", \"rot.yellow\")))\n\np &lt;- d %&gt;%\n    ggplot() +\n    geom_point(data = d_point, mapping = aes(x = accuracy, y = mean_rt, color = interaction_word_color), size = 3) +\n    geom_jitter(data = d_jitter, mapping = aes(x = accuracy, y = rt, color = interaction_word_color), alpha = 0.016) +\n    geom_boxplot(data = d_boxplot, mapping = aes(x = accuracy, y = rt, color = interaction_word_color), alpha = 0.02)+\n    scale_color_manual(values = c(\"blue3\", \"red3\", \"gold\", \"darkolivegreen3\", \"chartreuse4\", \"darkorchid1\", \"blueviolet\", \"orange\", \"darkorange\"), name = \"Wort-Farbe-Kombination\", guide_legend(override.aes = list(size = 5))) +\n    labs(y = \"Mittlere Reaktionszeit und Boxplots\", x = \"Accuracy\", title = \"Accuracy und mittlere Reaktionszeit f√ºr jede Wort-Farbe Kombination\", subtitle = \"Unterschiede in accuracy und mittlerer Rektionszeit zwischen verschiedenen Wort-Farbe Kombinationen \\n sowie Verteilung der mittleren Reaktionszeit f√ºr jede Wort-Farbe Kombination\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.border = element_blank(),\n          axis.line = element_line(size = 0.5, color = \"black\"),\n          plot.title = element_text(size = 16)) +\n    theme(panel.grid = element_blank()) +\n    scale_x_continuous(limits = c(0.93, 1), breaks = seq(0.93, 1.02, 0.015)) +\n    scale_y_continuous(limits = c(0.15, 0.85), breaks = seq(0.15, 0.85, 0.05))\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Rohdaten und Zusammenfassung\np &lt;- d %&gt;%\n  ggplot(aes(x = as.factor(congruent), y = rt, fill = as.factor(congruent))) +\n  geom_jitter( width = 0.2, alpha = 0.5, size = 2, color = \"lightgrey\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, position = position_dodge(width = 0.75)) +\n  stat_summary(fun = mean, geom = \"errorbar\", position = position_dodge(width = 0.75), width = 0.3, color = \"black\", size = 1.5) +\n  geom_line(stat = \"summary\", aes(group = 1), position = position_dodge(width = 0.75), linetype = \"dashed\") +\n  geom_text(stat = \"summary\", aes(label = round(..y.., 2)), vjust = -1, position = position_dodge(width = 0.8)) +\n  labs(\n    title = \"Durchschnittliche Reaktionszeiten zwischen den Bedingungen\",\n    subtitle = \"Unterschiede in den durchschnittlichen Reaktionszeiten zwischen kongruent und inkongruent\",\n    x = \"Kongruenz\",\n    y = \"Durchschnittliche Reaktionszeit (in ms)\",\n    fill = \"Kongruenz\"\n  ) +\n  scale_fill_manual(values = c(\"magenta\",\"blue\")) +\n  scale_x_discrete(labels = c(\"Nicht kongruent\",\"Kongruent\")) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n#Frage: Unterscheidet sich die Geschwindigkeit je nach Farbe?\n\n# Daten zusammenfassen und fehlende Werte entfernen\nd_summary &lt;- d %&gt;%\n    filter(!is.na(rt)) %&gt;%\n    group_by(color) %&gt;%\n    summarise(mean_rt = mean(rt),\n              sd_rt = sd(rt))\n# glimpse(d_summary)\n\n#Rohdaten verarbeiten damit nur ein reduzierter Bereich gezeigt wird (dort wo MW & SD liegen)\nd= d %&gt;%\n    filter(rt&lt;1.2)\n\n\n# Erstellen einer Zuordnung von Farben zu Farben\ncolor_mapping &lt;- c(\"blue\" = \"lightskyblue\", \"red\" = \"lightcoral\", \"yellow\" = \"lightgoldenrod1\")\n\n# ggplot mit MW,SD & individuellen Datenpunkten\n\np&lt;- ggplot(d_summary, aes(x = color, y = mean_rt, fill = color)) +\n    geom_bar(stat = 'identity') +\n    geom_point(data = d, aes(y = rt, group = color), position = position_jitter(width = 0.2), color = \"lightgrey\", size = 2.5, alpha=0.05)+\n    geom_errorbar(aes(ymin = mean_rt - sd_rt, ymax = mean_rt + sd_rt), width = 0.2)+ # errorbars=sd\n    scale_fill_manual(values = color_mapping) +\n    labs(title = \"Forschungsfrage\",\n         subtitle = \"Unterscheidet sich die Reaktionsgeschwindigkeit je nach Farbe?\",\n         x = \"Farbe\",\n         y = \"Durchschnittliche Reaktionszeit [ms]\",\n         caption= \"Errorbar= Standardabweichung\")+\n    theme_linedraw()\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Fragestellung:\n# Ist die Reaktionszeit der Versuchspersonen l√§nger bei inkongruenten Items im Vergleich zu kongruenten Items?\n\n# Daten bereinigen und √úberblick verschaffen:\nd_clean &lt;- d %&gt;%\n  filter(!is.na(rt))\n\nsummary_data &lt;- d_clean %&gt;%\n  group_by(congruent) %&gt;%\n  summarize(median_rt = median(rt),\n            mean_rt = mean(rt),\n            sd_rt = sd(rt))\n\n# Anzeigen der summary\n# summary_data\n\n# congruent zu Faktor umwandeln (inkongruent = 0; kongruent = 1)\nd_final &lt;- d_clean %&gt;%\n  mutate(congruent = factor(congruent))\n\n\n# Plot erstellen\np &lt;- ggplot(d_final, aes(x = congruent, y = rt, fill = congruent)) +\n  geom_jitter(width = 0.25, alpha = 0.6, color = \"#000000\", shape = 21, size = 3, stroke = 0.4) +\n  geom_boxplot(alpha = 0.8) +\n  labs(title = \"Reaktionszeiten nach Kongruenz im Stroop Task\",\n       subtitle = \"Ist die Reaktionszeit der Versuchspersonen l√§nger bei inkongruenten Items im Vergleich zu kongruenten Items?\",\n       x = \"Kongruenz\",\n       y = \"Reaktionszeit [s]\",\n       fill = \"Kongruenz\",\n       caption = \"Neurowissenschaft Computerlab - Stroop Task - Universit√§t Bern\") +\n  scale_fill_manual(values = c(\"#9bcd9b\", \"#68829E\"), labels = c(\"inkongruent\", \"kongruent\")) +\n  scale_x_discrete(labels = c(\"inkongruent\", \"kongruent\")) +\n  theme_bw()\n\n# Plot anzeigen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Fragestellung: Reaktionszeit bei kongruenten oder bei inkroguenten Stimuli + mean von rt in Form von Linie einf√ºgen zum Vergleich\n# glimpse(d)\n\n#Variable congruent in Faktor umrechnen weil sonst alle Werte m√∂glich zwischen 0 & 1\nd1 &lt;- d %&gt;%\n    mutate(congruent = as.factor(congruent)) %&gt;%\n    filter(rt &lt; 4 & rt &gt; 0.1)\n\n#Mittelwert von rt bilden um diesen als Vergleich in den plot einzuf√ºgen\nd2 &lt;- d1 %&gt;%\n    summarise(mean_rt = mean(rt),\n              sd_rt = sd(rt))\n#Plot\np = d1 %&gt;%\n    ggplot(mapping = aes(x = congruent, y = rt, color = congruent)) +\n    scale_y_continuous(breaks = seq(0, 4, by = 0.5)) +\n    geom_jitter(width = 0.35, alpha = 0.1) +\n    geom_hline(yintercept = d2$mean_rt, linetype = \"dashed\", color = \"black\", linewidth = 0.8) +\n    scale_color_manual(values = c(\"darkgreen\", \"darkorange\")) +\n    labs(title = \"Kongruente Stimuli gehen mit einer k√ºrzeren Reaktionszeit einher\",\n         subtitle = \"Weist die kongruente Bedingung schnellere Reaktionszeiten auf als die inkongruente?\",\n         x = \"inkongruent vs. kongruent\",\n         y = \"Reaktionszeit in Sekunden\") +\n    guides(color = FALSE) #Ausblenden der Farb-Legende\n    # theme_classic()\np\n\n#glimpse(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# glimpse(d)\n\n#Datensatz gruppiert und summarized anhand der f√ºr die Beantwortung der Fragestellung relevanten Daten.\nd_grouped_and_summarized_data &lt;- d |&gt;\n  mutate(corr = as.factor(corr))|&gt;\n  group_by(id, corr, rt)|&gt;\n  summarise(\n    ncorrect = corr,\n  )\n\n#Plot erstellt anhand des erstellten Datensatzes\np = d_grouped_and_summarized_data |&gt;\n  filter(rt &lt; 5)|&gt;\n  ggplot(mapping =\n           aes(x= ncorrect, y= rt, color = ncorrect)) +\n  geom_jitter(size= 1, width= 0.4) +\n  geom_boxplot(width = 0.3, color= \"black\") +\n  labs(title = \"Reaktionzeiten und falsche Antworten\",\n       subtitle = \"Haben die VP schneller geantwortet wenn sie falsch geantwortet haben?\",\n       x=\"Antwort\",\n       y = \"Reaktionszeit\")+\n  scale_x_discrete(labels = c(\"Falsch\", \"Richtig\"))+\n  theme_bw()\n\n#Ausgabe des Plots\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd$congruent &lt;- as.factor(d$congruent) # changing 'congruent' to factor\n\nd_agr &lt;- d %&gt;% filter(corr == 1)%&gt;%\n    group_by(congruent) %&gt;%\n    na.omit() %&gt;%\n    summarise(mean = mean(rt)) # creating a vector with the mean of all correct responses for congruent and incongruent trials.\n\ncolors_1 &lt;- c(\"#5F8575\", \"#A95C68\") # vector for the colors used in geom_hline and geom_histogram\n\np &lt;- d %&gt;% filter(corr == 1) %&gt;% # filter only correct trials\n    group_by(congruent) %&gt;% ggplot(aes(x = rt, fill = congruent)) +\n    geom_histogram( linetype = 0 ,\n                    alpha=0.5,\n                    position = 'identity',\n                    binwidth= 0.04 ) + # binwidth set to 40ms\n    scale_fill_manual(values=colors_1) +\n    facet_grid(congruent ~ .) + # seperating the distrbutions, as the overlap is messy.\n    xlim(0, 2.5) +\n    geom_vline(data= d_agr , aes(xintercept= mean, color= congruent),\n               linetype=\"solid\", size = 0.8, color = colors_1) + # a line with colors of distribution for the mean\n    theme_minimal() + #theme\n    ggtitle(\"Reaction-times for correct responses by congruence\") + # descriptions\n    labs(fill = \"congruency\",\n         x = \"reaction-time in seconds\",\n         y = \"responses per 40ms\",\n         subtitle = \"Mean and distribution of reaction-times for correct answers, based on word and color congruence (1) or incongruence (0)\")\n\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd$congruent &lt;- factor(d$congruent, levels = c(0, 1), labels = c(\"Nicht-Kongruent\", \"Kongruent\"))\n\n# Zuf√§llige Auswahl von 15 Personen\nrandom_ids &lt;- sample(unique(d$id), 15)\n\nr_d &lt;- d %&gt;%\n    filter(id %in% random_ids)\n\np = r_d |&gt;\n    ggplot(aes(x = congruent, y = rt, fill = congruent)) +\n    geom_jitter(width = 0.3, alpha = 0.5) +\n    geom_boxplot(outlier.color = \"darkorange\", alpha = 0.75) +\n    scale_fill_manual(values = c(\"cornflowerblue\", \"darkmagenta\")) +\n    labs(title = \"Die Reaktionszeiten von der kongruenten und nicht-kongruenten Bedingung\",\n         subtitle = \"Besteht ein Unterschied in den Reaktionszeiten zwischen der kongruenten und nicht-kongruenten Bedingung?\",\n         x = \"Kongruenz\",\n         y = \"Reaktionszeit [s]\") +\n    theme_bw() +\n    theme(legend.position = \"none\")\n\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n[1] \"incongruence\" \"congruence\"  \n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n# Datensatz anschauen\n# glimpse(d)\n\n#zu Factors machen\nd &lt;- mutate(d, color = as.factor(color),\n            condition = as.factor(congruent))\n\nlevels (d$condition) [levels (d$condition) == '0'] &lt;- 'incongruence'\nlevels (d$condition) [levels (d$condition) == '1'] &lt;- 'congruence'\nlevels (d$condition)\n\n# glimpse(d)\n#Diagnostik: Daten untersuchen\n\n#Fehlende Werte\n# install.packages(\"naniar\")\nlibrary(naniar)\n# naniar::vis_miss(d)\n\n#zu schnelle und zu langsame Antworten ausschliessen\nd &lt;- d |&gt;\n    filter(rt &gt; 0.09 & rt &lt; 15)\n\n#Person mit correct 1 ausschliessen\nd &lt;- d |&gt;\n    filter(id != \"sub-80444009\")\n\n# Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, condition, ) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\n#Plot darstellen\nplot1 &lt;- acc_rt_individual %&gt;%\n    ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.6,\n                width = 0, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(incongruence = \"#9467BD\",\n                                  congruence = \"#8FBC8F\")) +\n    labs(x = \"Bedingung\",\n         y = \"Anteil korrekter Antworten\",\n         title = \"Anteil korrekter Antworten \\npro Person und Bedingung\",\n         subtitle = \"Konnten die VPn die Aufgabe l√∂sen?\") +\n    theme_grey(base_size = 12) +\n    theme(legend.position = \"none\")\n\nplot2 &lt;- acc_rt_individual |&gt;\n    ggplot(aes(x = condition, y = median_rt, color = condition)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.6,\n                width = 0, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(incongruence = \"#9467BD\",\n                                  congruence = \"#8FBC8F\")) +\n    labs(x = \"Bedingung\",\n         y = \"Mittlere Reaktionszeit [s]\",\n         title = \"Mittlere Reaktionszeit \\npro Person und Bedingung\",\n         subtitle = \"Wie schnell waren die VPn?\",) +\n    theme_grey(base_size = 12) +\n    theme(legend.position = \"none\")\n\n#Install k√∂nnte man auch oben hinschreiben\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nplot1 + plot2\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd$congruent &lt;- as.factor(d$congruent)\nlevels(d$congruent) &lt;- c(\"inkongruent\", \"kongruent\")\n\np = d %&gt;%\n    ggplot(mapping = aes(congruent,\n                         rt))+\n    geom_jitter(alpha = 0.4, color = \"grey18\")+\n    geom_boxplot(color = \"lightsalmon2\", alpha = 0.5)+\n    labs(x = \"Kongruenz\",\n         y = \"Reaktionszeit\",\n         title = \"Kongruenz macht schneller!\",\n         subtitle = \"Hat die √úbereinstimmung der Farben mit den W√∂rtern einen Einfluss auf die Reaktionszeit?\")+\n    theme_bw()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n#alles zu faktoren machen\nd &lt;- d |&gt;\n    mutate(across(where(is.character), as.factor))\n\n#daten anschauen\n# d |&gt;\n#     slice_head(n = 10)\n\n# Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nd_acc_rt_individual &lt;- d |&gt;\n    group_by(id, congruent) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\n\n# Plot erstellen\n\np &lt;- d_acc_rt_individual |&gt;\n    ggplot(aes(x = congruent, y = median_rt, fill = factor(congruent))) +\n    geom_boxplot(alpha = .5) +\n    geom_jitter(alpha = .25, width = .2) +\n    scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"green\")) +\n    labs(title = \"Inhibitionsf√§higkeit\",\n         subtitle = \"Ist die Reaktionszeit bei der inkongruenten Bedingung verlangsamt?\",\n         x = \"Kongruenz [Nein (0), Ja (1)]\",\n         y = \"Reaktionszeit [ms]\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n#Plot ausgeben\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_id_color &lt;- d |&gt;\n    group_by(id, color) |&gt;\n    summarise(\n        accuracy = mean(corr),\n    )  |&gt;\n    filter(accuracy &gt; 0.5)\n\nd_color &lt;- d |&gt;\n    group_by(color) |&gt;\n    summarise(\n        accuracy = mean(corr),\n    ) |&gt;\n    filter(accuracy &gt; 0.5)\n\np = d |&gt;\n    ggplot(data = d_id_color,\n           mapping = aes(x = color,\n                         y = accuracy,\n                         fill = color)) +\n    geom_violin(show.legend = FALSE,\n                alpha = .4) +\n    scale_fill_manual(values = c(blue = \"blue\",\n                                 yellow = \"yellow\",\n                                 red = \"red\")) +\n    theme_minimal() +\n    geom_point(data = d_color,\n               show.legend = FALSE,\n               size = 3) +\n    labs(title = \"Accuracy in Abh√§ngigkeit von der Farbe, in der das Wort pr√§sentiert wurde\",\n         subtitle = \"Gibt es einen Unterschied in der Accuracy abh√§ngig von der Farbe, in der das Wort pr√§sentiert wurde?\",\n         x = \"Farbe, in der das Wort pr√§sentiert wurde\",\n         y = \"Accuracy\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n# glimpse(d)\n\n#Daten filtern\nd &lt;- d |&gt;\n    filter(rt &gt; 0.09 & rt &lt; 15)\n\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, congruent) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\nn_exclusions &lt;- acc_rt_individual |&gt;\n    filter(N &lt; 40)\n\nd &lt;- d |&gt;\n    filter(!id%in% n_exclusions$id)\n\n#Vorbereitung f√ºr Plot\nreaction_time_zf &lt;- d %&gt;%\n    group_by(congruent) %&gt;%\n    summarise(mean_reaction_time = mean(rt),\n              sd_reaction_time = sd(rt),\n              n = n())\n\naccuracy_zf &lt;- d %&gt;%\n    group_by(congruent) %&gt;%\n    summarise(accuracy = mean(corr))\n\nid_zf &lt;- d %&gt;%\n    group_by(id) %&gt;%\n    summarise(mean_reaction_time = mean(rt),\n              mean_accuracy = mean(corr))\n\n#Plot erstellen\np = ggplot(id_zf, aes(x = mean_reaction_time, y = mean_accuracy)) +\n    geom_point(color=\"lightslategray\") +\n    geom_smooth(method=lm , color=\"orange\", se=TRUE) +\n    labs(title = \"Stroop-Experiment\",\n         subtitle = \"Wie h√§ngen die mittlere Genauigkeit und die mittlere Reaktionszeit zusammen?\",\n         x = \"Mittlere Reaktionszeit\",\n         y = \"Mittlere Genauigkeit\") +\n    theme_minimal()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- mutate(d, congruence = as.factor(congruent),\n                color = as.factor(color))\n\nlevels(d$congruence)[levels(d$congruence) == '0'] &lt;- 'incongruent'\nlevels(d$congruence)[levels(d$congruence) == '1'] &lt;- 'congruent'\n# levels(d$congruence)\n\nd_acc_con_trial &lt;- d |&gt;\n    group_by(congruence, trial) |&gt;\n    summarise(accuracy = mean(corr))\n\nd_acc_con_trial |&gt;\n    ggplot(aes(x = trial, y = accuracy, color = congruence)) +\n    geom_point(size = 2, alpha = 0.8) +\n    geom_line() +\n    scale_color_manual(values = c(incongruent = \"orange2\",\n                                  congruent = \"mediumpurple3\")) +\n    facet_wrap(~ congruence) +\n    labs(x = \"Trial\",\n         y = \"Accuracy\",\n         title = \"Accuracy over time\",\n         subtitle = \"Do participants benefit from learning effects?\")+\n    geom_smooth(method = lm) +\n    theme_gray(base_size = 11)\n\n# d_acc_con_trial\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Fragestellung: Ist die Person schneller, wenn Farbe und Wort kongruent sind? d.h. Variablen congruent und rt relevant\n\n# glimpse(d)\n#Wie viele unterschiedliche Variablen gibt es? -&gt; 8\n#Wie heissen die Variablen? -&gt; id, trial, word, color, congruent, resp, corr, rt\n#Welches Skalenniveau haben sie? -&gt; congruent, corr, word, color = Nominalskaliert; rt = Verh√§ltnisskalsiert\n\n\n#Variable congruent als Faktor definieren\nd &lt;- d %&gt;% mutate(congruent = as.factor(congruent))\n\n\n# Code des Plots:\n## Plot soll Verteilung der Reaktionszeiten sowie die mittlere Reaktionszeit\n## bei den beiden Bedingungen Kongruent = 1 und Kongruent = 0 zeigen\n\np = d %&gt;%\n    ggplot(mapping =\n               aes(x = congruent,\n                   y = rt,\n                   color = congruent,\n                   fill = congruent)) +\n    geom_jitter(size = 1,\n                alpha = 0.7) +\n    geom_boxplot(width = 0.3,\n                 fill = \"white\",\n                 color =\"black\",\n                 outlier.color = \"black\",\n                 outlier.shape = NA,\n                 alpha = 0.5) +\n    scale_color_manual(values = c(\"#B4EEB4\", \"#87CEEB\"), name = \"Kongruenz\",\n                       (labels = c(\"nicht kongruent\", \"kongruent\"))) +\n    scale_fill_manual(values = c(\"#B4EEB4\", \"#87CEEB\")) +\n    stat_summary(fun = mean,\n                 geom = \"point\",\n                 shape = 19,\n                 size = 1.5,\n                 color = \"black\",\n                 fill = \"black\",\n                 position=position_dodge(width=0.5)) +  # Punkt f√ºr Mittelwert\n    stat_summary(fun.data = mean_sdl,\n                 geom = \"errorbar\",\n                 width = 0.2,\n                 color = \"black\",\n                 fill = \"black\",\n                 position = position_dodge(width = 0.5)) +  # Standardabweichung\n    stat_summary(fun = mean,\n                 geom = \"line\",\n                 aes(group = 1),\n                 linetype = \"solid\",\n                 size = 0.7,\n                 color = \"black\",\n                 position = position_dodge(width = 0.8)) +  # Linie f√ºr Mittelwert-Differenz\n    geom_text(aes(label = round(..y.., 2)),\n              stat = \"summary\",\n              vjust = -0.5,\n              color = \"black\",\n              fill = \"black\",\n              position = position_dodge(width = 0.5)) +  # Mittelwert-Beschriftung\n    labs(title = \"Stroop-Experiment\",\n         subtitle = \"Ist die Reaktionszeit k√ºrzer, wenn die Farbe des Worts mit dem Text √ºbereinstimmt?\",\n         x = \"Kongruenz von Wort und Farbe\",\n         y = \"Reaktionszeit\") +\n    scale_y_continuous(limits = c(0, 2.75), breaks = seq(0, 3, by = 0.25)) +  # Definiert die Achsenskalierung f√ºr die y-Achse\n    theme_classic() +\n    guides(fill = FALSE)\np\n\n#ggsave(filename = \"thomann_laura_plot.png\",\n#       plot = p)\n\n\n\n\n\nPlotCode\n\n\n\n\n[1] \"incongruent\" \"congruent\"  \n\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Datenbereinigung und Faktorenbenennung: Reaktionszeiten unter 100ms und √ºber 10s\n# werden ausgeschlossen, Reduktion von 31800 zu 31780 observations\nd &lt;-  mutate(d, color = as.factor(color),\n             condition = as.factor(congruent))%&gt;%\n    filter(rt &gt; 0.09 & rt &lt; 10.01)\n\n# Umbennenung der Faktoren von \"Condition\"\nlevels(d$condition)[levels(d$condition) == '0'] &lt;- 'incongruent'\nlevels(d$condition)[levels(d$condition) == '1'] &lt;- 'congruent'\nlevels(d$condition)\n\n# Suche nach fehlenden Werten (NA), keine gefunden, daher muss nichts ausgeschlossen werden\nd_missings &lt;- d %&gt;% naniar::add_label_missings() %&gt;%\n    filter(any_missing == \"Missing\")\n\n# head(d_missings)\n\n#Vorbereitung der Daten mit Accuracy als Mittelwert der Korrekten Antworten und Median_rt als Median der Reaktionszeit\nd_acc_rt_trial &lt;- d %&gt;%\n    group_by(condition, trial) %&gt;%\n    summarise(\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\n#Darstellung der Erm√ºdungseffekte mit einem Line-Plot\nerm√ºdungseffekte &lt;- d_acc_rt_trial %&gt;%\n    ggplot(aes(x = trial, y = accuracy, color = condition)) +\n    geom_point(size = 1.5, alpha = 0.6) +\n    geom_line() +\n    scale_color_manual(values = c(incongruent = \"tomato4\",\n                                  congruent = \"green4\")) +\n    facet_wrap(~ condition) +\n    labs(x = \"Trial\",\n         y = \"Accuracy\",\n         title = \"Accuracy in the Conditions\",\n         subtitle = \"Are there strong effects of fatigue in the two conditions?\") +\n    geom_smooth(method = lm) +\n    theme_minimal(base_size =10) +\n    theme(legend.position = \"none\")\nerm√ºdungseffekte\n\n#Darstellung der Verteilung der Median Reaction Times\nhisto_rt_and_accuracy &lt;- d_acc_rt_trial %&gt;%\n    ggplot(aes(x = median_rt, fill = condition)) +\n    geom_histogram()+\n    scale_fill_manual(values = c(incongruent = \"tomato4\",\n                                  congruent = \"green4\")) +\n    labs(x = \"Median Reaction Time [s]\",\n         y = \"Count\",\n         title = \"Distribution of Median Reaction Times over the conditions\",\n         subtitle = \"Was the reaction time shorter in the congruent condition?\") +\n    theme_light()\nhisto_rt_and_accuracy\n\n#Darstellung der Reaktionszeiten von 3 Teilnehmern √ºber die Konditionen hinweg mit line plot und Trendlinie\nrt_of_three_participants &lt;- d %&gt;%\n    filter(id %in% c(\"sub-10318869\", \"sub-1106725\", \"sub-113945\")) %&gt;%\n    ggplot(aes(x = trial, y = rt, color = condition)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_point(alpha = 0.5) +\n    scale_color_manual(values = c(incongruent = \"tomato3\",\n                                  congruent = \"green4\")) +\n    facet_wrap(~ id) +\n    labs(x = \"Trial\",\n         y = \"Reaction Time [s]\",\n         title = \"Reaction time of 3 participants\",\n         subtitle = \"Was the reaction time shorter in the congruent condition for specific participants?\") +\n    theme_light()\n# rt_of_three_participants\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Filtern der relevanten Variablen\nben√∂tigte_daten &lt;- d[, c(\"congruent\", \"rt\")]\n\n# Anzeigen der ersten Zeilen des gefilterten Datensatzes\n# head(ben√∂tigte_daten)\n\n# Umwandlung der Variable 'congruent' in einen Faktor\nben√∂tigte_daten$congruent &lt;- factor(ben√∂tigte_daten$congruent, levels = c(0, 1), labels = c(\"incongruent\", \"congruent\"))\n\n# Erstellung des Plots\nplot&lt;- ggplot(ben√∂tigte_daten, aes(x = congruent, y = rt, fill = congruent, color = congruent)) +\n  # Hinzuf√ºgen der Balken f√ºr die Mittelwerte und Standardabweichungen\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"bar\", size = 1.5, color = \"black\") +\n  # Hinzuf√ºgen der rohen Daten als Punkte\n  geom_jitter(width = 0.2) +\n  # Hinzuf√ºgen von Linien f√ºr die Standardabweichungen\n  stat_summary(fun.data = mean_sdl, geom = \"errorbar\", width = 0.2, size = 1, color = \"black\") +\n  # Farben\n  scale_fill_manual(values = c(\"congruent\" = \"blue\", \"incongruent\" = \"red\")) +\n  scale_color_manual(values = c(\"congruent\" = \"blue\", \"incongruent\" = \"red\")) +\n  # Beschriftungen\n  labs(title = \"Stroop-Test: Reaktionszeit nach Bedingung\",\n       subtitle = \"Unterschiede in der Reaktionszeit zwischen kongruenten und inkongruenten Bedingungen\",\n       x = \"Bedingung\", y = \"Reaktionszeit (ms)\") +\n  # Design\n  theme_get()\n\n#anzeigen lassen\nprint(plot)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht ver√§ndert werden\n# --------------------------------------------------------------------------------------\n# Beginnen Sie hier mit Ihrem Code:\n\n# glimpse(d)\n\n# Daten nach Farbe gruppieren\nd &lt;- d %&gt;%\n  group_by(color)\n\n# Daten plotten\nggplot(d, aes(x = color, y = rt)) +\n  geom_boxplot(width = 0.8, outlier.shape = NA) +\n  labs(title = \"Reaktionszeit nach Korrektheit der Antwort und Farbe\", subtitle = \"Wurden die Antworten bei jeder Farbe gleich schnell gegeben?\", x = \"Farbe\", y = \"Reaktionszeit [s]\") +\n  geom_jitter(aes(color = color), width = 0.2) +\n  coord_cartesian(ylim = c(0, 4.51)) + # rt &gt; 4.51 werden nicht in die Auswertung miteinbezogen\n  scale_x_discrete(labels = c(\"blue\" = \"blau\", \"red\" = \"rot\", \"yellow\" = \"gelb\")) +\n  scale_color_manual(values = c(\"blue\" = \"blue\", \"red\" = \"red\", \"yellow\" = \"yellow\")) +\n  guides(color = FALSE) +\n  theme_minimal()  +\n  facet_wrap(~ corr, labeller = labeller(corr = c(\"0\" = \"Falsche Antwort\", \"1\" = \"Korrekte Antwort\")))\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\ngrouped_and_summarized_data&lt;- d |&gt;\n    group_by(id,congruent) |&gt;\n    summarise(mean_rt= mean(rt))\n\ndata_kongruent &lt;- filter(grouped_and_summarized_data, congruent == 0)\ndata_inkongruent &lt;- filter(grouped_and_summarized_data, congruent == 1)\n\np = ggplot(data = grouped_and_summarized_data,\n       aes(x = factor(congruent),\n           y = mean_rt,\n           fill = factor(congruent))) +\n    geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n    geom_point(position = position_jitter(width = 0.1), size = 2, alpha = 0.5) +\n    labs(x = \"Bedingung\", y = \"Mittelwert der Reaktionszeit\", fill = \"Bedingung\") +\n    ggtitle(\"Mittelwerte der Reaktionszeit nach Bedingung\") +\n    scale_fill_manual(name = \"Bedingung\", values = c(\"0\" = \"pink\", \"1\" = \"yellow\"),\n                      labels = c(\"0\" = \"Inkongruent\", \"1\" = \"Kongruent\")) +\n    theme_linedraw() +\n    scale_x_discrete(name = \"Bedingung\", labels = c(\"0\" = \"Inkongruent\", \"1\" = \"Kongruent\")) +\n    scale_y_continuous(breaks = seq(0, 2, by = 0.2)) +\n    labs(x = \"Bedingung\", y = \"Mittelwert der Reaktionszeit\", fill = \"Bedingung\",\n     subtitle = \"Wurde bei kongruent schneller geantwortet als bei inkongruent?\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Package laden\n\nlibrary(tidyverse)\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\n# Daten einlesen und Textvariablen zu Faktoren umwandeln\n# d |&gt; #&lt;- read.csv(\"data/dataset_stroop_clean.csv\") |&gt;\n  # mutate(across(where(is.character), as.factor))\n\n#Median von den Reaktionszeiten erstellen/berechnen\nacc_rt_individual &lt;- d |&gt;\n  group_by(id, congruent) |&gt;\n  summarise(\n    N = n(),\n    ncorrect = sum(corr),\n    accuracy = mean(corr),\n    median_rt = median(rt)\n  )\n\n# Datensatz anschauen\n# acc_rt_individual |&gt;\n  # slice_head(n = 10)\n\n#Aufgabenschwierigkeit und Performanz der Versuchspersonen\n\n# Beginnen Sie hier mit Ihrem Code:\np1 &lt;- acc_rt_individual |&gt;\n  na.omit()|&gt;\n  ggplot(aes(x = factor(congruent, levels = c(0,1)), y = median_rt, color = factor(congruent))) +\n    geom_jitter(size = 2, alpha = 0.4,\n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_x_discrete(labels =c(\"0\" = \"inkongruent\", \"1\" = \"kongruent\"))+\n    scale_color_manual(values = c(\"0\" = \"green3\",\n                                \"1\" = \"skyblue3\"))+\n    labs(x =\"Bedingung\",\n         y = \"Median Reaktionszeit [s]\",\n         title = \"Median Reaktionszeit pro Person and Bedingung\",\n         subtitle = \"Wie ver√§ndert sich die Reaktionszeit der Versuchspersonen \\n in Abh√§ngigkeit der Bedingung?\")+\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n\nlibrary(patchwork)\n\np1\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\nmy_palette &lt;- c(\"blue\", \"red\", \"yellow\")\n\nd_rt_within &lt;- d |&gt;\n    mutate(congruent = as.factor(congruent)) |&gt;\n    group_by(id, congruent, color) |&gt;\n    summarise(mean_rt = mean(rt))\n\n\np = d_rt_within |&gt;\n    Rmisc::summarySEwithin(measurevar = \"mean_rt\",\n                           withinvars = \"congruent\",\n                           na.rm = TRUE,\n                           conf.interval = 0.95)|&gt;\n    ggplot(mapping = aes(\n        x = congruent,\n        y = mean_rt)) +\n    geom_jitter(data = d_rt_within,\n                aes(colour = factor(color)))+\n    geom_boxplot(data = d_rt_within,\n                 aes(colour = factor(color)),\n                 outlier.alpha = 0.5) +\n    geom_errorbar(width = .1, aes(ymin = mean_rt-se, ymax = mean_rt+se)) +\n    geom_point() +\n    scale_color_manual(values = my_palette) +\n    theme_bw(base_size = 10) +\n    ggtitle(\"Stroop Experiment\") +\n    xlab(\"√úbereinstimmung zwischen Schriftfarbe und Wort [0 = inkongruent; 1 = kongruent]\") +\n    ylab(\"Durchschnittliche Reaktionszeit [sec]\") +\n    labs(subtitle = \"Antworten Personen schneller, wenn die Schriftfarbe mit dem Wort √ºbereinstimmt?\",\n         colour = \"Schriftfarbe\",\n         caption = \"black dot = mean_rt, black lines = standard error (se)\")\n\np + scale_y_continuous(breaks = seq(0.5, 2.3, by = 0.25))\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n## Wie verh√§lt sich die Reaktionszeit und deren Standardabweichungen bei Ausreisser\n\nd &lt;- d |&gt;\n    filter(rt &gt; 0.1 & rt&lt;5)\n\nd_corr_individual &lt;- d |&gt;\n    group_by(id, corr) |&gt;\n    summarise(\n        rt = rt,\n        median_rt = median(rt),\n        sd_rt = sd(rt)) %&gt;%\n        mutate(corr= as.factor(corr))\n\n\np&lt;- d_corr_individual %&gt;%\n  filter(id %in% c(\"sub-13366559\", \"sub-19843991\", \"sub-66321563\", \"sub-21474524\")) %&gt;%\n  ggplot(aes(x = corr, y = rt, color = id)) +\n  geom_jitter(alpha = 1, width = 0.2) +\n  geom_boxplot(alpha = 0, width = 0.2, color = \"black\") +\n  scale_fill_manual(values = c(\"sub-13366559\" = \"tomato3\",\n                               \"sub-19843991\" = \"skyblue3\",\n                               \"sub-66321563\" = \"pink\" ,\n                               \"sub-21474524\" = \"green\")) +\n  labs(title = \"Ausreisser\",\n       subtitle = \"Wie verh√§lt sich die Reaktionszeit und deren Standardabweichungen bei Ausreisser\",\n       x = \"Korrektheit\",\n       y = \"Reaktionszeit\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n\n\np = d |&gt;\n    ggplot(aes(x = as.factor(congruent), y = rt, color = as.factor(congruent))) +\n    geom_boxplot(alpha = 0.5, outlier.shape = NA, width = 0.5) +\n    geom_jitter(width = 0.1, alpha = 0.5) +  # √úberlappung der Punkte reduzieren\n    stat_summary(\n        fun.data = mean_se, geom = \"errorbar\", width = 0.2,\n        aes(group = congruent)\n    ) +  # Fehlerbalken f√ºr den Mittelwert +/- Standardfehler\n    stat_summary(\n        fun = mean, geom = \"point\", size = 3, shape = 18,\n        aes(group = congruent), color = \"black\"\n    ) +  # Mittelwerte als Punkte\n    labs(\n        title = \"Einfluss der Kongruenz von Wort und Farbe auf die Reaktionszeiten\",\n        subtitle = \"Wie unterscheiden sich die Reaktionszeiten zwischen kongruenten und inkongruenten Bedingungen?\",\n        x = \"Kongruenz (1 = kongruent, 0 = inkongruent)\",\n        y = \"Reaktionszeit (ms)\",\n        color = \"Kongruenz\"  # Legendenbeschriftung auf Deutsch\n    ) +\n    scale_color_manual(values = c(\"1\" = \"green\", \"0\" = \"red\")) +  # Farben anpassen\n    theme_light() +\n    theme(\n        legend.title = element_text(size = 12),  # Gr√∂sse der Legenden√ºberschrift anpassen\n        legend.position = \"top\",  # Legende oben platzieren\n        plot.title = element_text(size = 14, face = \"bold\"),  # Titel gr√∂sser und fett\n        axis.title = element_text(size = 12),  # Achsentitelgr√∂√üe anpassen\n        axis.text = element_text(size = 10)  # Achsenbeschriftungsgr√∂√üe anpassen\n    )\n\n\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n[1] \"incongruence\" \"congruence\"  \n\n\n\n\n\n\n\n\n# Beginnen Sie hier mit Ihrem Code:\n#Zu Factors machen\nd &lt;- mutate(d, color = as.factor(color),\n            condition = as.factor(congruent))\n\n#Diagnostik: Daten untersuchen\n#zu schnelle und zu langsame Antworten ausschliessen\nd &lt;- d |&gt;\n    filter(rt &gt; 0.09 & rt &lt; 15)\n\n#Person mit correct 1 ausschliessen\nd &lt;- d |&gt;\n    filter(id != \"sub-80444009\")\n\n#Benennung √§ndern\nlevels (d$condition) [levels (d$condition) == '0'] &lt;- 'incongruence'\nlevels (d$condition) [levels (d$condition) == '1'] &lt;- 'congruence'\nlevels (d$condition)\n\n#Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\n#Plot darstellen\nplot1 &lt;- acc_rt_individual %&gt;%\n    ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8,\n                width = 0, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(incongruence = \"#E9967A\",\n                                  congruence = \"#8FBC8F\")) +\n    labs(x = \"Bedingung\",\n         y = \"Anteil korrekter Antworten\",\n         title = \"Anteil korrekter Antworten pro Person und Bedingung\",\n         subtitle = \"Konnten die VPN die Aufgabe korrekt l√∂sen?\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\nplot2 &lt;- acc_rt_individual %&gt;%\n    ggplot(aes(x = condition, y = median_rt, color = condition)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8,\n                width = 0, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(incongruence = \"#E9967A\",\n                                  congruence = \"#8FBC8F\")) +\n    labs(x = \"Bedingung\",\n         y = \"Mittlere Reaktionszeit [s]\",\n         title = \"Mittlere Reaktionszeit pro Person und Bedingung\",\n         subtitle = \"Wie schnell reagierten die VPN?\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nplot1 + plot2\n\n\n\n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{fitze,\n  author = {Fitze, Daniel and Wyssen, Gerda},\n  title = {Plot {Gallery} - {Mo}},\n  url = {https://kogpsy.github.io/neuroscicomplabFS24//pages/chapters/gallery_mo.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFitze, Daniel, and Gerda Wyssen. n.d. ‚ÄúPlot Gallery - Mo.‚Äù\nhttps://kogpsy.github.io/neuroscicomplabFS24//pages/chapters/gallery_mo.html."
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html",
    "href": "pages/chapters/goodpractices_data.html",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "In der neurowissenschaftlichen Forschung werden zunehmend sehr grosse und komplexe Datens√§tze generiert. Daten aus unterschiedlichen Datenerhebungsverfahren sollen miteinander verkn√ºpft (aggregiert) werden, um neue Erkenntnisse zu gewinnen. Eine sehr h√§ufige Kombination sind beispielsweise Verhaltens- und Bildgebungsdaten, wie es in vielen fMRI-Studien der Fall ist. Das erfordert Kenntnisse der unterschiedlichen Formate und Eigenschaften dieser Daten sowie Programmierkenntnisse um diese Daten m√∂glichst automatisiert vorzuverarbeiten, zu verkn√ºpfen, visualisieren und analysieren.\n\nDefinition Datenmanagement\n\n\n\n\n\n\n\nHands-on: Herausforderungen von neurowissenschaftlichen Daten\n\n\n\nLesen Sie den untenstehenden Abschnitt aus Pierr√© et al. (2024). Besprechen Sie in Gruppen, welche spezifischen Herausforderungen Datenmanagement, -vorverarbeitung und -analyse in den Neurowissenschaften bestehen.\n\n\n\nIncreasing complexity of neuroscience data\nOver the past 20 years, neuroscience research has been radically changed by two major trends in data production and analysis.\nFirst, neuroscience research now routinely generates large datasets of high complexity. Examples include recordings of activity across large populations of neurons, often with high resolution behavioral tracking (Steinmetz et al., 2019; Stringer et al., 2019; Mathis et al., 2018; Siegle et al., 2021; Koch et al., 2022), analyses of neural connectivity at high spatial resolution and across large brain areas (Scheffer et al., 2020; Loomba et al., 2022), and detailed molecular profiling of neural cells (Yao et al., 2023; Langlieb et al., 2023; Braun et al., 2022; Callaway et al., 2021). Such large, multi-modal data sets are essential for solving major questions about brain function (Brose, 2016; Jorgenson et al., 2015; Koch and Jones, 2016).\nSecond, the collection and analysis of such datasets requires interdisciplinary teams, incorporating expertise in systems neuroscience, engineering, molecular biology, data science, and theory. These two trends are reflected in the increasing numbers of authors on scientific publications (Wareham, 2016), and the creation of mechanisms to support team science by the NIH and similar research funding bodies (Cooke and Hilton, 2015; Volkow, 2022; Brose, 2016).\nThere is also an increasing scope of research questions that can be addressed by aggregating ‚Äúopen data‚Äù from multiple studies across independent labs. Funding agencies and publishers have begun to aggressively promote data sharing and open data, with the goals of improving reproducibility and increasing data reuse (Dallmeier-Tiessen et al., 2014; Tenopir et al., 2015; Pasquetto et al., 2017). However, open data may be unusable if scattered in a wide variety of naming conventions and file formats lacking machine-readable metadata.\nBig data and team science necessitate new strategies for how to best organize data, with a key technical challenge being the development of standardized file formats for storing, sharing, and querying datasets. Prominent examples include the Brain Imaging Data Structure (BIDS) for neuroimaging, and Neurodata Without Borders (NWB) for neurophysiology data (Teeters et al., 2015; Gorgolewski et al., 2016; R√ºbel et al., 2022; Holdgraf et al., 2019). The Open Neurophysiology Environment (ONE), best known from adoption by The International Brain Laboratory (The International Brain Laboratory et al., 2020, 2023), has a similar application domain to NWB, but a highly different technical design. (‚Ä¶)\nThese initiatives provide technical tools for storing and accessing data in known formats, but more importantly provide conceptual frameworks with which to standardize data organization and description in an (ideally) universal, interoperable, and machine-readable way. Pierr√© et al. (2024) (Preprint)\n\n\n\nNeurowissenschaftliche Daten von Verhaltensdaten zu Bildgebungsdaten stammen aus unterschiedlichsten Quellen und haben alle spezifische Eigenschaften, die in der Datenverarbeitung ber√ºcksichtigt werden m√ºssen. Das bedeutet, dass sehr unterschiedliche Formate miteinander verkn√ºpft werden m√ºssen f√ºr die Analyse. Bei der Generierung von Daten sind oft auch Personen aus unterschiedlichen Fachrichtungen beteiligt, welche andere Hintergr√ºnde bez√ºglich Datenmanagement haben (z.B. bei fMRI Experimenten Fachpersonen aus den Bereichen Radiologie, Physik, Neurowissenschaften, Psychologie, Medizin, etc.). Auch bei der Analyse sind oft verschiedene Personen beteiligt, die alle darauf angewiesen sind, den Datensatz zu verstehen.\n\n\n\nNeurowissenschaftliche Datens√§tze sind oft sehr gross (pro Versuchsperson oft mehrere Gigabytes) und f√ºr die Speicherung und das Safety Back-up wird also viel Speicherplatz ben√∂tigt (mehrere Terrabytes). Komplexe Datens√§tze bedeutet, dass eine gute Dokumentation n√∂tig ist, welche Variable welche Bedeutung hat und wo gegebenenfalls √Ñnderungen gemacht wurden.\n\n\n\nFalls dabei keine Pers√∂nlichkeitsrechte verletzt werden, sollten Daten m√∂glichst zug√§nglich gemacht werden, um einerseits transparente Resultate zu erm√∂glichen und andererseits kann so ein Datensatz auch von anderen Forschenden f√ºr weitere Fragestellung verwendet werden. Dies ist aber nur m√∂glich, wenn der Datensatz f√ºr Aussenstehende verst√§ndlich abgespeichert wurde.\n\n\n\nVon der Datenerhebung bis zur Datenanalyse werden die Daten oft ausf√ºhrlich vorverarbeitet. Dazu geh√∂ren u.a. folgende Schritte (wobei jeder eigene Fehlerquellen beinhalten kann):\n\nImportieren der Rohdaten\nVer√§ndern des Formats (z.B. .dcm -&gt; .nii in fMRI)\nIdentifikation von Missings\nAusschluss von ung√ºltigen Messungen\nAnonymisierung\nWeglassen nicht ben√∂tigter Datenpunkte\nFehlende Werte erg√§nzen\nDuplizierungen identifizieren und l√∂schen\nMetadaten hinzuf√ºgen\nDatens√§tze zusammenf√ºgen\nSpalten teilen\nVariablen umbenennen\nVariablen normalisieren/standardisieren\nVariablen ver√§ndern (z.B. ms zu sec)\nVariablentypen anpassen\nVariablen recodieren\nNeue Variablen erstellen\nNeuer Datensatz abspeichern\n\n\n\n\n\n\n\nTipp: Datencheck\n\n\n\nBevor mit einem Datensatz gearbeitet wird, empfiehlt es sich den Datensatz anzuschauen und folgendes zu identifizieren:\n\nIn welchem Dateiformat ist der Datensatz gespeichert? z.B. in .csv, .xlsx oder anderen?`\nIn welchem Datenformat ist der Datensatz geordnet? (long oder wide oder mixed?)\nGibt es ein data dictionnary mit Erkl√§rungen zu den Variablen?\n\n\n\n\n\n\nEs gibt zahlreiche Bem√ºhungen Datenmanagement in den Neurowissenschaften zu vereinheitlichen. Viele verf√ºgbare Standards und Tools machen die Datenverarbeitung aber nicht nur einfacher. Im n√§chsten Kapitel finden Sie deshalb eine √úbersicht von Ans√§tzen, die uns sinnvoll und allgemein anwendbar erscheinen und sich in unseren Labors bew√§hrt haben.\n xkcd Comic"
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#datenquellen-datenformate-und-interdisziplinari√§t",
    "href": "pages/chapters/goodpractices_data.html#datenquellen-datenformate-und-interdisziplinari√§t",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Neurowissenschaftliche Daten von Verhaltensdaten zu Bildgebungsdaten stammen aus unterschiedlichsten Quellen und haben alle spezifische Eigenschaften, die in der Datenverarbeitung ber√ºcksichtigt werden m√ºssen. Das bedeutet, dass sehr unterschiedliche Formate miteinander verkn√ºpft werden m√ºssen f√ºr die Analyse. Bei der Generierung von Daten sind oft auch Personen aus unterschiedlichen Fachrichtungen beteiligt, welche andere Hintergr√ºnde bez√ºglich Datenmanagement haben (z.B. bei fMRI Experimenten Fachpersonen aus den Bereichen Radiologie, Physik, Neurowissenschaften, Psychologie, Medizin, etc.). Auch bei der Analyse sind oft verschiedene Personen beteiligt, die alle darauf angewiesen sind, den Datensatz zu verstehen."
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#grosse-und-komplexe-datens√§tze",
    "href": "pages/chapters/goodpractices_data.html#grosse-und-komplexe-datens√§tze",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Neurowissenschaftliche Datens√§tze sind oft sehr gross (pro Versuchsperson oft mehrere Gigabytes) und f√ºr die Speicherung und das Safety Back-up wird also viel Speicherplatz ben√∂tigt (mehrere Terrabytes). Komplexe Datens√§tze bedeutet, dass eine gute Dokumentation n√∂tig ist, welche Variable welche Bedeutung hat und wo gegebenenfalls √Ñnderungen gemacht wurden."
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#open-science",
    "href": "pages/chapters/goodpractices_data.html#open-science",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Falls dabei keine Pers√∂nlichkeitsrechte verletzt werden, sollten Daten m√∂glichst zug√§nglich gemacht werden, um einerseits transparente Resultate zu erm√∂glichen und andererseits kann so ein Datensatz auch von anderen Forschenden f√ºr weitere Fragestellung verwendet werden. Dies ist aber nur m√∂glich, wenn der Datensatz f√ºr Aussenstehende verst√§ndlich abgespeichert wurde."
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#zahlreiche-zwischenschritte",
    "href": "pages/chapters/goodpractices_data.html#zahlreiche-zwischenschritte",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Von der Datenerhebung bis zur Datenanalyse werden die Daten oft ausf√ºhrlich vorverarbeitet. Dazu geh√∂ren u.a. folgende Schritte (wobei jeder eigene Fehlerquellen beinhalten kann):\n\nImportieren der Rohdaten\nVer√§ndern des Formats (z.B. .dcm -&gt; .nii in fMRI)\nIdentifikation von Missings\nAusschluss von ung√ºltigen Messungen\nAnonymisierung\nWeglassen nicht ben√∂tigter Datenpunkte\nFehlende Werte erg√§nzen\nDuplizierungen identifizieren und l√∂schen\nMetadaten hinzuf√ºgen\nDatens√§tze zusammenf√ºgen\nSpalten teilen\nVariablen umbenennen\nVariablen normalisieren/standardisieren\nVariablen ver√§ndern (z.B. ms zu sec)\nVariablentypen anpassen\nVariablen recodieren\nNeue Variablen erstellen\nNeuer Datensatz abspeichern\n\n\n\n\n\n\n\nTipp: Datencheck\n\n\n\nBevor mit einem Datensatz gearbeitet wird, empfiehlt es sich den Datensatz anzuschauen und folgendes zu identifizieren:\n\nIn welchem Dateiformat ist der Datensatz gespeichert? z.B. in .csv, .xlsx oder anderen?`\nIn welchem Datenformat ist der Datensatz geordnet? (long oder wide oder mixed?)\nGibt es ein data dictionnary mit Erkl√§rungen zu den Variablen?"
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#verschiedene-standards",
    "href": "pages/chapters/goodpractices_data.html#verschiedene-standards",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Es gibt zahlreiche Bem√ºhungen Datenmanagement in den Neurowissenschaften zu vereinheitlichen. Viele verf√ºgbare Standards und Tools machen die Datenverarbeitung aber nicht nur einfacher. Im n√§chsten Kapitel finden Sie deshalb eine √úbersicht von Ans√§tzen, die uns sinnvoll und allgemein anwendbar erscheinen und sich in unseren Labors bew√§hrt haben.\n xkcd Comic"
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#reproduzierbarkeit",
    "href": "pages/chapters/goodpractices_data.html#reproduzierbarkeit",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "Reproduzierbarkeit",
    "text": "Reproduzierbarkeit\nDie Replikationskrise hat in der Psychologie, aber auch in den kognitiven Neurowissenschaften ein Umdenken ausgel√∂st. Mit dem Ziel nachhaltigere Forschungsergebnisse zu erreichen sind verschiedene Begriffe wie Reproduzierbarkeit und Replizierbarkeit zu wichtigen Schlagworten geworden. Die Begrifflichkeiten werden verwirrenderweise aber oft unterschiedlich definiert und verwendet (Plesser (2018)).\n\n\n\n\n\n\n\n\n\n\nReplizierbarkeit (replicability) bedeutet, dass ein Experiment von einer anderen Forschungsgruppe mit einer neuen Stichprobe durchgef√ºhrt werden kann, und √§hnliche oder dieselben Resultate hervorbringt, wie die Originalstudie. Wird eine Studie mehrmals repliziert, steigt die Wahrscheinlichkeit, dass kein Zufallsbefund vorliegt.\n\nReplicability refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected. Cacioppo et al. (2015)\n\nReproduzierbarkeit (reproducibility) h√§ngt eng mit der Replizierbarkeit zusammen. Der Begriff wird teilweise sehr allgemein verwendet, und bedeutet so dass Forschungsergebnisse wiederholt gefunden werden. Reproduzierbarkeit im engeren Sinn hingegen bezieht sich darauf, ob die durchgef√ºhrte Analyse wiederholt werden kann. Die Reproduzierbarkeit ist somit hoch, wenn Forschende die Daten und Datenanalyseskripts bereitstellen und andere Forschende damit dieselben Analysen durchf√ºhren k√∂nnen und zu gleichen Resultaten kommen.\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results‚Ä¶. Reproducibility is a minimum necessary condition for a finding to be believable and informative. Cacioppo et al. (2015)\n\nUm die Begriffe zusammenzufassen schlugen Goodman, Fanelli, and Ioannidis (2016) vor von\n\nReproduzierbarkeit der Methoden (Daten und Prozesse k√∂nnen exakt wiederholt werden)\nReproduzierbarkeit der Resultate (andere Studien kommen auf dieselben Resultate) und\nReproduzierbarkeit der wissenschaftlichen Schlussfolgerung (bei Repetition der Analyse oder der Experimente werden dieselben Schl√ºsse gezogen)\n\nzu sprechen. Die Reproduzierbarkeit von Resultaten und Schlussfolgerungen ist hier nicht klar abgrenzbar vom Begriff der Replizierbarkeit. Grunds√§tzlich besteht das Ziel, dass in der Forschung m√∂glichst viel Evidenz f√ºr eine Schlussfolgerung gesammelt werden kann. Dies gelingt, wenn die Prozesse transparent, fehlerfrei und wiederholbar sind.\n\nHindernisse bei der Reproduzierbarkeit\nIn diesem Kurs besch√§ftigen wir uns vor allem mit der Reproduzierbarkeit der Methoden als Grundlage f√ºr solide Erkenntnisgewinne. Reproduzierbarkeit kann laut Nosek et al. (2022) vor allem aus zwei Gr√ºnden nicht gegeben sein: Weil die Daten/Skripte nicht zur Verf√ºgung stehen, oder weil diese Fehler enthalten:\n\nIn principle, all reported evidence should be reproducible. If someone applies the same analysis to the same data, the same result should occur. Reproducibility tests can fail for two reasons. A process reproducibility failure occurs when the original analysis cannot be repeated because of the unavailability of data, code, information needed to recreate the code, or necessary software or tools. An outcome reproducibility failure occurs when the reanalysis obtains a different result than the one reported originally. This can occur because of an error in either the original or the reproduction study. Nosek et al. (2022)\n\nF√ºhrt die Reproduktion nicht zum selben Resultat, l√∂st das Zweifel am Forschungsergebnis aus. Wenn die Reproduzierbarkeit am Prozess scheitert, etwa weil die Daten nicht vorhanden sind, kann kein Schluss gezogen werden, ob die Resultate stimmen.\n\nAchieving reproducibility is a basic foundation of credibility, and yet many efforts to test reproducibility reveal success rates below 100%. ‚Ä¶ Whereas an outcome reproducibility failure suggests that the original result may be wrong, a process reproducibility failure merely indicates that the original result cannot be verified. Either reason challenges credibility and increases uncertainty about the value of investing additional resources to replicate or extend the findings (Nuijten et al.¬†2018). Sharing data and code reduces process reproducibility failures (Kidwell et al.¬†2016), which can reveal more outcome reproducibility failures (Hardwicke et al.¬†2018, 2021; Wicherts et al.¬†2011). Nosek et al. (2022)\n\nDas Teilen von Daten und Datenverarbeitungsskripten erh√∂ht demnach die Wahrscheinlichkeit, dass m√∂gliche Fehler im Prozess gefunden werden, da auch andere Forschende die Daten/Skripts verwenden k√∂nnen. Das ist vorerst unangenehm, geh√∂rt aber zum wissenschaftlichen Prozess dazu. Die Normalisierung einer Fehlerkultur in diesem Bereich w√ºrde indirekt auch die Replizierbarkeit von Ergebnissen erh√∂hen.\n\nNeuroimaging experiments result in complicated data that can be arranged in many different ways. So far there is no consensus how to organize and share data obtained in neuroimaging experiments. Even two researchers working in the same lab can opt to arrange their data in a different way. Lack of consensus (or a standard) leads to misunderstandings and time wasted on rearranging data or rewriting scripts expecting certain structure. BIDS Website (2024)\n\n\n\nTools f√ºr Reproduzierbarkeit\nF√ºr reproduzierbare Forschung gibt es inzwischen viele gute Tools:\n\nOSF: Website der Open Science Foundation\nEine kostenfreie und unkomplizierte M√∂glichkeit Daten und Skripts zu teilen, und diese in Projekten abzulegen. Es l√§sst sich daf√ºr sogar ein doi erstellen. Auch Formulare f√ºr eine Pr√§registration sind hier zu finden.\n\n\nFAIR Guiding Principles\nBeim Ver√∂ffentlichen von wissenschaftlichen Artikeln ist es empfohlen, die Daten (falls Anonymisierung m√∂glich) sowie die Analyse-Skripte mitzuver√∂ffentlichen.\nF√ºr Datens√§tze gelten die FAIR Guiding Principles (Wilkinson et al. (2016)):\n\nF indability: Es ist klar unter welchen Umst√§nden und wie die Daten zug√§nglich sind\nA ccessibility: Daten sind zug√§nglich bzw. es ist klar wo sie zu finden w√§ren\nI nteroperability: Verwendbare Datenformate/strukturen\nR eusability: gute Beschreibung des Datensatzes/der enthaltenen Variablen\n\n\nHier finden Sie weitere Informationen zu FAIR.\n\n\n\nBIDS: Brain Imaging Data Structure\nF√ºr Neuroimaging-Daten gibt es beispielsweise vorgegebene Konventionen, wie ein Datensatz und die Verarbeitungsskripts abgespeichert werden. Ein Beispiel daf√ºr ist Brain Imaging Data Structure (BIDS). So k√∂nnen Datens√§tze mit einer f√ºr alle verst√§ndlichen Struktur ver√∂ffentlicht und geteilt werden. Gorgolewski et al. (2016)\n\nHier finden Sie weitere Informationen zu BIDS.\n\nHier sehen Sie ein Beispiel, wie ein fMRI Datensatz in BIDS Struktur umgewandelt wird:\n Gorgolewski et al. (2016)\n\n\n\nCoding: Best practices\nF√ºr das Ver√∂ffentlichen von Analyseskripts eignen sich Formate wie RMarkdown in R, oder LiveScripts in MATLAB aber auch .r-Skripte sehr gut. Beim Teilen von Code erh√∂ht sich die Reproduzierbarkeit, wenn dieser verst√§ndlich strukturiert und kommentiert ist.\nWichtig ist hier:\n\nDas Verwenden relativer Pfade (data/raw_data.csv statt C:/Users/IhrName/Desktop/Projekt/Versuch1/finaleDaten/raw_data.csv)\nKonsequentes L√∂schen ‚Äúalter‚Äù Variablen sowie Testen, ob der Code in sich l√§uft\nVersionskontrolle: Entweder konsistentes Umbenennen oder mittels Github/Gitlab o.√§., Erstellen von Changelogs\nVor dem Ver√∂ffentlichen, lohnt es sich jemanden den Code ausf√ºhren lassen. So zeigt sich wo noch unklare Stellen sind, die Kommentare ben√∂tigen.\n\nBeim Kommentieren von Code sollte folgendes beachtet werden:\n\nKommentare sollten geschrieben werden, wenn der Code erstellt wird und laufend √ºberarbeitet werden. Oft wird es sonst nicht nachgeholt.\nWenn man nicht genau kommentieren kann, was man im Code macht, dann ist evtl. der Code unklar, oder man versteht ihn noch nicht. Vielleicht kann man Variablennamen vereinfachen/pr√§zisieren und es braucht weniger Kommentare?\nWenn Code von anderen kopiert wird, sollte die Quelle angegeben werden."
  },
  {
    "objectID": "pages/chapters/goodpractices_data.html#datenmanagement",
    "href": "pages/chapters/goodpractices_data.html#datenmanagement",
    "title": "Good Practices in der Datenverarbeitung",
    "section": "Datenmanagement",
    "text": "Datenmanagement\nIn diesem Kapitel wird auf die Sicherstellung der Datenqualit√§t, sowie die Wichtigkeit von Data collection plans und Data dictionaries eingegangen.\n\nDatenqualit√§t\nDie folgenden sieben Indikatoren f√ºr gute Datenqualit√§t wurden von C. Lewis (2024) zusammengefasst und sind hier zu finden.\n\n1. Analysierbarkeit\nDaten sollten in einem Format gespeichert werden in welchem sie analysiert werden sollen.\n\nDie Variablennamen stehen (nur!) in der ersten Zeile.\nDie restlichen Daten sind Werte in Zellen. Diese Werte sind analysierbar und Informationen sind explizit enthalten (keine leeren Felder, keine Farben als Information, nur 1 Information pro Variable).\n\n\n\n2. Interpretierbarkeit\n\nVariablennamen maschinenlesbar (kommen nur einmal vor, keine L√ºcken/spezielle Sonderzeichen/Farben, beginnen nicht mit einer Nummer, nicht zu lange)\nVariablennamen sind menschenlesbar (klare Bedeutung, konsistent formattiert, logisch geordnet)\nDatens√§tze sind in einem nicht-propriet√§ren Format abgespeichert (z.B. .csv nicht .xlsx oder .sav)\n\n\n\n3. Vollst√§ndigkeit\n\nKeine Missings, keine Duplikationen\nEnth√§lt alle erhobenen Variablen\nErkl√§rung f√ºr alle Variablen in zus√§tzlichem File (z.B. in einem data dictionary)\n\n\n\n4. Validit√§t\n\nVariablen entsprechen den geplanten Inhalten, d.h. Variablentypen und -werte stimmen mit Daten √ºberein (z.B. numerisch/kategorial, range: 1-5)\nMissings sind unverwechselbar angegeben (z.B. nicht -99, sondern NA)\n\n\n\n5. Genauigkeit\n\nDaten sollten in sich √ºbereinstimmen\nDaten sollten doppelt kontrolliert werden, wenn sie eingegeben werden\n\n\n\n6. Konsistenz\n\nVariablennamen sollten konsistent gew√§hlt werden\nInnerhalb wie auch zwischen den Variablen sollten Werte konsistent kodiert und formattiert sein\n\n\n\n7. Anonymisiert\n\nAlle Identifikationsmerkmale, sowohl direkte (Name, Adresse, E.Mail oder IP-Adresse, etc.) sowie indirekte (Alter, Geschlecht, Geburtsdatum, etc.) sollten so anonymisiert/kodiert sein, dass keine R√ºckschl√ºsse auf Personen m√∂glich sind\nBesondere Vorsicht bei: Offenen Fragen mit w√∂rtlichen Antworten, extremen Werten, kleinen Stichproben/kleinen Zellgr√∂ssen (&lt;3), Diagnosen, sensiblen Studienthemen\n\nEs empfiehlt sich vor der definitiven Abspeicherung oder gar Ver√∂ffentlichung des Datensatzes, diese Punkte durchzuarbeiten und zu kontrollieren.\n\n\n\nData collection plan\nDie Datenqualit√§t kann erh√∂ht werden, indem die obenstehenden Punkte schon bei der Planung der Datenerhebung beachtet und einbezogen werden. Wenn die Daten schon so eingegeben werden, werden die darauffolgenden Data Cleaning Schritte verk√ºrzt und weniger Fehler passieren. Die Dateneingabe und -analyse sollte vor der Datenerhebung getestet und w√§hrenddessen immer wieder kontrolliert werden.\n\n\nData dictionaries\nEin data dictionary ist eine Sammlung von Metadaten die beschreibt, welche Variablen ein Datensatz enth√§lt und wie die Werte dieser Variable formattiert sind.\nWichtige Inhalte f√ºr die Anwendung in den Neurowissenschaftlichen Forschung sind z.B. Name, Definition, Typ, Wertebereich (values), angewandte Transformationen, Geltungsbereiche (universe) und Format fehlender Angaben (skip logic).\nBeispiel Data Dictionary\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\ntype\nvalues\nsource\ntransformations\nuniverse\nskip_logic\n\n\n\n\nparticipant identification\nid\ncharacter, factor\n‚Äúsub-001‚Äù - ‚Äúsub-100‚Äù\nraw files\nanonymized variable ‚Äúparticipant‚Äù\nall\nNA\n\n\nparticipant age\nage\ninteger\n18 - 55\nraw files\n\nall\nNA\n\n\nexperimental block\nblock\ninteger\n1, 2\nraw files\n\nall\nNA\n\n\n\n\n\n\n\n\n\nHands-on: Erstellen Data Dictionary\n\n\n\nErstellen Sie in kleinen Gruppen f√ºr eines unserer beiden Experimente ein Data Dictionary. Verwenden Sie hierf√ºr die in den oben abgebildeten Beispielstabelle vorhandenen Spalten.\nTipp: Sie k√∂nnen die Tabelle auch ganz simpel in RMarkdown erstellen in dem Sie die Tabelle als Text (nicht im Codechunk) erstellen. Wie das funktioniert k√∂nnen Sie hier nachlesen.\n\n\n\nAusf√ºhrlichere Informationen √ºber das Erstellen von data dictionaries finden Sie z.B. hier und hier.\n\nInformationen zu Datentypen finden Sie hier oder in der untenstehenden Tabelle\nDatentypen\n\n\n\n\n\n\n\n\nDatentyp\nBeschrieb\nBeispiel\n\n\n\n\ninteger\nGanzzahliger Wert\n1, 2, 3, 29\n\n\ndouble, float\nZahl mit Kommastellen\n1.234, 89.3, 1.0\n\n\ncharacter, factor\nText oder Kategorie (nicht numerisch interpretiert)\nspeed, 0\n\n\nstring\nText oder Kategorie in ‚Äú‚Äù oder ‚Äô‚Äô\n‚Äúsub-001‚Äù\n\n\ndatetime\nDatum, Zeitpunkt\n2024-04-13, 21.12.23, 00:45:45\n\n\nboolean\nEiner von 2 m√∂glichen Werten\nFALSE, 0"
  },
  {
    "objectID": "pages/chapters/neurowissenschaft.html",
    "href": "pages/chapters/neurowissenschaft.html",
    "title": "Neurowissenschaft",
    "section": "",
    "text": "In der Neurowissenschaft wird mit naturwissenschaftlichem Schwerpunkt der Aufbau und die Funktionen des Nervensystems untersucht.\nLink Uni\n\n\n\n\n\n\n\n\n\nBereich\nBeschreibung\n\n\n\n\nBehavioral neuroscience\nEinfluss des Gehirns auf Verhalten\n\n\nClinical neuroscience\nSt√∂rungen des Nervensystems, Vorbeugung, Behandlung und Rehabilitation\n\n\nComputational neuroscience\nSimulation und Modellierung von Hirnfunktionen\n\n\nCultural neuroscience\nEinfluss kultureller Faktoren auf das Gehirn\n\n\nDevelopmental neuroscience\nEntwicklung des Nervensystems √ºber die ganze Alterspanne\n\n\nMolecular and cellular neuroscience\nRolle einzelner Molek√ºle, Gene und Proteine bei der Funktion des Nervensystems\n\n\nNeuroengineering\nneuronale Systeme verstehen, ersetzen, reparieren oder verbessern\n\n\nNeuroimaging\nTeilbereich medizinischer Bildgebung mit Fokus auf dem Gehirn\n\n\nNeuroinformatics\nEntwicklung von Methoden zur Sammlung, Analyse, Nutzung und Ver√∂ffentlichung von Daten\n\n\nNeurolinguistics\nRolle des Gehirns beim erwerben, speichern, verstehen und ausdr√ºcken von Sprache\n\n\nNeurophysiology\nRolle des Nervensystems von der subzellul√§ren Ebene bis hin zu ganzen Organen\n\n\n\nQuelle\nAufteilung nach:\n\nUntersuchter Spezies\n\nz.B. Mensch, Affe, Zebrafisch, Fruchtfliege\n\nverwendeter Methode\n\nz.B. EEG, fMRI, Verhalten"
  },
  {
    "objectID": "pages/chapters/neurowissenschaft.html#hauptbereiche-der-neurowissenschaften",
    "href": "pages/chapters/neurowissenschaft.html#hauptbereiche-der-neurowissenschaften",
    "title": "Neurowissenschaft",
    "section": "",
    "text": "Bereich\nBeschreibung\n\n\n\n\nBehavioral neuroscience\nEinfluss des Gehirns auf Verhalten\n\n\nClinical neuroscience\nSt√∂rungen des Nervensystems, Vorbeugung, Behandlung und Rehabilitation\n\n\nComputational neuroscience\nSimulation und Modellierung von Hirnfunktionen\n\n\nCultural neuroscience\nEinfluss kultureller Faktoren auf das Gehirn\n\n\nDevelopmental neuroscience\nEntwicklung des Nervensystems √ºber die ganze Alterspanne\n\n\nMolecular and cellular neuroscience\nRolle einzelner Molek√ºle, Gene und Proteine bei der Funktion des Nervensystems\n\n\nNeuroengineering\nneuronale Systeme verstehen, ersetzen, reparieren oder verbessern\n\n\nNeuroimaging\nTeilbereich medizinischer Bildgebung mit Fokus auf dem Gehirn\n\n\nNeuroinformatics\nEntwicklung von Methoden zur Sammlung, Analyse, Nutzung und Ver√∂ffentlichung von Daten\n\n\nNeurolinguistics\nRolle des Gehirns beim erwerben, speichern, verstehen und ausdr√ºcken von Sprache\n\n\nNeurophysiology\nRolle des Nervensystems von der subzellul√§ren Ebene bis hin zu ganzen Organen\n\n\n\nQuelle\nAufteilung nach:\n\nUntersuchter Spezies\n\nz.B. Mensch, Affe, Zebrafisch, Fruchtfliege\n\nverwendeter Methode\n\nz.B. EEG, fMRI, Verhalten"
  },
  {
    "objectID": "pages/chapters/neurowissenschaftliche_experimente.html",
    "href": "pages/chapters/neurowissenschaftliche_experimente.html",
    "title": "Neurowissenschaftliche Experimente",
    "section": "",
    "text": "Nothing in neuroscience makes sense except in the light of behavior.\nShepherd (1988)"
  },
  {
    "objectID": "pages/chapters/neurowissenschaftliche_experimente.html#besondere-herausforderungen-von-experimenten-in-den-verhaltens-neurowissenschaften",
    "href": "pages/chapters/neurowissenschaftliche_experimente.html#besondere-herausforderungen-von-experimenten-in-den-verhaltens-neurowissenschaften",
    "title": "Neurowissenschaftliche Experimente",
    "section": "Besondere Herausforderungen von Experimenten in den Verhaltens-/Neurowissenschaften",
    "text": "Besondere Herausforderungen von Experimenten in den Verhaltens-/Neurowissenschaften\n\nPr√§zision: Hohe r√§umliche und zeitliche Aufl√∂sung\nEine grosse Schwierigkeit neurowissenschaftlicher Experimente ist oft, dass eine pr√§zise Kontrolle von r√§umlichen und zeitliche Eigenschaften der Experimente n√∂tig ist um sinnvolle Daten zu erhalten. Visuelle Stimuli m√ºssen z.B. sehr genau und immer gleich pr√§sentiert werden k√∂nnen. Die zeitliche Aufl√∂sung ist gerade bei EEG Experimenten von enormer Bedeutung, da EEG eine sehr hohe zeitliche Aufl√∂sung hat. R√§umliche Aufl√∂sung kann bedeuten, dass sehr pr√§zise visuelle Darbietung m√∂glich sein muss, sowie dass die Versuchsperson sich im Setup nicht bewegen darf, weil dies die Distanzen verschiebt (z.B. im MRT, oder der Abstand zum Bildschirm beim Eyetracking).\n\n\nSynchronisation: Mehrere Datenspuren\nNeurowissenschaftliche Experimente beinhalten oft die Datenerhebung auf mehreren Ebenen, z.B. wird gleichzeitig Hirnaktivit√§t und das Dr√ºcken von Antwortbuttons aufgenommen. Das bedeutet, dass Bildschirm, MRT/EEG/Eyetracking/etc., sowie die Antwort zeitlich koregistriert/synchronisiert werden m√ºssen, um die Daten im Nachhinein auswerten zu k√∂nnen. Technisch ist das oft mit grossem Aufwand verbunden und ben√∂tigt einiges an Pilotierung.\n\n\nKomplexit√§t: Zu untersuchender Prozess und St√∂rprozesse\nOft soll ein ganz spezifischer Prozess untersucht werden, aber das ist eine sehr komplexe Aufgabe, weil im menschlichen Gehirn gleichzeitig sehr viele verschiedene Prozesse ablaufen, kein Hirnareal hat nur eine einzige Aufgabe und aus ethischen Gr√ºnden ist das ‚ÄúAusschalten‚Äù von St√∂rfaktoren nicht immer m√∂glich. Was kann man tun?\nEin Weg den Prozess sichtbar zu machen ist es zum Beispiel einen Kontrast zu rechnen, dies wird beispielsweise bei EEG und fMRI Experimenten, aber auch bei Reaktionszeitexperimenten sehr oft gemacht. Hierf√ºr erhebt man Daten in einer Test-Bedingung in der der Prozess abgerufen wird und eine Kontroll-Bedingung, welche als ‚ÄúBaseline‚Äù dient. Die Baseline enth√§lt alle ‚Äúnicht interessierenden‚Äù Prozesse, die in der Test-Bedingung vorhanden sind. Durch das Vergleichen der Test- und Kontrollbedingung erh√§lt man einen Kontrast: Also das was den interessierenden Prozess ausmacht!\nSie m√ºssen sich beim Erstellen eines Experiments also nicht nur Gedanken dazu machen, was Sie interessiert - sondern genau so auch dar√ºber was Sie nicht interessiert. In der Theorie t√∂nt das einfach, in der Praxis ist das oft recht kniffelig.\n\n\nDatenerhebung: Teuer und anspruchsvoll\nBildgebende Verfahren, ben√∂tigen zum Teil extrem teure Ger√§te, wie z.B. fMRI, und bedeuten oft hohen Aufwand, z.B. das Kleben der Elektroden beim EEG. Bei der Untersuchung von ganz bestimmten Patientengruppen hat man zudem oft nicht sehr viele Personen zur Verf√ºgung die den Einschlusskriterien entsprechen. Oft m√ºssen Personen auch aus dem Experiment ausgeschlossen werden, weil sie z.B. Auff√§lligkeiten im MRI zeigen, die nichts mit dem zu untersuchenden Prozess zu tun hat oder sie brechen w√§hrend der Untersuchung ab. Gerade bei der Untersuchung klinischer Aspekte stellen sich oft Schwierigkeiten, wie beispielsweise fehlende Motivation oder Compliance von Patient:innen. Daher k√∂nnen oft keine sehr grossen Stichproben erhoben werden, was im Gegenzug besonders pr√§zise Experimente erfordert.\n\n\nPassendes Experiment muss selber erstellt werden\nOft muss ein neues Paradigma erstellt werden, d.h. Forschende k√∂nnen kein schon bestehendes Experiment nutzen, sondern untersuchen einen Aspekt eines neuronalen Prozesses mit einer neuen Methode, einer neuen Fragestellung oder einem neuen Ansatz. Dies erfordert breiteKenntnisse im Programmieren, der zu verwendenden Technik, wie auch der Gehirnprozesse."
  },
  {
    "objectID": "pages/chapters/neurowissenschaftliche_experimente.html#wichtige-elemente-von-experimenten",
    "href": "pages/chapters/neurowissenschaftliche_experimente.html#wichtige-elemente-von-experimenten",
    "title": "Neurowissenschaftliche Experimente",
    "section": "Wichtige Elemente von Experimenten",
    "text": "Wichtige Elemente von Experimenten\nBeim Programmieren von Experimenten lohnt es sich, sich zuerst dar√ºber im klaren zu sein, welche Bausteine das geplante Experiment hat. Im Folgenden werden einige typische Elemente eines Verhaltensexperiments beschrieben. Oft kommen hier nat√ºrlich noch Stimulations- oder Aufnahmemethoden hinzu.\n\nBegr√ºssung und Einverst√§ndniserkl√§rung\nHier wird die Versuchsperson begr√ºsst, wird √ºber das Experiment aufgekl√§rt und gibt (wenn nicht vorher auf Papier schon geschehen) ihre Einverst√§ndnis zur Teilnahme am Experiment. Dies wird je nach Ethikkommission und Ethikantrag unterschiedlich gehandhabt. Wichtige Informationen sind hierbei, dass die Versuchsperson weiss worauf sie sich einl√§sst (Ist zum Beispiel Hirnstimulation/fMRI/etc. geplant? Wie lange dauert das Experiment ungef√§hr? Was soll sie tun, wenn sie abbrechen m√∂chte?). Die Schwierigkeit ist oft, gen√ºgend Information zu geben aber die Hypothese nicht zu verraten.\n\n\nInstruktion\nDie Instruktion wird oft schriftlich gegeben, um diese zwischen den Versuchspersonen konstant zu halten. Es ist teilweise herausfordernd, einen Task so genau zu erkl√§ren, dass er verst√§ndlich ist, aber die Erkl√§rung auch kurz genug zu halten, dass die Instruktion auch gelesen wird. Oft werden √úbungstrials verwendet um die Instruktion zu verdeutlichen.\n\n\nStimuli\nUnter Stimuli werden die gezeigten Elemente verstanden, die den Task ausmachen. Es k√∂nnen T√∂ne, Bilder, W√∂rter, etc. als Stimuli verwendet werden.\n\n\n\n\n\n\nHands-on: Stimuli\n\n\n\nWelche Stimuli aus neurowissenschaftlichen Experimenten kennen Sie?\nTauschen Sie sich mit Ihren Mitstudierenden aus und schreiben/zeichnen Sie ein paar Beispiele vorne an die Tafel.\n[~5 Minuten]\n\n\n\n\nTrial\nEin Trial beschreibt ein sich wiederholender Vorgang in dem der Stimulus gezeigt wird und z.B. von der Versuchsperson eine Antwort erwartet wird. Ein Trial wird oft sehr viele Male wiederholt. Die Stimuli k√∂nnen zwischen den Trials variieren oder gleich bleiben. Das Timing der Trials kann konstant sein (ein Stimulus wird bspw. immer gleich lang gezeigt) oder variiert werden (unterschiedliche Anzeigedauer).\nZwischen den Trials wird ein Inter-Trial-Interval (ITI) festgelegt. Dies wird z.B. bei fMRI Experimenten dann variiert, damit (je nach Repetition Time/TR) nicht immer in derselben Schicht aufgenommen wird bei Stimuluspr√§sentation.\nW√§hrend einem Trial wird die Antwort / Response der Versuchsperson aufgenommen. Bei der Aufnahme von Reaktiosnzeiten muss festgelegt werden, wann der Trial oder die Stimuluspr√§sentation beginnt und mit welcher Handlung sie endet. Es kann bestimmt werden, welche Antworten zul√§ssig sind (bspw. nur bestimmte Tasten) und was passiert wenn eine richtige oder falsche Antwort gegeben wird: Gibt es z.B. ein Feedback bei falschen Antworten?\n\n\nRun / Block\nEin Run/ein Block bezeichnet eine Einheit mit mehreren Trials. Oft werden Bedingungen z.B. zwischen den Runs randomisiert. Zwischen den Runs sind Pausen m√∂glich, damit sich die Versuchsperson erholen kann. Oft wird vor dem Experimentstart ein ‚Äú√úbungsblock‚Äù durchgef√ºhrt, um sich sicher zu sein, dass die Versuchspersonen die Aufgabe und Instruktion verstanden haben.\n\n\nDebriefing und Verabschiedung\nIm Debriefing wird der Versuchsperson erkl√§rt, um was es im Experiment gegangen ist, welche Hypothesen untersucht wurden und eine eventuelle Coverstory aufgedeckt. Oft werden Personen vor dem Debriefing nach der getesteten Hypothese gefragt, um zu schauen, ob sie diese erraten hatten. Das gibt Aufschluss dar√ºber wie sehr das Experiment dadurch verzerrt sein k√∂nnte, dass die Versuchspersonen Bescheid wissen. Wichtig ist es auch den Versuchspersonen zum Schluss zu danken."
  },
  {
    "objectID": "pages/chapters/neurowissenschaftliche_experimente.html#vorgehen-experiment-programmieren",
    "href": "pages/chapters/neurowissenschaftliche_experimente.html#vorgehen-experiment-programmieren",
    "title": "Neurowissenschaftliche Experimente",
    "section": "Vorgehen Experiment programmieren",
    "text": "Vorgehen Experiment programmieren\nWichtige Schritte beim Programmieren von Experimenten sind folgende (nicht unbedingt in dieser Reihenfolge, das kommt auf das Experiment an):\n\nTask ausw√§hlen\nStimuli ausw√§hlen\nTiming festlegen: Dauer Stimuluspr√§sentation? ITIs (Inter-Trial-Intervals)?\nAnzahl Bedingungen und Trials bestimmen (Power bedenken), within oder between Design?\nAblauf des Experiments festlegen: Gesamtdauer? Pausen n√∂tig?\nCoverstory, Stimulus-Masking, etc. n√∂tig?\nEinbindung von allen technischen Ger√§ten (z.B. EEG Recorder, MRT, Brainstimulation-Devices, Eyetracking) und Synchronisation\n\n\nFlowcharts\nBei der Planung und dem Erstellen eines Experiments ist es sehr hilfreich eine Flowchart zu erstellen. In einer Flowchart sind die oben genannten Elemente des Experimentes in Boxen eingezeichnet und mit Pfeilen verbunden um sie zeitlich einzuordnen. Timing-Informationen k√∂nnen unter oder neben den Boxen festgehalten werden. Die Anzahl Repetitionen wird oft neben den Pfeilen eingef√ºgt.\n\nWir werden zu einem sp√§teren Zeitpunkt in diesem Kurs darauf eingehen, wie man ein Experiment m√∂glichst gut planen kann um aussagekr√§ftige Daten zu erhalten. Hier gibt es viele M√∂glichkeiten wie Pilotierung, Datensimulation und die ad√§quate Wahl der statistischen Verfahren in Bezug auf die Fragestellung.\n\nEine Flowchart eignet sich ebenfalls sehr gut, um in einem Artikel darzustellen, wie der Ablauf des Experiments war."
  },
  {
    "objectID": "pages/chapters/programmieren_1.html",
    "href": "pages/chapters/programmieren_1.html",
    "title": "Sprachen & Umgebungen",
    "section": "",
    "text": "Programmiersprachen sind essentielle Werkzeuge f√ºr die Neurowissenschaftliche Forschung. Wir werden uns zuerst einen kurzen √úberblick √ºber h√§ufig verwendete Programmiersprachen verschaffen und kurz deren Verwendungszwecke und Vor- und Nachteile diskutieren.\n\n\n\n\n\nMatlab ist ein Software f√ºr numerische Anwendung, welche in den Ingenieurwissenschaften, Naturwissenschaften und der Mathematik weit verbreitet ist.\n\n\n\nLeistungsstarke Matrix- und Vektoroperationen, gut geeignet f√ºr Matrix-basierte Operationen, die in der Neurowissenschaftlichen Forschung h√§ufig vorkommen.\nUmfangreiche Bibliothek von integrierten Funktionen f√ºr wissenschaftliches Rechnen.\n\n\n\n\n\nTeuer\nWeniger flexibel als Python oder R in Bezug auf Datenarten und Strukturen.\nMatlab is kommerziell und propriet√§r. Dies bedeutet man muss teuere Lizenzen kaufen, und der Source Code der Software ist nicht offen.\n\n\n\n\n\nDatenverarbeitung und -analyse\nVisualisierung\nViele fMRI Forscher arbeiten mit Matlab, da es daf√ºr eine spezielle Toolbox gibt: SPM\nExperimente programmieren, z.B. mit Psychtoolbox\n\n\n\n\nload('data.mat')\nfs = 1000;\nt = (0:numel(data)-1)/fs;\nplot(t, data)\n\n\n\n\n\nPython ist eine allgemeine (general purpose) Programmiersprache, die in vielen verschiedenen Bereichen wie wissenschaftlichem Rechnen, Datenanalyse und maschinellem Lernen weit verbreitet ist.\n\n\n\n\nEine Vielzahl von Bibliotheken und Modulen wie NumPy, SciPy und Pandas, die leistungsstarke Werkzeuge f√ºr wissenschaftliches Rechnen und Datenanalyse bieten.\nDatenanalysewerkzeuge wie Pandas-Dataframes, die Seaborn-Visualisierungsbibliothek, und Jupyter Notebooks.\nOpen-source und kostenlos\n\n\n\n\n\nKann in einigen numerischen Berechnungen langsamer sein als Matlab.\nDa Python eine allgemeine Sprache ist, muss man f√ºr numerische Anwendungen immer verschiedene Packages importieren (z.B.) numpy, wenn man damit rechnen will. Dies f√ºhrt zu weniger gut lesbarem Code.\n\n\n\n\n\nDatenverarbeitung und -analyse,\nVisualisierung\nMachine learning und K√ºnstliche Intelligenz\nExperimente programmieren, z.B. mit PsychoPy\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\ndata = pd.read_csv('data.csv')\nsns.lineplot(data=data, x='time', y='voltage')\n\n\n\n\n\nR ist eine Programmiersprache f√ºr statistisches Rechnen und Grafiken.\n\n\n\n\nEntwickelt von Statistikern f√ºr statistisches Rechnen und Grafiken.\nEine gro√üe Bibliothek von statistischen Werkzeugen und Paketen, einschliesslich Visualisierungspackages (grammar of graphics).\nOpen-source und kostenlos\ntidyverse Packages f√ºr Data Wrangling (sehr elegante Syntax, um mit Daten zu arbeiten).\n\n\n\n\n\nSteilere Lernkurve als Python.\nKann in einigen numerischen Berechnungen langsamer sein als Matlab oder Python.\nEntwickelt von Statistiker (nicht von Software Designers). R gilt als sehr idiosynkratisch.\n\n\n\n\n\nStatistische Analyse\nDatenvisualisierung. R hat eine sehr gute Bibliothek f√ºr Grafiken, die ggplot2 Bibliothek. Diese Bibliothek verwendet die sogenannte grammar of graphics (GoG) - eine Methode, um Daten in Grafiken zu visualisieren. Die GoG ist eine sehr elegante und effiziente Methode, um Daten zu visualisieren.\n\n\n\n\nlibrary(tidyverse)\ndata &lt;- read.csv('data.csv')\nggplot(data, aes(x=time, y=voltage)) + geom_line()\n\n\n\n\n\nJulia ist eine allgemeine (general purpose) Programmierspache, die vor allem f√ºr numerisches und wissenschaftliches Rechnen entwickelt wurde.\n\n\n\n\nEntwickelt von Mathematikern f√ºr scientific computing\nLeistungsstark und schnell\nOpen-source und kostenlos\n\n\n\n\n\nwenig verbreitet\n\n\n\n\n\nDatenverarbeitung und -analyse,\nSignalverarbeitung\nVisualisierung\nMachine learning und K√ºnstliche Intelligenz\n\n\n\n\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie\ndat = CSV.read('data.csv', DataFrame)\ndata(dat) * visual(Lines) * mapping(:time, :voltage) |&gt; draw()\n\n\n\n\n\nBash ist eine Mensch-Maschine-Schnittstelle und dient als Grundlage unixbasierter Betriebssysteme (Linux, macOS). Mit Bash-Skripts k√∂nnen repetitive Aufgaben automatisiert werden, was Fehler vermeidet und komplexe Aufgaben vereinfacht.\n\n\n\n\nEinfaches Konzept und Verwendung\nBereits instaliert (nicht auf Windows)\n\n\n\n\n\nSteile Lernkurve\nUmstellung notwendig von GUI\n\n\n\n\n\nAutomatisierung\neinfache bis komplexe Applikationen\nInteraktion mit dem High Performance Computing Cluster der Universit√§t Bern\n\n\n\n\n#!/bin/bash\necho \"Enter Your Name\"\nread name\nprintf \"\\n Welcome $name \\n\\n\"\n\n\n\nDiese Sprachen sind leistungsstarke Werkzeuge f√ºr die neurowissenschaftliche Forschung. Die Wahl der Sprache h√§ngt unter anderem von der spezifischen Aufgabe ab. Weitere Faktoren ist Tradition: bestimmte Gruppen bevorzugen eher eine Sprache als andere. So ist Matlab unter Ingenieuren weit verbreiten und R unter Statistikern. Python ist im Bereich K√ºnstliche Intelligenz und Machine Learning die beliebteste Sprache. Eine neuere Sprache ist Julia - diese vereint die Vorteile aller oben genannten Sprachen (ohne viele deren Nachteile), ist aber weniger weit verbreitet.\nUm mehr zu erfahren, erkunden Sie die umfangreichen Online-Ressourcen und Dokumentationen f√ºr jede Sprache."
  },
  {
    "objectID": "pages/chapters/programmieren_1.html#programmiersprachen",
    "href": "pages/chapters/programmieren_1.html#programmiersprachen",
    "title": "Sprachen & Umgebungen",
    "section": "",
    "text": "Programmiersprachen sind essentielle Werkzeuge f√ºr die Neurowissenschaftliche Forschung. Wir werden uns zuerst einen kurzen √úberblick √ºber h√§ufig verwendete Programmiersprachen verschaffen und kurz deren Verwendungszwecke und Vor- und Nachteile diskutieren.\n\n\n\n\n\nMatlab ist ein Software f√ºr numerische Anwendung, welche in den Ingenieurwissenschaften, Naturwissenschaften und der Mathematik weit verbreitet ist.\n\n\n\nLeistungsstarke Matrix- und Vektoroperationen, gut geeignet f√ºr Matrix-basierte Operationen, die in der Neurowissenschaftlichen Forschung h√§ufig vorkommen.\nUmfangreiche Bibliothek von integrierten Funktionen f√ºr wissenschaftliches Rechnen.\n\n\n\n\n\nTeuer\nWeniger flexibel als Python oder R in Bezug auf Datenarten und Strukturen.\nMatlab is kommerziell und propriet√§r. Dies bedeutet man muss teuere Lizenzen kaufen, und der Source Code der Software ist nicht offen.\n\n\n\n\n\nDatenverarbeitung und -analyse\nVisualisierung\nViele fMRI Forscher arbeiten mit Matlab, da es daf√ºr eine spezielle Toolbox gibt: SPM\nExperimente programmieren, z.B. mit Psychtoolbox\n\n\n\n\nload('data.mat')\nfs = 1000;\nt = (0:numel(data)-1)/fs;\nplot(t, data)\n\n\n\n\n\nPython ist eine allgemeine (general purpose) Programmiersprache, die in vielen verschiedenen Bereichen wie wissenschaftlichem Rechnen, Datenanalyse und maschinellem Lernen weit verbreitet ist.\n\n\n\n\nEine Vielzahl von Bibliotheken und Modulen wie NumPy, SciPy und Pandas, die leistungsstarke Werkzeuge f√ºr wissenschaftliches Rechnen und Datenanalyse bieten.\nDatenanalysewerkzeuge wie Pandas-Dataframes, die Seaborn-Visualisierungsbibliothek, und Jupyter Notebooks.\nOpen-source und kostenlos\n\n\n\n\n\nKann in einigen numerischen Berechnungen langsamer sein als Matlab.\nDa Python eine allgemeine Sprache ist, muss man f√ºr numerische Anwendungen immer verschiedene Packages importieren (z.B.) numpy, wenn man damit rechnen will. Dies f√ºhrt zu weniger gut lesbarem Code.\n\n\n\n\n\nDatenverarbeitung und -analyse,\nVisualisierung\nMachine learning und K√ºnstliche Intelligenz\nExperimente programmieren, z.B. mit PsychoPy\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\ndata = pd.read_csv('data.csv')\nsns.lineplot(data=data, x='time', y='voltage')\n\n\n\n\n\nR ist eine Programmiersprache f√ºr statistisches Rechnen und Grafiken.\n\n\n\n\nEntwickelt von Statistikern f√ºr statistisches Rechnen und Grafiken.\nEine gro√üe Bibliothek von statistischen Werkzeugen und Paketen, einschliesslich Visualisierungspackages (grammar of graphics).\nOpen-source und kostenlos\ntidyverse Packages f√ºr Data Wrangling (sehr elegante Syntax, um mit Daten zu arbeiten).\n\n\n\n\n\nSteilere Lernkurve als Python.\nKann in einigen numerischen Berechnungen langsamer sein als Matlab oder Python.\nEntwickelt von Statistiker (nicht von Software Designers). R gilt als sehr idiosynkratisch.\n\n\n\n\n\nStatistische Analyse\nDatenvisualisierung. R hat eine sehr gute Bibliothek f√ºr Grafiken, die ggplot2 Bibliothek. Diese Bibliothek verwendet die sogenannte grammar of graphics (GoG) - eine Methode, um Daten in Grafiken zu visualisieren. Die GoG ist eine sehr elegante und effiziente Methode, um Daten zu visualisieren.\n\n\n\n\nlibrary(tidyverse)\ndata &lt;- read.csv('data.csv')\nggplot(data, aes(x=time, y=voltage)) + geom_line()\n\n\n\n\n\nJulia ist eine allgemeine (general purpose) Programmierspache, die vor allem f√ºr numerisches und wissenschaftliches Rechnen entwickelt wurde.\n\n\n\n\nEntwickelt von Mathematikern f√ºr scientific computing\nLeistungsstark und schnell\nOpen-source und kostenlos\n\n\n\n\n\nwenig verbreitet\n\n\n\n\n\nDatenverarbeitung und -analyse,\nSignalverarbeitung\nVisualisierung\nMachine learning und K√ºnstliche Intelligenz\n\n\n\n\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie\ndat = CSV.read('data.csv', DataFrame)\ndata(dat) * visual(Lines) * mapping(:time, :voltage) |&gt; draw()\n\n\n\n\n\nBash ist eine Mensch-Maschine-Schnittstelle und dient als Grundlage unixbasierter Betriebssysteme (Linux, macOS). Mit Bash-Skripts k√∂nnen repetitive Aufgaben automatisiert werden, was Fehler vermeidet und komplexe Aufgaben vereinfacht.\n\n\n\n\nEinfaches Konzept und Verwendung\nBereits instaliert (nicht auf Windows)\n\n\n\n\n\nSteile Lernkurve\nUmstellung notwendig von GUI\n\n\n\n\n\nAutomatisierung\neinfache bis komplexe Applikationen\nInteraktion mit dem High Performance Computing Cluster der Universit√§t Bern\n\n\n\n\n#!/bin/bash\necho \"Enter Your Name\"\nread name\nprintf \"\\n Welcome $name \\n\\n\"\n\n\n\nDiese Sprachen sind leistungsstarke Werkzeuge f√ºr die neurowissenschaftliche Forschung. Die Wahl der Sprache h√§ngt unter anderem von der spezifischen Aufgabe ab. Weitere Faktoren ist Tradition: bestimmte Gruppen bevorzugen eher eine Sprache als andere. So ist Matlab unter Ingenieuren weit verbreiten und R unter Statistikern. Python ist im Bereich K√ºnstliche Intelligenz und Machine Learning die beliebteste Sprache. Eine neuere Sprache ist Julia - diese vereint die Vorteile aller oben genannten Sprachen (ohne viele deren Nachteile), ist aber weniger weit verbreitet.\nUm mehr zu erfahren, erkunden Sie die umfangreichen Online-Ressourcen und Dokumentationen f√ºr jede Sprache."
  },
  {
    "objectID": "pages/chapters/programmieren_1.html#programmierumgebungen",
    "href": "pages/chapters/programmieren_1.html#programmierumgebungen",
    "title": "Sprachen & Umgebungen",
    "section": "Programmierumgebungen",
    "text": "Programmierumgebungen\nEine IDE kombiniert h√§ufig verwendete Entwicklertools in einer grafischen Benutzeroberfl√§che. Typischerweise handelt es sich dabei um Funktionen wie Softwarebearbeitung, Erstellung und Testen von Code.\nIn diesem Kurs werden wir RStudio verwenden."
  },
  {
    "objectID": "pages/chapters/programmieren_2.html",
    "href": "pages/chapters/programmieren_2.html",
    "title": "Automatisieren",
    "section": "",
    "text": "Lernziele\n\n\n\n\n\nIn der heutigen Sitzung lernen wir:\n\nConditionals und Control Flow\nFunktionen erstellen\nLoops anwenden\n\nF√ºhren Sie die folgenden Code Beispiele auf Ihrem Computer aus. Wenn Sie R und RStudio noch nicht installiert haben, verwenden Sie diese online R Konsole.\nWir lernen nun zwei Programmierkonzepte kennen, die uns dabei helfen, Tasks zu automatisieren. Wir werden hier nicht in die Tiefe gehen; es geht uns vielmehr darum, Ihnen einen √úberblick zu geben, was Sie mit diesen Konzepten machen k√∂nnen. Falls Sie tiefer in die Materie einsteigen m√∂chten, gibt es entsprechende Kurse auf Datacamp."
  },
  {
    "objectID": "pages/chapters/programmieren_2.html#alternativen-zu-for-loops",
    "href": "pages/chapters/programmieren_2.html#alternativen-zu-for-loops",
    "title": "Automatisieren",
    "section": "Alternativen zu for-Loops",
    "text": "Alternativen zu for-Loops\nEs gibt in R mehrere M√∂glichkeiten, um √ºber Vektoren oder Listen zu iterieren, ohne dabei explizite for-Loops zu schreiben. Dies hat den Vorteil, dass der Code k√ºrzer und √ºbersichtlicher wird.\n\nlapply und sapply\n\nlapply und sapply sind zwei Funktionen, welche √ºber Listen iterieren. lapply und sapply sind sehr √§hnlich. lapply gibt eine Liste zur√ºck, w√§hrend sapply versucht den output zu vereinfachen (Vektor, Matrix).\nAls Beispiel wollen wir jedes Element eines Vektors verdoppeln (dies kann in R auch einfacher gemacht werden, aber dies ist nur ein √úbungsbeispiel).\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\nMit for k√∂nnen wir dies wie folgt tun.\n\nfor (number in numbers) {\n    print(number * 2)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nMit lapply und sapply haben wir zwei M√∂glichkeiten. Wir k√∂nnen entweder eine anonyme Funktion definieren, oder wir k√∂nnen eine Funktion zuerst definieren, und dann verwenden.\n\\(x) x * 2 definiert eine sogenannte anonyme Funktion. Diese Funktion nimmt ein Argument x und multipliziert es mit 2, erh√§lt aber keinen eigenen Namen. Folglich k√∂nnen wir diese Funktion nicht wiederverwenden.\n\nlapply(numbers, \\(x) x * 2)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\n\nMit einer Funktion, die wir zuerst definieren, sieht unser Beispiel so aus.\n\ndouble &lt;- function(x) {\n    x * 2\n}\n\n\nlapply(numbers, double)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\n\n\nsapply(numbers, double)\n\n[1]  2  4  6  8 10\n\n\nmap\nEine weitere M√∂glichkeit, √ºber Listen zu iterieren, ist die Funktion map. map ist eine Funktion aus dem Paket purrr (wird automatisch geladen, wenn tidyverse geladen wird). map gibt immer eine Liste zur√ºck.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.0     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nnumbers |&gt; map(double)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\n\nWenn wir als Output einen Vektor haben wollen, m√ºssen wir die Funktion unlist() verwenden.\n\nnumbers |&gt; map(double) |&gt; unlist()\n\n[1]  2  4  6  8 10"
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html",
    "href": "pages/chapters/psychopy_experiments.html",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "",
    "text": "In diesem Kurs fokussieren wir auf verhaltenswissenschaftliche Forschung, die sich f√ºr die Gehirnprozesse von Menschen interessiert. Wir erstellen in PsychoPy selber zwei Experimente aus der Wahrnehmungs- und Entscheidungsforschung sowie der Neuropsychologie an. Das Random Dot Experiment ist ein perzeptuelles Entscheidungsexperiment mit welchem wir den Einfluss von Anweisungen/Instruktionen auf Entscheidungen untersuchen. Das Stroop Experiment beinhaltet eine typische Aufgabe aus der (klinischen) Neuropsychologie zur Untersuchung von exekutiven Funktionen.\nPsychoPy ist eine kostenfreie Software zum Erstellen von verhaltenswissenschaftlichen Experimenten im Labor oder Online. Die Software basiert auf der Programmiersprache Python, man kann die Experimente (mit gewissen Begrenzungen) jedoch auch in einem GUI (guided user interface) erstellen und braucht so (fast) keine Programmierkenntnisse. PsychoPy-Experimente erm√∂glichen pr√§zise r√§umliche und zeitliche Kontrolle. (Peirce et al.¬†2019)\nIn PsychoPy erstellen Experimente k√∂nnen direkt auf Pavlovia hochgeladen, und so als Online-Experimente gehostet und durchgef√ºhrt werden. Die Speicherung des Experimentes auf gitlab erm√∂glicht dabei eine Versionskontrolle sowie das Teilen des Experimentalcodes."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#experiment-file-erstellen-und-abspeichern",
    "href": "pages/chapters/psychopy_experiments.html#experiment-file-erstellen-und-abspeichern",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "1.1 Experiment-File erstellen und abspeichern",
    "text": "1.1 Experiment-File erstellen und abspeichern\n\n√ñffnen Sie PsychoPy und speichern Sie in einem daf√ºr erstellten Ordner (z.B. psychopy_experiment) das Experiment-File ab (z.B. unter experiment_stroop-task)."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#builder-gui-und-coder",
    "href": "pages/chapters/psychopy_experiments.html#builder-gui-und-coder",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "1.2 Builder (GUI) und Coder?",
    "text": "1.2 Builder (GUI) und Coder?\nExperimente k√∂nnen in PsychoPy mit dem Builder (in einem GUI) erstellt werden, der Python Code wird so automatisch f√ºr Sie generiert. Sie k√∂nnen sich diesen Code auch anschauen und ver√§ndern. Leider k√∂nnen Sie sobald Sie den Code ver√§ndert haben, diese √Ñnderungen nicht zur√ºck in den Builder √ºbertragen. Im Builder-Modus k√∂nnen Sie aber Codest√ºcke einf√ºgen um einzelne Teile des Experiments in Python (oder anderen Programmiersprachen) zu programmieren und dennoch im Builder weiterarbeiten zu k√∂nnen.\n\nFalls Sie planen ein Online-Experiment durchzuf√ºhren, eignet sich der Builder besonders, da die Experimente direkt online durchgef√ºhrt werden k√∂nnen."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#stimuli",
    "href": "pages/chapters/psychopy_experiments.html#stimuli",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "2.1 Stimuli",
    "text": "2.1 Stimuli\nIn PsychoPy finden Sie schon vorprogrammierte Stimulus Elemente, wie Gratings oder Rating Scales und k√∂nnen Texte, geometrische Figuren, Bilder und Filme einf√ºgen. Auch komplexere Stimuluselemente wie Random Dots k√∂nnen sehr einfach konfiguriert werden ohne dass sie von Grund auf neu programmiert werden m√ºssen.\n\nErstellen Sie einen Stimulus. Beachten Sie folgende Aspekte:\n\nFarbe\nGr√∂sse\nweitere Eigenschaften, wie Bedingung/Kongruenz?\nTiming (Stimulusdauer, Stimulusende)\n\nNotieren Sie, welche Eigenschaften des Stimulus sich √ºber die Trials hinweg ver√§ndern sollte. Dies k√∂nnen auch mehrere Eigenschaften sein. Diese Liste ben√∂tigen Sie sp√§ter."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#trial",
    "href": "pages/chapters/psychopy_experiments.html#trial",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "2.2 Trial",
    "text": "2.2 Trial\n\nErg√§nzen Sie alle Elemente, die f√ºr einen vollst√§ndigen Trial notwendig sind:\n\nAntwort der Versuchsperson / Response (siehe auch 2.4)\nInter-Trial-Intervall (ITI): kann vor oder nach dem Stimulus eingef√ºgt werden. (Die Zeit des ITI wird oft variiert. Dies m√ºsste also auch auf die Liste oben)\nFixationskreuz?\nMask?\nFeedback?"
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#trialschleife",
    "href": "pages/chapters/psychopy_experiments.html#trialschleife",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "2.3 Trialschleife",
    "text": "2.3 Trialschleife\nSie m√ºssen nicht alle Trials (oder in PsychoPy: Routines) des Experiments einzeln programmieren, sondern k√∂nnen diese wiederholen, in dem Sie eine Trial-Schleife (loop) um den Trial herum erstellen.\n\nErstellen Sie einen loopindem Sie im Feld Flow auf Insert loop klicken.\n\nMit loopType k√∂nnen Sie steuern, die Bedingungen randomisiert/gemischt oder sequentiell/der Reihe nach angezeigt werden sollen.\nMit nReps k√∂nnen Sie angeben, wie oft jeder Stimulus wiederholt werden soll. Haben Sie also einen Stimulus mit zwei zu varierenden Eigenschaften (die je 3 Stufen haben) (also 9 Zeilen im conditions-File und nReps= 2, ergibt das 18 Trials.\n\n\nMittels diesen Schleifen k√∂nnen die Bedingungen implementiert werden z.B. dass sich der Stimulus bei jedem Trial ver√§ndert. Dies kann mit einer conditions-Datei spezifiziert werden, idealerweise im .csv oder .xlsx-Format.\n\nDie Endung .csv bedeutet, dass die Daten als comma separated values abgespeichert werden, also durch ein Komma getrennt. Dieses Dateiformat eignet sich besser als .xlsx, weil es mit vielen Programmen kompatibel und gut einlesbar ist.\n\nBeispielsweise wollen wir drei verschiedene Worte anzeigen (dog, cat und rabbit) und dieses Wort unterschiedlich lange anzeigen (Dauer: 1, 10 und 100 Frames). Die Versuchspersonen sollen dann den Anfangsbuchstaben des Wortes dr√ºcken, also d f√ºr dog, c f√ºr cat und r f√ºr rabbit.\n\nUm die Bedingungen (in unserem Fall: die sich ver√§ndernden Stimuluseigenschaften) zu definieren, erstellen wir eine .csvoder .xlsx-Datei (z.B. in Excel/Notepad/etc.) mit dem Namen conditions und speichern dieses im selben Ordner wie das Experiment.\n\nF√ºgen Sie f√ºr jedes sich ver√§ndernde Element einen Variablennamen und die entsprechenden Werte ein (dies sind die Eigenschaften, die Sie sich bei Punkt 2.1 notiert haben). Die Variablennamen schreiben wir immer in die oberste Zeile der Datei.\nWenn wir z.B. einen Text anzeigen m√∂chten, schreiben wir in die erste Zeile word und duration.\nIn die Spalte unter die Variablennamen schreiben wir die Werte.\nAls Beispiel k√∂nnten die Worte die wir anzeigen lassen wollen cat, dog und rabbit lauten. Dann stehen in der Spalte word, diese 3 W√∂rter unter dem Variablennamen. Unter dem Variablennamen duration geben wir die Anzahl Frames ein, also 1, 10 und 100. Wir wollen jedes Wort mit jeder Dauer kombinieren. Das ergibt 9 Zeilen.\nF√ºgen Sie in jeder Zeile unter dem Variablennamen corrAns die jeweils korrekte Antwort ein.\nF√ºgen Sie, falls vorhanden, in jeder Zeile weitere wichtige Information zum Stimulus ein.\nIm Beispiel m√∂chten Sie z.B. sp√§ter fleischfressende mit pflanzenfressenden Tieren vergleichen, deshalb eine Spalte meat. Dies ver√§ndert im Experiment nichts, dient aber am Schluss zur Auswertung, weil diese Variable auch immer in den Datensatz geschrieben wird.\n\n\nF√ºgen Sie nun im Loop-Fenster die conditions-Datei ein.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nJede Zeile in der conditions-Datei unterhalb des Variablennamens entspricht einer Bedingung (condition).\nSetzen Sie nReps auf 1 w√§hrend Sie das Experiment erstellen, so sparen Sie Zeit.\n\n\nIm PsychoPy k√∂nnen Sie Variablen mit einem vorangestellten $einf√ºgen.\n\n√ñffnen Sie nun wieder das Stimulusfenster und passen Sie dort die Stimuluseigenschaften an. Anstatt von hard-coded values (also einmalig, fix festgelegten Werten) geben wir nun einen Variablennamen ein. Der Stimulus darf nicht auf constant gesetzt sein, sonst kann er sich nicht Trial f√ºr Trial ver√§ndern, setzen Sie ihn deshalb unbedingt auf set every repeat.\nIn unserem Beispiel f√ºgen wir bei text die Laufvariable (ver√§ndernde Eigenschaft) ein: $word. Die Anzeigedauer des Textes soll $duration in Frames sein.\n\n\n\nLassen Sie das Experiment laufen und kontrollieren Sie, ob alles funktioniert hat.\n\n\n\n\n\n\n\nTipp\n\n\n\nMit dieser Methode k√∂nnen Sie auch Instruktionen, ITIs, etc. variieren lassen."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#antworten-aufnehmen",
    "href": "pages/chapters/psychopy_experiments.html#antworten-aufnehmen",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "2.4 Antworten aufnehmen",
    "text": "2.4 Antworten aufnehmen\nIn PsychoPy muss definiert werden, wie die Antwort der Versuchsperson aufgenommen wird. Dies kann mit der Maus, der Tastatur oder anderen Devices umgesetzt werden. Die M√∂glichkeiten sehen Sie unter Responses.\n\nF√ºgen Sie eine Antwortkomponente hinzu und benennen Sie diese sinnvoll.\nIn unserem Beispiel m√∂chten wir, dass die Versuchsperson mittels Keyboard antwortet.\n\nMit Force end of Routine k√∂nnen Sie einstellen, ob eine Antwort den Trial beendet und mit dem n√§chsten fortf√§hrt.\nDer Namen der Antwortkomponente wird sp√§ter im Datensatz als Variable zu finden sein.\nWerden in einer Antwortkomponente namens key_resp mittels Tastendruck Antwort und Response Time aufgenommen, heissen die Variablen dann key_resp.keys(gedr√ºckte Taste) und key_resp.rt (Antwortdauer).\nEntscheiden Sie, ob PsychoPy √ºberpr√ºfen soll, ob die richtige Antwort gegeben wurde.\nWenn Sie dies m√∂chten, gleicht PsychoPy in unserem Beispiel die gegebene Antwort (key_resp.keys) mit der daf√ºr eingegebenen Variable (hier corrAns) ab. Stimmen diese √ºberein, f√ºgt es in die Variable key_resp.corr 1 ein, wenn nicht 0).\nMit first key definieren Sie, dass der erste Tastendruck z√§hlt."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#weitere-elemente",
    "href": "pages/chapters/psychopy_experiments.html#weitere-elemente",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "2.5 Weitere Elemente",
    "text": "2.5 Weitere Elemente\nIn PsychoPy GUI wird Ihnen im Fenster Floweine Art Flowchart angezeigt. Hier sehen Sie, welche Elemente Ihr aktuelles Experiment enth√§lt.\n\nF√ºgen Sie nun alle weiteren Elemente, die Sie zu Beginn auf Ihrer Flowchart eingezeichnet hatten, z.B.\n\nBegr√ºssung\nEinverst√§ndnis\nInstruktion\nDebriefing, Verabschiedung\n\nLassen Sie das Experiment laufen und kontrollieren Sie, ob alles funktioniert hat.\n\n\n\n\n\n\n\nTipp\n\n\n\nBeim Programmieren lohnt es sich oft, die kleinen Schritte zwischenzutesten, weil es dann einfacher ist herauszufinden, wo genau der Fehler passiert ist."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#variable-dauer-von-elementen",
    "href": "pages/chapters/psychopy_experiments.html#variable-dauer-von-elementen",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "5.1 Variable Dauer von Elementen",
    "text": "5.1 Variable Dauer von Elementen\n\nFixationskreuz und ITI mit randomisierter Dauer\nUm das Experiment f√ºr die Versuchsperson unvorhersehbarer zu machen, implementieren wir vor dem eigentlichen Stimulus ein Fixationskreuz mit variabler L√§nge. Diese L√§nge soll 0.2, 0.4, 0.6, oder 0.8 Sekunden betragen.\n\nF√ºgen Sie einen Codeblock code_fixationcross ein und definieren Sie unter Begin Routine die Variable fixationcross_duration.\n\nF√ºgen Sie einen Textblock fixationcross ein mit dem Text + und Schriftgr√∂sse 10. Geben Sie unter duration Ihre vorher definierte Variable ein (vergessen Sie dabei das $ nicht): $fixationcross_duration.\n\n\n\n\n\n\n\n\nHands-on: Variable ITI einbauen\n\n\n\nF√ºgen Sie nach dem Stimulus eine ITI mit variabler Dauer hinzu.\nEinfachere Variante: Die ITI soll 10, 20, 30, 40 oder 50 Frames betragen.\nSchwierigere Variante: Die ITI soll eine Zufallszahl zwischen 0.2 und 0.8 Sekunden betragen."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#feedback",
    "href": "pages/chapters/psychopy_experiments.html#feedback",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "5.2 Feedback",
    "text": "5.2 Feedback\nEs gibt Experimente, welche Feedback erfordern. Oft wird vor der Datenerhebung ein √úbungsblock eingebaut, welcher Feedback enth√§lt, so dass die Versuchspersonen wissen, ob sie den Task richtig verstanden haben.\n\nErstellen Sie zuerst eine Trialschleife mit einem Stimulus und einer Response.\nF√ºgen Sie nach dem Stimulus und der Antwort (aber innerhalb der Trialschleife!) eine Routine feedback ein.\nF√ºgen Sie innerhalb der Routine feedback eine Codekomponente hinzu. In dieser Komponente k√∂nnen Sie nun\n\nF√ºgen Sie nun eine Textkomponente hinzu und f√ºgen Sie beim Textfeld die Variable $response_msg ein, damit die Versuchsperson abh√§ngig von ihrer Antwort das entsprechende Feedback erh√§lt, welches zuvor in der Codekomponente definiert wurde.\n\n\n\n\n\n\n\n\nHands-on: Feedback geben\n\n\n\nSie k√∂nnen mittels einer Codekomponente auch reagieren, wenn die Versuchsperson zu schnell, zu langsam oder gar nicht antwortet.\n\nErstellen Sie einen √úbungsdurchgang. F√ºgen Sie eine Code-Komponente hinzu und legen Sie fest, welches Feedback die Versuchsperson erhalten soll.\n\nEinfache Variante: Geben Sie der Person Feedback, ob ihre Antwort richtig oder falsch war.\nMittelschwere Variante: Geben Sie der Person Feedback, wenn Sie zu schnell oder zu langsam antwortet.\nSchwere Variante: Erstellen Sie einen Counter, welcher der Versuchsperson anzeigt, wie gut sie ist, indem f√ºr jede richtige Antwort 5 Punkte erh√§lt, f√ºr jede falsche Antwort 5 Punkte abgezogen werden.\nFalls Sie zur Geschwindigkeit R√ºckmeldung geben wollen oder einen Counter bauen, k√∂nnen Sie etwas in dieser Art machen.\nif dots_keyboard_response.keys is None:\n    response_text = \"miss\"\n\nelif dots_keyboard_response.rt &lt;= 0.1:\n    response_text = \"too fast\"\n\nelse:\n    if (direction == \"left\" and dots_keyboard_response.keys == \"f\" or \n        direction == \"right\" and dots_keyboard_response.keys == \"j\"):\n        response_text = \"+5 points\"\n    else:\n        response_text = \"+0 points\""
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#degrees-of-visual-angle",
    "href": "pages/chapters/psychopy_experiments.html#degrees-of-visual-angle",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "6.1 Degrees of Visual Angle",
    "text": "6.1 Degrees of Visual Angle\nOftmals werden Gr√∂ssenangaben von Stimuli noch in Pixel oder Zentimeter, sondern in degrees of visual angle gemacht. Dies hat den Vorteil, dass die Angaben nicht vom Monitor selber oder der Entferung vom Monitor abh√§ngig sind. Degrees of visual angle gibt die wahrgenommene Gr√∂sse des Stimulus an, und ber√ºcksichtigt die Gr√∂sse des Monitors und des Stimulus, und die Entfernung der Versuchsperson vom Monitor. Weitere Informationen dazu finden Sie auf der Website von OpenSesame. √úblicherweise entspricht ein degrees of visual angle etwa einem cm bei einer Entfernung von 57 cm vom Monitor.\n\nOpenSesame ist ein weiteres, Python-basierendes Programm f√ºr die Erstellung behavioraler Experimente.\n\nZur Umrechnung zwischen cm und degrees of visual angle finden Sie unter diesem Link mehr Information."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#timing",
    "href": "pages/chapters/psychopy_experiments.html#timing",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "6.2 Timing",
    "text": "6.2 Timing\nFrames vs.¬†time (sec or ms): Die pr√§ziseste Art zur Steuerung des Timings von Stimuli besteht darin, sie f√ºr eine festgelegte Anzahl von Frames zu pr√§sentieren. Bei einer Framerate von 60 Hz k√∂nnen Sie Ihren Stimulus nicht z. B. 120 ms lange pr√§sentieren; die Bildperiode w√ºrde Sie auf einen Zeitraum von 116,7 ms (7 Bilder) oder 133,3 ms (8 Bilder) beschr√§nken. Dies ist besonders wichtig f√ºr Reaktionszeit-Aufgaben und EEG-Studien, wo ein pr√§zises Millisekunden-Timing erforderlich ist. Hier finden Sie weitere Informationen zu diesem Thema: Presening Stimuli - Psychopy.\n\nHertz ist eine Einheit die angibt, wie h√§ufig etwas pro Sekunde passiert. Hertz kann wie Mal pro Sekunde ausgesprochen werden. 60 Hertz bedeutet also, 60 Mal pro Sekunde."
  },
  {
    "objectID": "pages/chapters/psychopy_experiments.html#individualisierte-aufgabenschwierigkeit-schwellenmessung",
    "href": "pages/chapters/psychopy_experiments.html#individualisierte-aufgabenschwierigkeit-schwellenmessung",
    "title": "Verhaltensexperimente mit PsychoPy",
    "section": "6.3 Individualisierte Aufgabenschwierigkeit / Schwellenmessung",
    "text": "6.3 Individualisierte Aufgabenschwierigkeit / Schwellenmessung\nIm Random Dot Experiment macht es z.B. f√ºr gewisse Fragestellungen Sinn die Aufgabenschwierigkeit f√ºr jede Person anzupassen, da sonst ceiling/floor-Effekte auftreten k√∂nnen.\nIn PsychoPy kann ein Staircase in einem Loop verwendet werden, um die Schwierigkeit einer Aufgabe basierend auf der Leistung der Teilnehmer dynamisch anzupassen. Sie ist besonders h√§ufig in Experimenten zur Schwellenmessung, bei denen das Ziel darin besteht, die kleinste wahrnehmbare Reizintensit√§t zu bestimmen. Hier finden Sie weitere Informationen zu diesem Thema: Using a Staircase - PsychoPy."
  },
  {
    "objectID": "pages/chapters/quiz_one.html",
    "href": "pages/chapters/quiz_one.html",
    "title": "Quiz 1",
    "section": "",
    "text": "Sie finden das erste Quiz in Ihrer Iliasgruppe:\nILIAS (Montag) üëâ 468703-FS2024-0\nILIAS (Mittwoch) üëâ 468703-FS2024-1\nSie haben in der Veranstaltung Zeit das Quiz auszuf√ºllen. Falls Sie nicht an der Veranstaltung teilnehmen, k√∂nnen Sie das Quiz bis zum 28. Februar 2024 um 23:55 ausf√ºllen."
  },
  {
    "objectID": "pages/chapters/quiz_one.html#quiz-1",
    "href": "pages/chapters/quiz_one.html#quiz-1",
    "title": "Quiz 1",
    "section": "",
    "text": "Sie finden das erste Quiz in Ihrer Iliasgruppe:\nILIAS (Montag) üëâ 468703-FS2024-0\nILIAS (Mittwoch) üëâ 468703-FS2024-1\nSie haben in der Veranstaltung Zeit das Quiz auszuf√ºllen. Falls Sie nicht an der Veranstaltung teilnehmen, k√∂nnen Sie das Quiz bis zum 28. Februar 2024 um 23:55 ausf√ºllen."
  },
  {
    "objectID": "pages/chapters/random_dot_experiment.html",
    "href": "pages/chapters/random_dot_experiment.html",
    "title": "Random Dot Paradigma",
    "section": "",
    "text": "Jeden Tag treffen wir Tausende von kleinen Entscheidungen, meistens unter gewissem Zeitdruck. Viele davon sind trivial (z. B. welches Paar Socken man anzieht) und automatisch (z. B. ob man die Espresso- oder Lungo-Taste auf der Kaffeemaschine dr√ºckt). Die meisten Entscheidungen im wirklichen Leben setzen sich eigentlich aus zwei Entscheidungen zusammen: Einerseits der Entscheidung, mit dem Abw√§gen aufzuh√∂ren und aufgrund des aktuellen Wissenstandes zu handeln. Andererseits die Wahl oder Entscheidungshandlung selbst. Dieser sequentielle Charakter der Entscheidungsfindung ist eine grundlegende Eigenschaft des menschlichen Nervensystems und spiegelt seine Unf√§higkeit wieder, Informationen sofort zu verarbeiten.\nPerzeptuelle Entscheidungen sind Entscheidungen, welche auf der Wahrnehmung, Einordnung und Integration von Sinnesreizen beruhen. Um beispielsweise eine Strasse sicher √ºberqueren zu k√∂nnen, m√ºssen wir mittels den Sinnesinformationen der Augen und Ohren sowie der Verarbeitung dieser Reize einsch√§tzen mit welcher Geschwindigkeit ein herannahendes Auto unterwegs ist und ob wir lieber abwarten bis es vorbeigefahren ist. Innerhalb der Neurowissenschaften wird perceptual decision making untersucht, um die neuronalen Schaltkreise welche Wahrnehmungssignale kodieren, speichern und analysieren zu verstehen und mit beobachtbarem Verhalten in Verbindung zu bringen. Von Interesse ist zum Beispiel wie die Entscheidung ausf√§llt, wenn die sensorischen Daten undeutlich oder sogar widerspr√ºchlich sind. Besonders spannend ist auch wie Vorwissen (prior knowledge) auf das Entscheidungsverhalten einwirkt.\n\nPerceptual decision making is the process by which sensory information is used to guide behavior toward the external world. This involves gathering information through the senses, evaluating and integrating it according to the current goals and internal state of the subject, and using it to produce motor responses. In contrast to choice behavior and decision making in general (‚Ä¶) perceptual decision making emphasizes the role of sensory information in directing behavior (‚Ä¶) within neuroscience, the goal is to reveal the computational mechanisms whereby neural circuits encode, store, and analyze perceptual signals; combine them with other behaviorally relevant information; and use them to resolve conflicts between competing motor plans. (Hauser and Salinas (2014))\n\nObwohl das Treffen von Entscheidungen f√ºr uns etwas sehr Vertrautes ist, ist das Wissen darum, wie das Gehirn diese Entscheidungsaufgaben l√∂st noch sehr begrenzt. Eine einzelne Entscheidung kann schon sehr komplex sein. Um die Dynamik der Entscheidungsfindung zu verstehen, konzentrieren sich die meisten Studien deshalb auf einfache, wiederholbare Wahlprobleme mit nur zwei (bin√§ren) Antwortm√∂glichkeiten. Ein typisches Paradigma in neurowissenschaftlichen Studien ist das random-dot motion paradigm. Hierbei muss eine Person entscheiden in welche Richtung sich eine Punktewolke bewegt.\n\nDas Experiment Bias in the brain von Mulder et al. (2012) ist eine Reaktionszeit (RT) Version eines random-dot motion direction discrimination task. Sie k√∂nnen hier nachlesen, wie der Task verwendet wurde um den Einfluss von Vorwissen auf neuronale Aktivit√§t im Gehirn mittels fMRI zu untersuchen.\n\n\n\n\n\n\n\nHands-on: Random Dot Experiment\n\n\n\nLesen Sie zuerst hier eine kurze Einf√ºhrung in das Random Dot Paradigma.\nBesprechen Sie dann in kleinen Gruppen folgende Fragen:\n\nF√ºr welche neurowissenschaftlichen Forschungsfragen eignet sich dieser Task? Mit welchen weiteren Methoden (fMRI, EEG, Hirnstimulation, etc.) liesse er sich kombinieren und was k√∂nnte man dabei lernen? Welche Patientengruppen zeigen auff√§llige Antworten in diesem Task?\nWelche ‚ÄúElemente‚Äù des Experiments konnten Sie identifizieren?\nWelche Stimuluseigenschaften sind relevant f√ºr das Experiment? Was macht die Aufgabe einfach? Was macht sie schwieriger?\nWelche Antwortm√∂glichkeiten haben die Versuchspersonen?\nWelche Bedingungen w√ºrden Sie vergleichen?\nWelche Stimuluseigenschaften sind wichtig, damit Sie die Resultate nicht verf√§lschen oder auf was muss geachtet werden bei der Stimulusauswahl?\nWas denken Sie, misst der Task was er soll? Wie werden die Verhaltensdaten Ihrer Sch√§tzung nach aussehen?\n\nTipp: Schauen Sie sich diese Stimulusoption hier an.\n[~10 Minuten]\n\n\n\n\nIn unserem Experiment l√∂sen die Versuchspersonen einen Random Dot Task zweimal (in zwei Bl√∂cken). In jedem Block erhalten sie eine andere Instruktion, die Aufgabe bleibt jedoch dieselbe: Sie m√ºssen herausfinden in welche Richtung sich die Punktewolke bewegt. In einem Block werden sie instruiert die Aufgabe m√∂glichst schnell zu l√∂sen. Im anderen Block werden sie instruiert die Aufgabe m√∂glichst richtig zu l√∂sen. Wir werden dann analysieren, wie sich das Entscheidungsverhalten von Menschen ver√§ndert, je nachdem wie sie instruiert wurden.\nDas Random Dot Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Random Dot Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop l√§uft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die √úbungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enth√§lt. Wenn alles ok ist, ist das Experiment bereit f√ºr √úbung 1. F√ºhren Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgef√ºhrt?\nWie lange wird der Dot-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?"
  },
  {
    "objectID": "pages/chapters/random_dot_experiment.html#kurzbeschrieb-kursexperiment",
    "href": "pages/chapters/random_dot_experiment.html#kurzbeschrieb-kursexperiment",
    "title": "Random Dot Paradigma",
    "section": "",
    "text": "In unserem Experiment l√∂sen die Versuchspersonen einen Random Dot Task zweimal (in zwei Bl√∂cken). In jedem Block erhalten sie eine andere Instruktion, die Aufgabe bleibt jedoch dieselbe: Sie m√ºssen herausfinden in welche Richtung sich die Punktewolke bewegt. In einem Block werden sie instruiert die Aufgabe m√∂glichst schnell zu l√∂sen. Im anderen Block werden sie instruiert die Aufgabe m√∂glichst richtig zu l√∂sen. Wir werden dann analysieren, wie sich das Entscheidungsverhalten von Menschen ver√§ndert, je nachdem wie sie instruiert wurden.\nDas Random Dot Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Random Dot Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop l√§uft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die √úbungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enth√§lt. Wenn alles ok ist, ist das Experiment bereit f√ºr √úbung 1. F√ºhren Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgef√ºhrt?\nWie lange wird der Dot-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?"
  },
  {
    "objectID": "pages/chapters/rmarkdown.html",
    "href": "pages/chapters/rmarkdown.html",
    "title": "RStudio Projects und RMarkdown",
    "section": "",
    "text": "RStudio Projekte erm√∂glichen vereinfachtes Arbeiten mit R, da alle Dateien, die in diesem Projektordner gespeichert sind, direkt verf√ºgbar sind. So kann die aktuelle R Session abgespeichert werden und beim n√§chsten √ñffnen kann dort weitergearbeitet werden wo man aufgeh√∂rt hat. Projekte erm√∂glichen somit ein stabiles working directory f√ºr ein Datenanalyse-Projekt.\n\n\n\n\n\n\nHands-on: Erstellen eines RStudio Projects\n\n\n\n\n√ñffnen Sie RStudio.\nErstellen Sie ein neues RStudio-Project\n\nKlicken Sie daf√ºr auf File &gt; New Project\nBenennen Sie das Project neurosci_complab_rmarkdown und speichern Sie es an einem sinnvollen Ort auf Ihrem Computer.\n\n\n\n\nEs empfiehlt sich bei RProjekten eine Einstellungs√§nderung (Tools&gt; Project Options...) vorzunehmen, so dass die aktuell gespeicherten Variablen bei jedem Schliessen vom Projekt gel√∂scht werden. Dies verhindert, dass der aktuelle Code nur aufgrund fr√ºherer Speicherung l√§uft."
  },
  {
    "objectID": "pages/chapters/rmarkdown.html#r-markdown-file-erstellen-und-ausf√ºhren",
    "href": "pages/chapters/rmarkdown.html#r-markdown-file-erstellen-und-ausf√ºhren",
    "title": "RStudio Projects und RMarkdown",
    "section": "R Markdown File erstellen und ausf√ºhren",
    "text": "R Markdown File erstellen und ausf√ºhren\n\n\n\n\n\n\nHands-on: R Markdown File in einem Projekt erstellen\n\n\n\n\nErstellen Sie ein neues .Rmd-File (File &gt; New File &gt; R Markdown).\nGeben Sie einen Titel und Ihren Namen ein und w√§hlen Sie HTML als Output-Format.\nSpeichern Sie dieses Dokument unter dem Namen rmarkdown_exampleab.\n\nWelches Format (Endung) hat das abgespeicherte R Markdown Skript nun in Ihrem Ordner?\n\n\n\nWeiterf√ºhrende Informationen:\nüëâ Einf√ºhrung in die Verwendung von R/RStudio/Notebooks im Rahmen des Psychologie Studiums von Andrew Ellis und Boris Mayer Einf√ºhrung in R\nüëâ Sehr kompakte, praxisnahe Einf√ºhrung in R Markdown von Danielle Navarro (Slidedeck in englisch) Einf√ºhrung in R Markdown"
  },
  {
    "objectID": "pages/chapters/rmarkdown.html#knitten",
    "href": "pages/chapters/rmarkdown.html#knitten",
    "title": "RStudio Projects und RMarkdown",
    "section": "Knitten",
    "text": "Knitten\nMit Knit wird das R Markdown Skript ausgef√ºhrt und eine zus√§tzliche Datei wird erstellt, z.B. ein html-File.\n\n\n\n\n\n\nHands-on: Knitten\n\n\n\nF√ºhren Sie das File mit Knit aus und vergleichen Sie das R Markdown Skript mit dem Output den Sie erhalten haben. Was f√§llt Ihnen auf?\n\nWas ist nicht mehr zu sehen?\nWas ist zus√§tzlich zu sehen?\nWas hat sich im Projekt-Ordner ver√§ndert?"
  },
  {
    "objectID": "pages/chapters/rmarkdown.html#yaml-header",
    "href": "pages/chapters/rmarkdown.html#yaml-header",
    "title": "RStudio Projects und RMarkdown",
    "section": "YAML header",
    "text": "YAML header\nAm Anfang des R Markdown Skripts befindet sich der YAML header. Hier werden Informationen zu Titel, Autor:Innen, Datum, Outputformat, Literaturverzeichnis und Layout festgelegt.\n\nYAML: Yet Another Markdown Language\n\nDas Layout kann unter themege√§ndert werden. Das kann beispielsweise wie folgt aussehen:\noutput:\n  html_document:\n    theme: cosmo\nAchtung: Die Einr√ºckungen m√ºssen genau stimmen! Hier wurde das theme namens cosmo ausgew√§hlt. M√∂gliche andere themessind z.B. default, cerulean, journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, yeti.\n\n\n\n\n\n\nHands-on: Namen und Einstellungen anpassen\n\n\n\n\nGeben Sie dem Dokument einen neuen Titel z.B. R Markdown Einf√ºhrung\n√Ñndern Sie das Layout so, dass es Ihnen gef√§llt."
  },
  {
    "objectID": "pages/chapters/rmarkdown.html#text-erstellen-in-r-markdown",
    "href": "pages/chapters/rmarkdown.html#text-erstellen-in-r-markdown",
    "title": "RStudio Projects und RMarkdown",
    "section": "Text erstellen in R Markdown",
    "text": "Text erstellen in R Markdown\nText kann in R Markdown Files nicht nur geschrieben, sondern auch relativ simpel formatiert werden.\nüëâ Hier k√∂nnen Sie das Cheatsheet herunterladen. Auf der rechten Seite finden Sie die Informationen f√ºr die Textformatierung.\nEs empfiehlt sich das Skript anfangs h√§ufig zu knitten, so findet man den Fehler schneller, weil man noch weiss, was man als letztes ver√§ndert hat. Code kann aber auch einfach innerhalb der Code-Chunks √ºberpr√ºft werden.\n\n\n\n\n\n\nHands-on: Texte, Formeln und Bilder in R Markdown einf√ºgen\n\n\n\n\nL√∂schen Sie alles bis auf den YAML-Header\nSchreiben Sie im Textbereich eine √úberschrift f√ºr ein Kapitel, ein Unterkapitel und normalen Text.\nSchreiben Sie im Text etwas kursiv und etwas fett.\nErstellen Sie im Textbereich eine Liste mit 3 Punkten.\nSchreiben Sie alpha innerhalb von $, was passiert?\nF√ºgen Sie die untenstehende Formel in den Text ein. Verwenden Sie daf√ºr zwei Dollarzeichen am Anfang und am Ende. Was passiert?\n\n\\[\na^2 + b^2 = c^2\n\\]\n\nF√ºgen Sie einen Link ein, knitten Sie das File und schauen Sie ob der Link funktioniert. K√∂nnen Sie einen Link nur mit einem unterstrichenen Text anzeigen, so dass die Linkadresse nicht sichtbar ist?\nF√ºgen Sie ein Bild ein. Speichern Sie dieses Bild zuerst in einem Ordner namens img im Projektordner.\n\n\n\n\nHier finden Sie weiterf√ºhrende Hilfe zum Einf√ºgen von Mathnotationen.\n\nMathematics in RMarkdown\nRMarkdown: The definitive Guide"
  },
  {
    "objectID": "pages/chapters/rmarkdown.html#code-erstellen-in-r-markdown",
    "href": "pages/chapters/rmarkdown.html#code-erstellen-in-r-markdown",
    "title": "RStudio Projects und RMarkdown",
    "section": "Code erstellen in R Markdown",
    "text": "Code erstellen in R Markdown\nCode muss jeweils in einem Code-Chunk eingef√ºgt werden. Ein Code-Chunk kann unter Code &gt; Insert Chunk eingef√ºgt werden oder mit dem K√ºrzel Ctrl+Alt+ I.\nCode-Chunks werden mit ``` begonnen und beendet. In den geschweiften Klammern steht r, das bedeutet dass der Code in R geschrieben ist. In dieser Klammer kann dem Code-Chunk einen Namen gegeben und bestimmt werden, ob der Code ausgef√ºhrt und z.B. nur angezeigt werden soll, sowie ob der Output des Codes angezeigt werden soll.\nMit dem gr√ºnen Pfeil kann der Code-Chunk einzeln ausgef√ºhrt werden. Aber auch einzelne Zeilen k√∂nnen ausgef√ºhrt werden, genau so wie in einem .R- Skript.\n\n\n\n\n\n\nHands-on: Code in R Markdown einf√ºgen\n\n\n\n\nErstellen Sie einen Code-Chunk, der ausgef√ºhrt, aber nicht angezeigt wird. Erstellen Sie eine Variable mit dem Namen numbers, die 10 Zahlen enth√§lt.\nErstellen Sie ein Code-Chunk, der ausgef√ºhrt wird und dessen Output angezeigt wird. Berechnen Sie in diesem Chunk den Mittelwert und die Standardabweichung von numbers.\nErstellen Sie einen Plot mit plot(numbers).\nKnitten Sie das File, um zu √ºberpr√ºfen, ob alles funktioniert\n\nüëâ Schauen Sie f√ºr Hilfe nochmals im Cheatsheet nach oder dr√ºcken Sie auf das Zahnr√§dchen-Symbol beim Code-Chunk.\nF√ºr Fortgeschrittene:\n\nTesten Sie, ob Sie Ihr File auch zu einem PDF knitten k√∂nnen.\nErstellen Sie eine Tabelle\nErstellen Sie einen Glossar\nErstellen Sie ein Dokument mit Reitern oben (z.B. Data, Preprocessing, Analysis, Conclusions)\nF√ºgen Sie interaktive Elemente ein."
  },
  {
    "objectID": "pages/chapters/sdt_1.html",
    "href": "pages/chapters/sdt_1.html",
    "title": "Signal-Detektionstheorie",
    "section": "",
    "text": "Im Random Dot Experiment mussten die Versuchspersonen ein perzeptuelle Entscheidungsaufgabe bearbeiten. Dabei musste jeweils entscheiden werden, in welche Richtung sich eine Punktewolke bewegt.\nEs gibt verschiedene m√∂glichkeiten die Leistung der Versuchspersonen im Random Dot Task zu beschreiben.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n  %%c((Condition)):::A --&gt; r\n  s((Sensitivit√§t)):::A --&gt; r((resp)):::B\n  \n  \n  classDef A fill:#ffffff, r:45px\n  classDef B fill:#e5e4e4, r:45px\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 6 √ó 5\n  id           condition stimulus resp   corr\n  &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 sub-10209782 speed     left     right     0\n2 sub-10209782 speed     right    right     1\n3 sub-10209782 speed     left     left      1\n4 sub-10209782 speed     right    right     1\n5 sub-10209782 speed     right    right     1\n6 sub-10209782 speed     left     left      1",
    "crumbs": [
      "Modellierung von Daten",
      "Signal-Detektionstheorie"
    ]
  },
  {
    "objectID": "pages/chapters/sdt_1.html#dag",
    "href": "pages/chapters/sdt_1.html#dag",
    "title": "Signal-Detektionstheorie",
    "section": "",
    "text": "flowchart TD\n  %%c((Condition)):::A --&gt; r\n  s((Sensitivit√§t)):::A --&gt; r((resp)):::B\n  \n  \n  classDef A fill:#ffffff, r:45px\n  classDef B fill:#e5e4e4, r:45px",
    "crumbs": [
      "Modellierung von Daten",
      "Signal-Detektionstheorie"
    ]
  },
  {
    "objectID": "pages/chapters/sdt_1.html#daten",
    "href": "pages/chapters/sdt_1.html#daten",
    "title": "Signal-Detektionstheorie",
    "section": "",
    "text": "# A tibble: 6 √ó 5\n  id           condition stimulus resp   corr\n  &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 sub-10209782 speed     left     right     0\n2 sub-10209782 speed     right    right     1\n3 sub-10209782 speed     left     left      1\n4 sub-10209782 speed     right    right     1\n5 sub-10209782 speed     right    right     1\n6 sub-10209782 speed     left     left      1",
    "crumbs": [
      "Modellierung von Daten",
      "Signal-Detektionstheorie"
    ]
  },
  {
    "objectID": "pages/chapters/sdt_1.html#dag-1",
    "href": "pages/chapters/sdt_1.html#dag-1",
    "title": "Signal-Detektionstheorie",
    "section": "DAG",
    "text": "DAG\n\n\n\n\n\nflowchart TD\n  %%c((Condition)):::A --&gt; r\n  c((c)):::A --&gt; r\n  s((d')):::A --&gt; r((resp)):::B\n  \n  classDef A fill:#ffffff, r:30px\n  classDef B fill:#e5e4e4, r:30px",
    "crumbs": [
      "Modellierung von Daten",
      "Signal-Detektionstheorie"
    ]
  },
  {
    "objectID": "pages/chapters/sdt_1.html#daten-1",
    "href": "pages/chapters/sdt_1.html#daten-1",
    "title": "Signal-Detektionstheorie",
    "section": "Daten",
    "text": "Daten\n\n\n# A tibble: 6 √ó 5\n  id           condition stimulus resp   corr\n  &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 sub-10209782 speed     left     right     0\n2 sub-10209782 speed     right    right     1\n3 sub-10209782 speed     left     left      1\n4 sub-10209782 speed     right    right     1\n5 sub-10209782 speed     right    right     1\n6 sub-10209782 speed     left     left      1",
    "crumbs": [
      "Modellierung von Daten",
      "Signal-Detektionstheorie"
    ]
  },
  {
    "objectID": "pages/chapters/setup.html",
    "href": "pages/chapters/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Wir haben uns entschieden, in dieser Veranstaltung Python zu verwenden, um ein Experiment zu erstellen, und R f√ºr die Analyse der Daten. Matlab wird nicht verwendet; einerseits da es kommerziell ist, andererseits weil es aus unserer Sicht nicht die beste Wahl f√ºr die Analyse von Verhaltensdaten ist. Ausserdem ist es schon schwierig genug, eine Programmiersprache zu lernen, ohne gleichzeitig noch zwei weitere zu lernen."
  },
  {
    "objectID": "pages/chapters/setup.html#in-dieser-veranstaltung-verwendete-software",
    "href": "pages/chapters/setup.html#in-dieser-veranstaltung-verwendete-software",
    "title": "Setup",
    "section": "",
    "text": "Wir haben uns entschieden, in dieser Veranstaltung Python zu verwenden, um ein Experiment zu erstellen, und R f√ºr die Analyse der Daten. Matlab wird nicht verwendet; einerseits da es kommerziell ist, andererseits weil es aus unserer Sicht nicht die beste Wahl f√ºr die Analyse von Verhaltensdaten ist. Ausserdem ist es schon schwierig genug, eine Programmiersprache zu lernen, ohne gleichzeitig noch zwei weitere zu lernen."
  },
  {
    "objectID": "pages/chapters/setup.html#python",
    "href": "pages/chapters/setup.html#python",
    "title": "Setup",
    "section": "Python",
    "text": "Python\nWenn Sie Python suf Ihrem Rechner installieren wollen, k√∂nnen Sie entweder den offiziellen Installer https://www.python.org/downloads/ downloaden, oder die Anaconda Distribution https://www.anaconda.com/products/distribution verwenden. Die Anaconda Distribution ist eine Python-Distribution, die viele n√ºtzliche Pakete enth√§lt, die f√ºr wissenschaftliches Rechnen und Datenanalyse verwendet werden. Wenn man tats√§chlich mit Python arbeiten will, empfiehlt es sich, die Anaconda Distribution zu benutzen. Wir werden in dieser Veranstaltung Python benutzen, um ein Experiment zu programmieren. Daf√ºr reicht es aus, den PsychoPy Installer zu verwenden; diesen finden Sie unter diesem Link: PsychoPy. PsychoPy ist ein Python-basiertes Tool, mit dem sich sowohl in einer grafischen Benutzeroberfl√§che (GUI) als auch mit Python Code Experimente programmieren lassen."
  },
  {
    "objectID": "pages/chapters/setup.html#r",
    "href": "pages/chapters/setup.html#r",
    "title": "Setup",
    "section": "R",
    "text": "R\nAb der vierten Sitzung werden wir viel mit R arbeiten, um Daten aufzubereiten und grafisch darzustellen. Daf√ºr m√ºssen Sie die aktuelle Version von R installieren. Diese ist zurzeit R 4.3.2, und kann unter folgender URL geladen werden:\nR üëâ https://cloud.r-project.org/\nWir empfehlen f√ºr die Arbeit mit R die RStudio IDE zu verwenden. Diese ist kostenlos und kann unter folgender URL heruntergeladen werden:\nRStudio üëâ https://www.rstudio.com/products/rstudio/download/#download"
  },
  {
    "objectID": "pages/chapters/setup.html#jasp",
    "href": "pages/chapters/setup.html#jasp",
    "title": "Setup",
    "section": "JASP",
    "text": "JASP\nEinen Teil der Bayesianischen Analysen werden wir mit JASP durchf√ºhren. Die aktuelle Version von JASP ist 0.18.3 und kann unter folgender URL heruntergeladen werden:\nJASP üëâ https://jasp-stats.org/download/"
  },
  {
    "objectID": "pages/chapters/stroop_experiment.html",
    "href": "pages/chapters/stroop_experiment.html",
    "title": "Stroop Paradigma",
    "section": "",
    "text": "Der Stroop Task wurde 1935 zum ersten Mal beschrieben (Stroop, 1935) und ist einer der meist zitierten und verwendeten neuropsychologischen Aufgaben (MacLeod, 1991). In der Neuropsychologie wird der Stroop Color and Word Test (SCWT) verwendet, um die F√§higkeit zur Inhibition kognitiver Interferenz zu messen, welche entsteht wenn zwei Stimuluseigenschaften gleichzeitig verarbeitet werden sich aber widersprechen (Scarpina & Tagini, 2017). Teilweise misst der Task auch andere kognitive Funktionen, wie visuelle Suche oder Arbeitsged√§chtnis, weshalb der Vergleich von Bedingungen relevant ist (Peri√°√±ez et al., 2021).\nW√§hrend dem Stroop Task wird ein Text mit Farbw√∂rtern pr√§sentiert. Im kongruenten Durchgang entsprechen die Farben des Textes dem Farbwort (das Wort ‚Äúrot‚Äù wird in rot pr√§sentiert), im inkongruenten Durchgang unterscheiden sich die Farben des Textes vom Farbwort (das Wort ‚Äúrot‚Äù wird in gelber Farbe pr√§sentiert). Die Person muss angeben in welcher Farbe das Wort abgedruckt ist. In der kongruenten Bedingung f√§llt dies leichter, weil das gelesene Wort auch der Farbe entspricht. In der inkongruenten Bedingung verlangsamt sich die Geschwindigkeit durch die entstehende Interferenz von Wort und Farbe, da das Wort automatisch gelesen wird. Oft wird auch noch eine neutrale Bedingung verwendet, wo nur die Farbe oder das Wort pr√§sentiert werden.\n\n\n\nDaten Stroop-Task (Stroop, 1935)\n\n\n\nHier finden Sie eine Online-Version des Originalpapers. Interessierte finden das Review von MacLeod (1991) auf Ilias.\n\n\n\n\n\n\n\nHands-on: Stroop Task ausprobieren\n\n\n\nHier finden Sie eine englische Online-Version eines Stroop Tasks.\nTesten Sie die Demo und diskutieren Sie dann in kleinen Gruppen folgende Fragen:\n\nF√ºr welche neurowissenschaftlichen Forschungsfragen eignet sich dieser Task? Mit welchen weiteren Methoden (fMRI, EEG, Hirnstimulation, etc.) liesse er sich kombinieren und was k√∂nnte man dabei lernen? Welche Patientengruppen zeigen auff√§llige Antworten in diesem Task?\nWelche ‚ÄúElemente‚Äù des Experiments konnten Sie identifizieren?\nWelche Stimuluseigenschaften sind relevant f√ºr das Experiment? Was macht die Aufgabe einfach? Was macht sie schwieriger?\nWelche Antwortm√∂glichkeiten haben die Versuchspersonen?\nWelche Bedingungen w√ºrden Sie vergleichen?\nWelche Stimuluseigenschaften sind wichtig, damit Sie die Resultate nicht verf√§lschen oder auf was muss geachtet werden bei der Stimulusauswahl?\nWas denken Sie, misst der Task was er soll? Wie werden die Verhaltensdaten Ihrer Sch√§tzung nach aussehen?\n\n[~10 Minuten]\n\n\n\n\nIn diesem Experiment l√∂sen die Personen zwei Bedingungen des Stroop Task, einmal geben sie die Farben der W√∂rter an in einer kongruenten Bedingung (Wortinhalt und Wortfarbe) stimmen √ºberein. Einmal l√∂sen sie die Aufgabe in einer inkongruenten Bedingung (Wortinhalt und Wortfarbe stimmen nicht √ºberein).\nDie kongruente und inkongruente Bedingung kommen im selben Block vor. Die Instruktion lautet f√ºr beide Bedingungen gleich, da immer die Wortfarbe angegeben werden muss. Drei Farben werden verwendet: rot, blau und gelb.\nDas Stroop Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Stroop Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop l√§uft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die √úbungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enth√§lt. Wenn alles ok ist, ist das Experiment bereit f√ºr √úbung 1. F√ºhren Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgef√ºhrt?\nWie lange wird der Wort-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?\n\n\n\n\n\n\nMacLeod C. M. (1991). Half a century of research on the Stroop effect: an integrative review. Psychological Bulletin. 109(2), 163‚Äì203. https://doi.org/10.1037/0033-2909.109.2.163\nPeri√°√±ez, J. A., Lubrini, G., Garc√≠a-Guti√©rrez, A., & R√≠os-Lago, M. (2021). Construct validity of the stroop color-word test: influence of speed of visual search, verbal fluency, working memory, cognitive flexibility, and conflict monitoring. Archives of Clinical Neuropsychology, 36(1), 99-111. https://doi.org/10.1093/arclin/acaa034\nScarpina, F., & Tagini, S. (2017). The stroop color and word test. Frontiers in psychology, 8, 557. https://doi.org/10.3389/fpsyg.2017.00557\nStroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643‚Äì662. https://doi.org/10.1037/"
  },
  {
    "objectID": "pages/chapters/stroop_experiment.html#kurzbeschrieb-kursexperiment",
    "href": "pages/chapters/stroop_experiment.html#kurzbeschrieb-kursexperiment",
    "title": "Stroop Paradigma",
    "section": "",
    "text": "In diesem Experiment l√∂sen die Personen zwei Bedingungen des Stroop Task, einmal geben sie die Farben der W√∂rter an in einer kongruenten Bedingung (Wortinhalt und Wortfarbe) stimmen √ºberein. Einmal l√∂sen sie die Aufgabe in einer inkongruenten Bedingung (Wortinhalt und Wortfarbe stimmen nicht √ºberein).\nDie kongruente und inkongruente Bedingung kommen im selben Block vor. Die Instruktion lautet f√ºr beide Bedingungen gleich, da immer die Wortfarbe angegeben werden muss. Drei Farben werden verwendet: rot, blau und gelb.\nDas Stroop Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Stroop Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop l√§uft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die √úbungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enth√§lt. Wenn alles ok ist, ist das Experiment bereit f√ºr √úbung 1. F√ºhren Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgef√ºhrt?\nWie lange wird der Wort-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?"
  },
  {
    "objectID": "pages/chapters/stroop_experiment.html#referenzen",
    "href": "pages/chapters/stroop_experiment.html#referenzen",
    "title": "Stroop Paradigma",
    "section": "",
    "text": "MacLeod C. M. (1991). Half a century of research on the Stroop effect: an integrative review. Psychological Bulletin. 109(2), 163‚Äì203. https://doi.org/10.1037/0033-2909.109.2.163\nPeri√°√±ez, J. A., Lubrini, G., Garc√≠a-Guti√©rrez, A., & R√≠os-Lago, M. (2021). Construct validity of the stroop color-word test: influence of speed of visual search, verbal fluency, working memory, cognitive flexibility, and conflict monitoring. Archives of Clinical Neuropsychology, 36(1), 99-111. https://doi.org/10.1093/arclin/acaa034\nScarpina, F., & Tagini, S. (2017). The stroop color and word test. Frontiers in psychology, 8, 557. https://doi.org/10.3389/fpsyg.2017.00557\nStroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643‚Äì662. https://doi.org/10.1037/"
  },
  {
    "objectID": "pages/chapters/uebung_1_experiment.html",
    "href": "pages/chapters/uebung_1_experiment.html",
    "title": "√úbung 1 - Psychopy Experiment",
    "section": "",
    "text": "F√ºhren Sie selbst und mit 2 weiteren Personen das Stroop und das Random Dot Experiment durch. Laden Sie anschliessend die 6 Datens√§tze auf Ilias hoch. Die beiden Experimente dauern zusammen ca. 30 Minuten (auch abh√§ngig von den Versuchspersonen).\nWichtig: Die erhobenen Daten werden wir dann in den kommenden Sitzungen verwenden, achten Sie also auf gute Datenqualit√§t."
  },
  {
    "objectID": "pages/chapters/uebung_1_experiment.html#auftrag",
    "href": "pages/chapters/uebung_1_experiment.html#auftrag",
    "title": "√úbung 1 - Psychopy Experiment",
    "section": "",
    "text": "F√ºhren Sie selbst und mit 2 weiteren Personen das Stroop und das Random Dot Experiment durch. Laden Sie anschliessend die 6 Datens√§tze auf Ilias hoch. Die beiden Experimente dauern zusammen ca. 30 Minuten (auch abh√§ngig von den Versuchspersonen).\nWichtig: Die erhobenen Daten werden wir dann in den kommenden Sitzungen verwenden, achten Sie also auf gute Datenqualit√§t."
  },
  {
    "objectID": "pages/chapters/uebung_1_experiment.html#vorgehen",
    "href": "pages/chapters/uebung_1_experiment.html#vorgehen",
    "title": "√úbung 1 - Psychopy Experiment",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nLaden Sie die 2 Experimente herunter und testen Sie, ob Sie einwandfrei laufen. Die Experimente befinden sich auf Github. Sie k√∂nnen sie unter den untenstehenden Links downloaden. Klicken Sie daf√ºr auf den ZIP-Ordner, und dann auf View Raw oder auf das Icon mit ... und dort auf Download. Sie m√ºssen das File dann evtl. entzippen, bevor Sie das Experiment starten k√∂nnen. Bei Problemen finden Sie unten einen Abschnitt Troubleshooting. Wenn das nichts hilft, k√∂nnen Sie sich bei der n√§chsten Veranstaltung an uns wenden.\n\nStroop Experiment\nRandom Dot Experiment\n\nF√ºhren Sie selber die beiden Experimente durch.\n\nStellen Sie sicher, dass hier ein vollst√§ndiger Datensatz abgespeichert wird. Testen Sie erst dann zus√§tzliche Personen.\n\nLassen Sie 2 weitere Personen die beiden Experimente ausf√ºhren (jede Person soll beide Experimente ausf√ºhren).\n\nDie Personen m√ºssen zwischen 18 und 60 Jahren alt sein.\nDie Personen sollten eine normale oder korrigiert-zu-normale (Brille/Kontaktlinsen) Sehst√§rke haben.\nKeine Mitstudierenden aus dem Computerlab testen.\nAchten Sie darauf, dass die Personen die Aufgaben unabgelenkt l√∂sen k√∂nnen.\n\nLaden Sie die 6 Datens√§tze auf ILIAS hoch.\n\nZippen Sie bitte die 6 .csv-Datens√§tze vor dem Hochladen zu einem (!) Ordner.\nLaden Sie den ZIP-Ordner auf Ilias unter √úbung 1 hoch."
  },
  {
    "objectID": "pages/chapters/uebung_1_experiment.html#abgabetermine",
    "href": "pages/chapters/uebung_1_experiment.html#abgabetermine",
    "title": "√úbung 1 - Psychopy Experiment",
    "section": "Abgabetermine",
    "text": "Abgabetermine\nGruppe Montag: 21. M√§rz 2024 23:55\nGruppe Mittwoch: 23. M√§rz 2024 23:55"
  },
  {
    "objectID": "pages/chapters/uebung_1_experiment.html#trouble-shooting",
    "href": "pages/chapters/uebung_1_experiment.html#trouble-shooting",
    "title": "√úbung 1 - Psychopy Experiment",
    "section": "Trouble shooting",
    "text": "Trouble shooting\nBitte Fehlermeldung im Fenster genau durchlesen. Dort finden Sie Hinweise darauf, was schief gelaufen ist.\nDas Experiment startet nicht.\n\nUnter Einstellungen (Radsymbol) den Reiter Basic ausw√§hlen. Bei Use PsychoPy version die neuste PsychoPy Version ausw√§hlen.\n\nDas Experiment startet zwar, der Bildschirm ist aber dann einfach f√ºr eine kurze Zeit grau und das Fenster schliesst sich wieder.\n\nZugriffsrechte gegeben? (Bei Windows: Als Administrator starten, bei MacOS: Zugriffsrechte erteilen)\nUnter Einstellungen (Radsymbol) den Reiter Input ausw√§hlen. Keyboard Backend auf PsychToolbox statt ioHub setzen."
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html",
    "href": "pages/chapters/uebung_2_datawrangling.html",
    "title": "√úbung 2 - Data Wrangling",
    "section": "",
    "text": "Erstellen Sie eine automatisierte Datenverarbeitungs-Pipeline, die die Daten des Random Dot Experiments einlesen und vorverarbeiten. Erstellen Sie einen neuen Datensatz und einen Data Report."
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html#auftrag",
    "href": "pages/chapters/uebung_2_datawrangling.html#auftrag",
    "title": "√úbung 2 - Data Wrangling",
    "section": "",
    "text": "Erstellen Sie eine automatisierte Datenverarbeitungs-Pipeline, die die Daten des Random Dot Experiments einlesen und vorverarbeiten. Erstellen Sie einen neuen Datensatz und einen Data Report."
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html#vorgehen",
    "href": "pages/chapters/uebung_2_datawrangling.html#vorgehen",
    "title": "√úbung 2 - Data Wrangling",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nSetup\n\nErstellen Sie ein R-Project inkl. Ordner namens complab_datawrangling_randomdot.\nErstellen Sie in diesem Projekt-Ordner einen Ordner namens data.\nLaden Sie den Ordner mit den Datens√§tzen hier herunter. Entzippen Sie den Ordner und speichern Sie die Datens√§tze direkt in Ihrem data-Ordner.\nErstellen Sie ein neues .Rmd-File und speichern Sie dieses unter preprocessing_randomdot_data im Projekt-Ordner.\n\nAutomatierter Datenimport inkl. Vorverarbeiten (mit folgenden Schritten):\n\nEinlesen Datensatz (read.csv())\nFiltern der Daten, so dass nur Experimenttrials im Datensatz sind, keine √úbungsaufgaben. (filter())\nErstellen einer neue Variable trial, die die Trialnummer startend mit 1 angibt (mutate())\nDatensatz vereinfachen: Der Datensatz soll in dieser Reihenfolge folgende Informationen/Variablennamen enthalten (select()):\n\nVersuchspersonenidentifikation (id)\nTrialnummer (trial)\nBewegungsrichtung der Punkte (direction)\nInstruktionsbedingung (condition)\nKorrekte Antwort f√ºr diesen Trial (corrAns)\nAntwort der Versuchsperson (resp),\nwar die Antwort der Versuchsperson korrekt? (corr)\nAntwortzeit der Versuchsperson (rt)\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nAm besten erstellen Sie zuerst f√ºr einen Datensatz einen funktionierenden Vorverarbeitungsablauf. Dann erstellen Sie eine Funktion f√ºr diesen Ablauf. In einem letzten Schritt automatisieren Sie dann diesen Ablauf f√ºr alle Datens√§tze im Datenordner indem Sie eine Liste mit allen Filenamen erstellen. Sie k√∂nnen sich an dem Automatisierungsbeispiel mit dem Stroop Datensatz orientieren.\nDas Einlesen kann eine Weile dauern, es sind sehr viele Datens√§tze.\n\n\n\nDatensatz kontrollieren:\n\nL√∂schen Sie nun alle Variablen in der RStudio Umgebung (Environment) mit dem Besen-Icon oben rechts und f√ºhren Sie den Code nochmals aus. Wenn alles funktioniert, fahren Sie weiter.\nIhr Datensatz sollte nun wie untenstehend aussehen. Benutzen Sie dazu in Ihrem Code den Sie abgeben zwingend die Funktion glimpse().\n\n\n\n\nRows: 32,520\nColumns: 8\n$ id        &lt;chr&gt; \"sub-10209782\", \"sub-10209782\", \"sub-10209782\", \"sub-1020978‚Ä¶\n$ trial     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶\n$ direction &lt;chr&gt; \"left\", \"right\", \"left\", \"right\", \"right\", \"left\", \"right\", ‚Ä¶\n$ condition &lt;chr&gt; \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed‚Ä¶\n$ corrAns   &lt;chr&gt; \"left\", \"right\", \"left\", \"right\", \"right\", \"left\", \"right\", ‚Ä¶\n$ resp      &lt;chr&gt; \"right\", \"right\", \"left\", \"right\", \"right\", \"left\", \"right\",‚Ä¶\n$ corr      &lt;int&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ rt        &lt;dbl&gt; 5.8753379, 0.8221688, 0.9351370, 0.7454416, 1.5085055, 0.939‚Ä¶\n\n\n\nDatensatz speichern\n\nSpeichern Sie den neuen Datensatz (der jetzt alle Datens√§tze vorverarbeitet und zusammengef√ºgt enth√§lt) als .csv-File namens dataset_rdk_clean.csv.\n\nErstellen Data Report in Form eines .html-Files (mit der Knit-Funktion).\n\nDr√ºcken Sie auf das Knit-Zeichen (Wollkn√§uel) um das .Rmd-File zu einem .html-File zu knitten.\n√ñffnen Sie das .html-File zur Kontrolle.\n\nHochladen Ordner auf Ilias unter √úbung 2 mit:\n\npreprocessing_randomdot_data.Rmd-File\npreprocessing_randomdot_data.html-File\ndataset_rdk_clean.csv-File"
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html#abgabetermine",
    "href": "pages/chapters/uebung_2_datawrangling.html#abgabetermine",
    "title": "√úbung 2 - Data Wrangling",
    "section": "Abgabetermine",
    "text": "Abgabetermine\nGruppe Montag: 17. April 2024 23:55\nGruppe Mittwoch: 17. April 2024 23:55\nAbgabetermin Korrektur: 5. Mai 2024 23:55 (gilt f√ºr beide Gruppen, Abgabe in Extraordner auf Ilias)"
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html#trouble-shooting",
    "href": "pages/chapters/uebung_2_datawrangling.html#trouble-shooting",
    "title": "√úbung 2 - Data Wrangling",
    "section": "Trouble shooting",
    "text": "Trouble shooting\nDas Einlesen funktioniert nicht.\nKontrollieren Sie bei der read.csv()-Funktion das sep = Zeichen. Passt dieses zum Datensatz?\nStimmt die Ordnerstruktur? Arbeiten Sie in einem R-Project, ist das .Rmd-File im selben Ordner wie das .Rproj-File und haben Sie die Daten im data-Ordner gespeichert (ungezippt, also nur die Datenfiles)?\nDie Datens√§tze k√∂nnen nicht alle eingelesen werden, obwohl ein einzelner Datensatz eingelesen werden kann.\nBitte √∂ffnen Sie die Dateien nur in R, nicht in Excel oder in einem anderen Programm, das kann die Formatierung √§ndern und Probleme beim Einlesen verursachen. Falls Sie die Daten schon ge√∂ffnet haben, laden Sie den Datensatz nochmals neu von der Website herunter und speichern Sie ihn im Datenordner.\nHaben Sie die Pfade richtig gesetzt? Haben Sie die Funktion paste() korrekt genutzt?\nWeitere Fehlermeldungen\nLesen Sie die Fehlermeldung genau durch: Finden Sie Informationen zur L√∂sung des Fehlers?\nH√§ufigste Fehlerquellen:\n\nTippfehler\nFormattierung Code-Chunk: Er muss mit ```{r} beginnen und mit ``` enden. Stellen Sie sicher, dass eine L√ºcke zum n√§chsten Code-Chunk besteht.\nReihenfolgenfehler: L√∂schen Sie alle Variablen mit dem ‚ÄúBesen‚Äù-Symbol oben rechts und lassen Sie Ihren Code nochmals von Beginn an durch laufen. Wo stockt der Code?\n\nBei Problemen, fragen Sie bitte in der n√§chsten Veranstaltung nach."
  },
  {
    "objectID": "pages/chapters/uebung_2_datawrangling.html#feedback",
    "href": "pages/chapters/uebung_2_datawrangling.html#feedback",
    "title": "√úbung 2 - Data Wrangling",
    "section": "Feedback",
    "text": "Feedback\nPositiv aufgefallen:\n\nKommentare zum Code und dem Vorgehen eingef√ºgt (erleichtert sp√§tere Nachvollziehbarkeit)\nKreative L√∂sungen (z.B. for-Loop zum Daten einlesen) und kreatives Layout (z.B. Inhaltsverzeichnis im Markdown f√ºr HTML-Datei)\nUnterdr√ºcken von warnings und messages: Diese k√∂nnen in RMarkdown im Output-File (z.B. html-File) unterdr√ºckt werden, so kann der relevante Code nachvollzogen werden, aber es wird nicht f√ºr jeden Datensatz die Information abgedruckt (Leserlichkeit)\nNeue Zeile nach der Pipe: Es muss zwar nicht nach jeder Pipe eine neue Zeile genommen werden, der Code ist aber damit √ºbersichtlicher und nachvollziehbarer.\n\nH√§ufige Fehler:\n\nglimpse() vergessen (dies diente dazu, dass im html-File sictbar wird, ob bspw. alle relevanten Variablen im bereinigten Datensatz vorhanden sind, etc.)\nVerwenden von hard coded paths: Der Pfad sollte in der Funktion, die f√ºr das Zusammenf√ºgen und Vorbereiten der Datens√§tze ist als Variable eingef√ºgt werden (z.B. path). Wird hier der Pfad einer Versuchsperson eingef√ºgt, wird immer wieder derselbe Datensatz eingelesen! Auch sollte der Pfad immer von dem .Rmd-File aus gelesen werden k√∂nnen und keine lokalen Informationen (z.B. C:/Users/... verwenden)."
  },
  {
    "objectID": "pages/chapters/uebung_3_plot.html",
    "href": "pages/chapters/uebung_3_plot.html",
    "title": "√úbung 3 - Plot",
    "section": "",
    "text": "Erstellen Sie einen Plot der Stroop Daten. Verwenden Sie dazu ggplot(). Alle Plots und der entsprechende Code werden in der Galerie auf der Kurshomepage anonym ver√∂ffentlicht."
  },
  {
    "objectID": "pages/chapters/uebung_3_plot.html#auftrag",
    "href": "pages/chapters/uebung_3_plot.html#auftrag",
    "title": "√úbung 3 - Plot",
    "section": "",
    "text": "Erstellen Sie einen Plot der Stroop Daten. Verwenden Sie dazu ggplot(). Alle Plots und der entsprechende Code werden in der Galerie auf der Kurshomepage anonym ver√∂ffentlicht."
  },
  {
    "objectID": "pages/chapters/uebung_3_plot.html#vorgehen",
    "href": "pages/chapters/uebung_3_plot.html#vorgehen",
    "title": "√úbung 3 - Plot",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nSetup\n\nLaden Sie das R-Project Uebung3 herunter und entzippen Sie den Ordner.\n√Ñndern Sie den Namen der Datei nachname_vorname_plot.R, indem Sie Ihren Vor- und Nachnamen einsetzen.\n\nErstellen des Plots (mit folgenden Schritten):\n\n√ñffnen Sie die die Datei nachname_vorname_plot.R. Der Inhalt dieser Datei muss gleich aussehen, wie im Beispiel unten.\nDer Code auf von Zeile 1 bis Zeile 8 darf nicht ver√§ndert werden!\nF√ºgen Sie den Code f√ºr Ihre Abbildung ab Zeile 9 ein.\nDer eingef√ºgte Code muss die Abbildung erstellen (vgl. Zeile 9-12) und anzeigen (vgl. Zeile 12).\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht ver√§ndert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_stroop_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(...) +\n    ...\np\n\n\nDer Plot muss Folgendes beinhalten:\n\nBeides, Rohdaten UND mind. 1 zusammenfassendes Mass(z.B. Mittelwert mit Standardabweichungen, Box-/Violinplot, etc.). TIPP: Mehrere Geoms k√∂nnen √ºbereinander gelegt werden.\nMind. 2 unterschiedliche Farben.\nBeschriftungen: Titel, Subtitel, Achsenbeschriftungen, (optional: Captions)\nDer Subtitel beinhaltet die Frage, welche der Plot beantwortet.\n\nEin Theme verwenden.\nOptional: Facets verwenden.\n\nHochladen Codefile auf Ilias unter √úbung 3 :\n\nGeben Sie die Datei nachname_vorname_plot.R (umbenannt mit Ihrem Vor- und Nachnamen) auf Ilias unter √úbung 3 ab."
  },
  {
    "objectID": "pages/chapters/uebung_3_plot.html#abgabetermine",
    "href": "pages/chapters/uebung_3_plot.html#abgabetermine",
    "title": "√úbung 3 - Plot",
    "section": "Abgabetermine",
    "text": "Abgabetermine\nGruppe Montag: 28. April 2024 23:55\nGruppe Mittwoch: 30. April 2024 23:55\n\n\n\n\n\n\nWichtig\n\n\n\nIhr Plot und der dazugeh√∂rige Code wird in der Galerie anonym ver√∂ffentlich. Deshalb ist es wichtig, dass die oben aufgelisteten Voraussetzungen erf√ºllt sind.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBei Problemen, fragen Sie bitte an der n√§chsten Veranstaltung nach."
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html",
    "href": "pages/chapters/uebung_4_dataanalysis.html",
    "title": "√úbung 4 - Daten Analyse",
    "section": "",
    "text": "Beantworten Sie eine Fragestellung mit einem Bayesianischen \\(t\\)-Test oder einem TOST-√Ñquivalenztest in JASP. Interpretieren Sie die Resultate und ordnen Sie die Erkenntnisse kritisch ein. Laden Sie anschliessend sowohl das JASP File, sowie das ausgef√ºllte Formular mit Ihren Antworten hoch."
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html#auftrag",
    "href": "pages/chapters/uebung_4_dataanalysis.html#auftrag",
    "title": "√úbung 4 - Daten Analyse",
    "section": "",
    "text": "Beantworten Sie eine Fragestellung mit einem Bayesianischen \\(t\\)-Test oder einem TOST-√Ñquivalenztest in JASP. Interpretieren Sie die Resultate und ordnen Sie die Erkenntnisse kritisch ein. Laden Sie anschliessend sowohl das JASP File, sowie das ausgef√ºllte Formular mit Ihren Antworten hoch."
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html#vorgehen",
    "href": "pages/chapters/uebung_4_dataanalysis.html#vorgehen",
    "title": "√úbung 4 - Daten Analyse",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nLaden Sie hier das Datenfile herunter.\nLaden Sie hier das auszuf√ºllende Word-File mit der Arbeitsanleitung herunter.\nArbeiten Sie entlang der Arbeitsanleitung.\nSpeichern Sie das Word-File und .jasp-File unter Name_Vorname_Uebung_4 ab. Kontrollieren Sie das Word-File auf Vollst√§ndigkeit.\nLaden Sie das .jasp-Analyse-File und das Word-File auf Ilias hoch."
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html#informationen-zum-datensatz",
    "href": "pages/chapters/uebung_4_dataanalysis.html#informationen-zum-datensatz",
    "title": "√úbung 4 - Daten Analyse",
    "section": "Informationen zum Datensatz",
    "text": "Informationen zum Datensatz\nDer Datensatz stammt aus dem Random-Dot Experiment und wurde vom long zum wide Format umgewandelt.\n\nFalls Sie der Code daf√ºr interessiert, ist dieser hier zu finden, diesen brauchen Sie aber f√ºr die √úbung nicht.\n\nEr enth√§lt folgende Variablen(paare), von denen Sie f√ºr die √úbung eines auslesen m√ºssen:\n\nVersuchspersonenidentifikation (id)\nrt_speed und rt_accuracy: Mittelwerte der Reaktionszeiten f√ºr die Instruktion speed(so schnell wie m√∂glich antworten) oder accuracy (so richtig wie m√∂glich antworten) pro Versuchsperson und Bedingung\ncorr_speed und corr_accuracy: Mittelwerte der korrekten Antworten f√ºr die Instruktion speed(so schnell wie m√∂glich antworten) oder accuracy (so richtig wie m√∂glich antworten) pro Versuchsperson und Bedingung\nrt_speed_left und rt_speed_right: Mittelwerte der Reaktionszeiten f√ºr die Instruktion speed(so schnell wie m√∂glich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson und Bedingung\nrt_accuracy_left und rt_accuracy_right: Mittelwerte der Reaktionszeiten f√ºr die Instruktion accuracy(so richtig wie m√∂glich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson und Bedingung\ncorr_speed_left und corr_speed_right: Mittelwerte der korrekten Antworten f√ºr die Instruktion speed(so schnell wie m√∂glich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson und Bedingung\ncorr_accuracy_left und corr_accuracy_right: Mittelwerte der korrekten Antworten f√ºr die Instruktion accuracy(so richtig wie m√∂glich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson und Bedingung"
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html#abgabetermine",
    "href": "pages/chapters/uebung_4_dataanalysis.html#abgabetermine",
    "title": "√úbung 4 - Daten Analyse",
    "section": "Abgabetermine",
    "text": "Abgabetermine\nGruppe Montag: 17.05.2024 23:55\nGruppe Mittwoch: 19.05.2024 23:55"
  },
  {
    "objectID": "pages/chapters/uebung_4_dataanalysis.html#trouble-shooting",
    "href": "pages/chapters/uebung_4_dataanalysis.html#trouble-shooting",
    "title": "√úbung 4 - Daten Analyse",
    "section": "Trouble Shooting",
    "text": "Trouble Shooting\nEquivalence Testing funktioniert nicht in JASP\nStellen Sie sicher, dass Sie die neuste Version (0.18.3.0) installiert haben, in fr√ºheren Versionen gab es bei diesem Modul einen Bug, der aber jetzt geflickt ist."
  },
  {
    "objectID": "pages/chapters/ddm.html",
    "href": "pages/chapters/ddm.html",
    "title": "Drift Diffusion Modell",
    "section": "",
    "text": "ddm blog post\nThis is a webR-enabled code cell in a Quarto HTML document."
  },
  {
    "objectID": "pages/chapters/ddm.html#random-walk-simulieren",
    "href": "pages/chapters/ddm.html#random-walk-simulieren",
    "title": "Drift Diffusion Modell",
    "section": "Random Walk simulieren",
    "text": "Random Walk simulieren\nEin random walk ist das Resultat der Aufsummierung von Zufallszahlen. Probieren Sie es selber aus; simulieren Sie einen random walk mit 100 Zeitschritten. Fangen Sie bei \\(0\\) an, ziehen Sie 99 normalverteilte Zufallszahlen und berechnen Sie die kumulierte Summe. Plotten Sie das Resultat.\nDieser random walk hat keinen Trend, weil wir immer aus einer Normalverteilung mit Mittelwert \\(\\mu = 0\\) ziehen. Wenn wir stattdessen aus einer Verteilung mit \\(\\mu = 0.1\\) ziehen, erhalten wir einen positiven Trend.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  }
]