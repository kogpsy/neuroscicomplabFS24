{
  "hash": "5b365ce79a543d129b0bb6c7187a0710",
  "result": {
    "markdown": "---\ntitle: \"Neurowissenschaftliche Daten & Good Practices im data wrangling\"\ndescription: Datenverarbeitung reproduzierbar machen.\ndate: \"2023-03-20\"\nauthor:\n  - name: Gerda Wyssen\n    # url: https://github.com/awellis\n    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universität Bern \n    affiliation-url: https://www.kog.psy.unibe.ch\n    orcid: 0000-0001-7427-3149\nlicense: CC BY\ncitation: true\nbibliography: ../../bibliography_nsci24.bib\nformat:\n    html:\n        toc: true\n        code-link: true\n---\n\n\n\n<!-- - Programmieren von Datenverarbeitungspipelines: Datenvorverarbeitung und Visualisierung („good practices“, Data Cleaning, Reproduzierbarkeit)  -->\n\n<!--  kennen wichtige neurowissenschaftlichen Datenformen und ihre Eigenschaften -->\n<!-- - können Daten aus neurowissenschaftlichen Experimenten selber vorverarbeiten und informativ visualisieren -->\n\n> Nothing in neuroscience makes sense except in the light of behavior.\n>@shepherd_neurobiology_1988\n\nIn der neurowissenschaftlichen Forschung werden zunehmend sehr grosse und komplexe Datensätze generiert und Daten aus unterschiedlichen Datenerhebungsverfahren sollen miteinander verknüpft werden um neue Erkenntnisse zu gewinnen. Eine sehr häufige Kombination sind beispielsweise Verhaltens- und Bildgebungsdaten, wie es in vielen fMRI-Studien der Fall ist. Das erfordert Kenntnisse der unterschiedlichen Formate und Eigenschaften der Daten und Programmierkenntnisse um diese Daten möglichst automatisiert vorzuverarbeiten, zu verknüpfen, zu visualisieren und analysieren.\n\n>Neuroimaging experiments result in complicated data that can be arranged in many different ways. So far there is no consensus how to organize and share data obtained in neuroimaging experiments. Even two researchers working in the same lab can opt to arrange their data in a different way. Lack of consensus (or a standard) leads to misunderstandings and time wasted on rearranging data or rewriting scripts expecting certain structure. \n>[BIDS Website (2024)](https://bids-specification.readthedocs.io/en/stable/introduction.html)\n\n \n\n# Neurowissenschaftliche Daten\n\n\n\n\n>__Increasing complexity of neuroscience data__\n>\n>Over the past 20 years, neuroscience research has been radically changed by two major trends in data production and analysis. >\n>First, neuroscience research now routinely generates large datasets of high complexity. Examples include recordings of activity across large populations of neurons, often with high resolution behavioral tracking (Steinmetz et al., 2019; Stringer et al., 2019; Mathis et al., 2018; Siegle et al., 2021; Koch et al., 2022), analyses of neural connectivity at high spatial resolution and across large brain areas (Scheffer et al., 2020; Loomba et al., 2022), and detailed molecular profiling of neural cells (Yao et al., 2023; Langlieb et al., 2023; Braun et al., 2022; Callaway et al., 2021). Such large, multi-modal data sets are essential for solving major questions about brain function (Brose, 2016; Jorgenson et al., 2015; Koch and Jones, 2016).\n>\n>Second, the collection and analysis of such datasets requires interdisciplinary teams, incorporating expertise in systems neuroscience, engineering, molecular biology, data science, and theory. These two trends are reflected in the increasing numbers of authors on scientific publications (Wareham, 2016), and the creation of mechanisms to support team science by the NIH and similar research funding bodies (Cooke and Hilton, 2015; Volkow, 2022; Brose, 2016).\n>\n>There is also an increasing scope of research questions that can be addressed by aggregating “open data” from multiple studies across independent labs. Funding agencies and publishers have begun to aggressively promote data sharing and open data, with the goals of improving reproducibility and increasing data reuse (Dallmeier-Tiessen et al., 2014; Tenopir et al., 2015; Pasquetto et al., 2017). However, open data may be unusable if scattered in a wide variety of naming conventions and file formats lacking machine-readable metadata.\n>\n>Big data and team science necessitate new strategies for how to best organize data, with a key technical challenge being the development of standardized file formats for storing, sharing, and querying datasets. Prominent examples include the Brain Imaging Data Structure (BIDS) for neuroimaging, and Neurodata Without Borders (NWB) for neurophysiology data (Teeters et al., 2015; Gorgolewski et al., 2016; Rübel et al., 2022; Holdgraf et al., 2019). The Open Neurophysiology Environment (ONE), best known from adoption by The International Brain Laboratory (The International Brain Laboratory et al., 2020, 2023), has a similar application domain to NWB, but a highly different technical design. \n>\n>These initiatives provide technical tools for storing and accessing data in known formats, but more importantly provide conceptual frameworks with which to standardize data organization and description in an (ideally) universal, interoperable, and machine-readable way.\n@pierre_perspective_2024 (Preprint)[Pierre et al. 2023 (Preprint)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10593085/)\n\n:::callout-caution\n## Hands-on: Daten in den Neurowissenschaften\n\nWelche Datenformen kennen Sie?\n\n\nFüllen Sie in Gruppen die untenstehende Tabelle zusammen aus. Es gibt folgende Gruppen:\n- EEG\n- MRI\n- Eye-tracking\n- Verhaltensdaten\n\n\n\n\nSchliessen Sie sich am besten einer Gruppe an, in der Sie das Methodenseminar absolviert haben.\n:::\n\n\n\n# Good Practices\n\n\n\nDie Replikationskrise hat in der Psychologie, aber auch in den kognitiven Neurowissenschaften ein Umdenken ausgelöst. Mit dem Ziel nachhaltigere Forschungsergebnisse zu erreichen sind verschiedene Begriffe wie Reproduzierbarkeit und Replizierbarkeit zu wichtigen Schlagworten geworden. Die Begrifflichkeiten werden verwirrenderweise aber oft unterschiedlich definiert und verwendet (@plesser_reproducibility_2018).\n\n\n\n::: {.cell hash='goodpractices_data_cache/html/unnamed-chunk-1_94ceec283dc01e3535d850628aa16f35'}\n\n```{.r .cell-code}\n## Hands-on: B\n```\n:::\n\n\n\n\n\n\nWichtige Begriffe in der\nreproducibility, replicability, reliability, robustness, and generalizability\n\n\n\n\n\n# Replizierbarkeit \n\n_Replizierbarkeit (replicability)_ bedeutet, dass ein Experiment von einer anderen Forschungsgruppe mit einer neuen Stichprobe durchgeführt werden kann, und ähnliche oder dieselben Resultate hervorbringt, wie die Originalstudie. Wird eine Studie mehrmals repliziert, steigt die Wahrscheinlichkeit, dass kein Zufallsbefund vorliegt. \n\n> Replicability refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected. @cacioppo_social_2015\n\n\n# Reproduzierbarkeit \n\n_Reproduzierbarkeit (reproducibility)_ hängt eng mit der Replizierbarkeit zusammen. Der Begriff wird teilweise sehr allgemein verwendet, und bedeutet so dass Forschungsergebnisse wiederholt gefunden werden. Reproduzierbarkeit im engeren Sinn hingegen bezieht sich darauf, ob die durchgeführte Analyse wiederholt werden kann. Die Reproduzierbarkeit ist somit hoch, wenn Forschende die Daten und Datenanalyseskripts bereitstellen und andere Forschende damit dieselben Analysen durchführen können und zu gleichen Resultaten kommen.\n\n> Reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative. @cacioppo_social_2015\n\nUm die Begriffe zusammenzufassen schlugen @goodman_what_2016 vor von _Reproduzierbarkeit der Methoden_ (Daten und Prozesse können exakt wiederholt werden), _Reproduzierbarkeit der Resultate_ (andere Studien kommen auf dieselben Resultate) und _Reproduzierbarkeit der wissenschaftlichen Schlussfolgerung_ (bei Repetition der Analyse oder der Experimente werden dieselben Schlüsse gezogen) zu sprechen.\n\nGrundsätzlich besteht das Ziel, dass in der Forschung möglichst viel Evidenz für eine Schlussfolgerung gesammelt werden kann. Dies gelingt, wenn die Prozesse transparent, fehlerfrei und wiederholbar sind.\n\n\n# Hindernisse bei der Reproduzierbarkeit\n\nReproduzierbarkeit kann laut @nosek_replicability_2022 vor allem aus zwei Gründen nicht gegeben sein: Weil die Daten/Skripte nicht zur Verfügung stehen, oder weil diese Fehler enthalten:\n\n> In principle, all reported evidence should be reproducible. If someone applies the same analysis to the same data, the same result should occur. Reproducibility tests can fail for two reasons. A process reproducibility failure occurs when the original analysis cannot be repeated because of the unavailability of data, code, information needed to recreate the code, or necessary software or tools. An outcome reproducibility failure occurs when the reanalysis obtains a different result than the one reported originally. This can occur because of an error in either the original or the reproduction study.\n\nFührt die Reproduktion nicht zum selben Resultat, löst das Zweifel am Forschungsergebnis aus. Wenn die Reproduzierbarkeit am Prozess scheitert, etwa weil die Daten nicht vorhanden sind, kann kein Schluss gezogen werden, ob die Resultate stimmen. \n\n> Achieving reproducibility is a basic foundation of credibility, and yet many efforts to test reproducibility reveal success rates below 100%. ... Whereas an outcome reproducibility failure suggests that the original result may be wrong, a process reproducibility failure merely indicates that the original result cannot be verified. Either reason challenges credibility and increases uncertainty about the value of investing additional resources to replicate or extend the findings (Nuijten et al. 2018). Sharing data and code reduces process reproducibility failures (Kidwell et al. 2016), which can reveal more outcome reproducibility failures (Hardwicke et al. 2018, 2021; Wicherts et al. 2011).  @nosek_replicability_2022\n\nDas Teilen von Daten und Datenverarbeitungsskripten erhöht die Wahrscheinlichkeit, dass mögliche Fehler im Prozess gefunden werden, da auch andere Forschende die Daten/Skripts verwenden können. Das ist vorerst unangenehm, gehört aber zum Prozess der Wissenschaft dazu.\nReproduzierbarkeit erhöht also indirekt auch die Replizierbarkeit.\n\n\n# Tools für Reproduzierbarkeit\n\nFür reproduzierbare Forschung gibt es inzwischen viele gute Tools:\n\n- Website der [Open Science Foundation](https://osf.io/): Eine kostenfreie und unkomplizierte Möglichkeit Daten und Skripts zu teilen, und diese in Projekten abzulegen. Es lässt sich dafür sogar ein *doi* erstellen. Auch Preregistrationsformulare sind hier implementiert.\n\nBeim Veröffentlichen von wissenschaftlichen Artikeln ist es empfohlen, die Daten (falls anonymisiert möglich) sowie die Analyseskripts mitzuveröffentlichen. \n\n- Für Datensätze gelten die _FAIR Guiding Principles_ (@wilkinson_fair_2016): \n    - **F** indability: Es ist klar unter welchen Umständen und wie die Daten zugänglich sind\n    - **A** ccessibility: Daten sind zugänglich bzw. es ist klar wo sie zu finden wären\n    - **I** nteroperability: Verwendbare Datenformate/strukturen\n    - **R** eusability: gute Beschreibung des Datensatzes/der enthaltenen Variablen\n    \n<aside> [Hier](https://www.go-fair.org/fair-principles) finden Sie weitere Informationen zu _FAIR_. </aside>\n\n- Für Neuroimaging-Daten gibt es beispielsweise vorgegebene Konventionen, wie ein Datensatz und die Verarbeitungsskripts abgespeichert werden. Ein Beispiel dafür ist [Brain Imaging Data Structure (BIDS)](https://bids.neuroimaging.io). So können Datensätze mit einer für alle verständlichen Struktur veröffentlicht und geteilt werden.\n\n<aside> [Hier](https://andysbrainbook.readthedocs.io/en/latest/OpenScience/OS/BIDS_Overview.html) finden Sie weitere Informationen zu _BIDS_. </aside>\n\n- Für das Veröffentlichen von Analyseskripts eignen sich Formate wie _RMarkdown_ in _R_, oder _LiveScripts_ in _MATLAB_ sehr gut. Aber auch `.r`-Skripte, wie Sie sie in dieser Veranstaltung verwenden können veröffentlicht werden.\n\n\n<aside> [Hier](https://djnavarro.net/slides-starting-rmarkdown/#1) finden Sie eine sehr gut erklärte Einführung zu _RMarkdown_. </aside>\n\n\n# Code kommentieren\n\nDas Teilen von Skripts macht am meisten Sinn, wenn sie verständlich strukturiert und kommentiert sind. Beim Kommentieren von Code sollte folgendes beachtet werden:\n\n-   Kommentare sollten geschrieben werden, wenn der Code erstellt wird und laufend überarbeitet werden. Oft wird es sonst nicht nachgeholt.\n\n-   Wenn man nicht genau kommentieren kann, was man im Code macht, dann ist evtl. der Code unklar, oder man versteht ihn noch nicht. Vielleicht kann man Variablennamen vereinfachen/präzisieren und es braucht weniger Kommentare?\n\n-   Wenn Code kopiert wird, sollte die Quelle angegeben werden.\n\n-   Vor dem Veröffentlichen, lohnt es sich jemanden den Code ausführen lassen. So zeigt sich wo noch unklare Stellen sind, die Kommentare benötigen.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}